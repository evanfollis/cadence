{
  "src/cadence/agents/sidekick.py": "# src/cadence/agents/sidekick.py\n\"\"\"\nPersona agent that *delegates* to a ReasoningAgent but presents a\nhuman-centric mentor/advisor interface.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nfrom .profile import AgentProfile, REASONING_PROFILE\nfrom .reasoning import ReasoningAgent\n\n\n_SIDEKICK_PROMPT = \"\"\"\nYou are an AI-enhanced co-developer and mentor. Your primary goal is to\nextract the most creative, high-leverage ideas from the human user and\ntransform them into actionable improvements for the Cadence platform.\nAvoid tactical implementation details unless asked; focus on vision,\narchitecture, and pragmatic next steps.\n\"\"\"\n\n\nclass Sidekick:\n    \"\"\"\n    Thin wrapper: exposes `run_interaction` but delegates work to an\n    internal ReasoningAgent instance configured with a custom prompt.\n    \"\"\"\n\n    def __init__(self):\n        profile = AgentProfile(\n            name=\"sidekick\",\n            role=\"advisor\",\n            model=REASONING_PROFILE.model,\n            context_limit=REASONING_PROFILE.context_limit,\n            review_policy=REASONING_PROFILE.review_policy,\n            default_system_prompt=REASONING_PROFILE.default_system_prompt,\n            extra=REASONING_PROFILE.extra.copy() if REASONING_PROFILE.extra else {},\n        )\n        self._agent = ReasoningAgent(profile=profile, system_prompt=_SIDEKICK_PROMPT)\n        self._inject_seed_context()\n\n    # ------------------------------------------------------------------ #\n    # Public façade\n    # ------------------------------------------------------------------ #\n    def run_interaction(self, user_input: str, **kwargs) -> str:\n        return self._agent.run_interaction(user_input, **kwargs)\n\n    async def async_run_interaction(self, user_input: str, **kwargs) -> str:\n        return await self._agent.async_run_interaction(user_input, **kwargs)\n\n    # ------------------------------------------------------------------ #\n    # Private helpers\n    # ------------------------------------------------------------------ #\n    def _inject_seed_context(self):\n        docs = self._agent.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n\n        modules_path = Path(\"agent_context/module_contexts.json\")\n        modules = {}\n        if modules_path.exists():\n            modules = json.loads(modules_path.read_text())\n\n        self._agent.append_message(\n            \"user\",\n            f\"DOCS:\\n{docs}\\n---\\nMODULE_CONTEXTS:\\n{json.dumps(modules)[:10_000]}\",\n        )",
  "src/cadence/agents/base.py": "# src/cadence/agents/base.py\nfrom __future__ import annotations\n\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\n\nfrom src.cadence.llm.client import LLMClient, get_default_client\nfrom src.cadence.context.provider import ContextProvider, SnapshotContextProvider\nfrom src.cadence.audit.agent_event_log import AgentEventLogger\nfrom .profile import AgentProfile\n\n\nclass BaseAgent:\n    \"\"\"\n    The one true superclass for *all* Cadence agents.\n\n    An agent = (profile) + (conversation state) + (LLM client) [+ optional helpers]\n\n    Subclasses SHOULD NOT hard-code models; they inherit that from the supplied\n    `AgentProfile`.  Core agents (Reasoning / Execution / Efficiency) simply\n    pass the canonical profile; personas may inject a custom one.\n    \"\"\"\n\n    def __init__(\n        self,\n        profile: AgentProfile,\n        *,\n        llm_client: Optional[LLMClient] = None,\n        system_prompt: Optional[str] = None,\n        context_provider: Optional[ContextProvider] = None,\n    ):\n        self.profile = profile\n        self.llm_client = llm_client or get_default_client()\n        self.system_prompt = system_prompt or profile.default_system_prompt\n        self.context_provider = context_provider or SnapshotContextProvider()\n        self.messages: List[Dict[str, Any]] = []\n        self.reset_context()\n\n        # ---- audit logger  ---------------------------------------\n        _alog = AgentEventLogger()\n        self._agent_id = _alog.register_agent(\n            self.profile.name,\n            self.system_prompt or \"\",\n            context_digest=self._context_digest(),\n        )\n        self._alog = _alog\n\n    # --------------------------------------------------------------------- #\n    # Conversation helpers\n    # --------------------------------------------------------------------- #\n    def reset_context(self, system_prompt: Optional[str] = None):\n        \"\"\"Clear history and (re)set the system prompt.\"\"\"\n        self.messages = []\n        sys_prompt = system_prompt or self.system_prompt\n        if sys_prompt:\n            self.append_message(\"system\", sys_prompt)\n\n    def append_message(self, role: str, content: str):\n        self.messages.append({\"role\": role, \"content\": content})\n\n    # --------------------------------------------------------------------- #\n    # LLM calls\n    # --------------------------------------------------------------------- #\n    def run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        try:\n            self._alog.log_message(self._agent_id, \"user\", user_input)\n        except Exception:\n            pass\n        response = self.llm_client.call(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,  # already injected\n            agent_id=self._agent_id,\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        try:\n            self._alog.log_message(self._agent_id, \"assistant\", response)\n        except Exception:\n            pass\n        return response\n\n    async def async_run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        try:\n            self._alog.log_message(self._agent_id, \"user\", user_input)\n        except Exception:\n            pass\n        response = await self.llm_client.acall(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,\n            agent_id=self._agent_id,\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        try:\n            self._alog.log_message(self._agent_id, \"assistant\", response)\n        except Exception:\n            pass\n        return response\n    \n    # ---------------- internal helper -------------------------------\n    def _context_digest(self) -> str:\n        \"\"\"\n        Quick SHA-1 fingerprint of the reference docs that were injected\n        on reset(); helps you prove later that the agent saw *fresh*\n        context when the conversation started.\n        \"\"\"\n        import hashlib, json\n        if self.messages and self.messages[-1][\"role\"] == \"user\" \\\n           and self.messages[-1][\"content\"].startswith(\"REFERENCE_DOCUMENTS:\"):\n            payload = self.messages[-1][\"content\"]\n            return hashlib.sha1(payload.encode()).hexdigest()\n        return hashlib.sha1(json.dumps(self.messages[:1]).encode()).hexdigest()\n\n    # --------------------------------------------------------------------- #\n    # Persistence\n    # --------------------------------------------------------------------- #\n    def save_history(self, path: str):\n        import json\n        Path(path).write_text(json.dumps(self.messages, indent=2, ensure_ascii=False))\n\n    def load_history(self, path: str):\n        import json\n        self.messages = json.loads(Path(path).read_text())\n\n    # --------------------------------------------------------------------- #\n    # Context helpers\n    # --------------------------------------------------------------------- #\n    def gather_codebase_context(\n        self,\n        root: Tuple[str, ...] | None = None,\n        ext: Tuple[str, ...] = (\".py\", \".md\", \".json\", \".mermaid\", \".txt\", \".yaml\", \".yml\"),\n        **kwargs,\n    ) -> str:\n        \"\"\"Return repo/docs snapshot via the injected ContextProvider.\"\"\"\n        # ---------- resolve roots -------------------------------------\n        # Prefer the real package path  src/cadence/  if it exists; fall back to\n        # legacy  cadence/  (used by older notebooks or when the repo is\n        # checked out directly inside PYTHONPATH).\n        if root is None:\n            candidates = (\"src/cadence\", \"cadence\", \"docs\", \"tools\", \"scripts\", \"tests\")\n        else:\n            candidates = root\n\n        paths = [Path(p) for p in candidates if Path(p).exists()]\n        if not paths:                       # nothing found → empty string\n            return \"\"\n\n        return self.context_provider.get_context(*paths, exts=ext, **kwargs)\n",
  "src/cadence/agents/efficiency.py": "# src/cadence/agents/efficiency.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EFFICIENCY_PROFILE, AgentProfile\n\n\nclass EfficiencyAgent(BaseAgent):\n    \"\"\"\n    Final class: fast, low-cost linting & summarisation.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EFFICIENCY_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/agents/reasoning.py": "# src/cadence/agents/reasoning.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import REASONING_PROFILE, AgentProfile\n\n\nclass ReasoningAgent(BaseAgent):\n    \"\"\"\n    Final class: provides deep, chain-of-thought reasoning and architectural review.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = REASONING_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)\n\n    # Automatically inject a fresh code snapshot on each reset\n    def reset_context(self, system_prompt: str | None = None):\n        super().reset_context(system_prompt)\n        docs = self.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n        self.append_message(\"user\", f\"REFERENCE_DOCUMENTS:\\n{docs}\\n---\\nYou are cleared for deep reasoning.\")",
  "src/cadence/agents/profile.py": "# src/cadence/agents/profile.py\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any\n\n\n@dataclass(frozen=True, slots=True)\nclass AgentProfile:\n    \"\"\"\n    Immutable definition of an agent’s operational contract.\n\n    Nothing here executes code; it is pure data that can be validated,\n    serialised, or inspected by the Meta-agent and CI tooling.\n    \"\"\"\n    name: str\n    role: str\n    model: str\n    context_limit: int\n    review_policy: str = \"\"\n    default_system_prompt: str = \"\"\n    extra: Dict[str, Any] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n# Canonical profiles – these are the ONLY ones that Core Agents will default to\n# --------------------------------------------------------------------------- #\nREASONING_PROFILE = AgentProfile(\n    name=\"reasoning\",\n    role=\"plan-review\",\n    model=\"o3-2025-04-16\",\n    context_limit=200_000,\n    review_policy=\"Cannot commit code; must review Execution diff\",\n)\n\nEXECUTION_PROFILE = AgentProfile(\n    name=\"execution\",\n    role=\"implement\",\n    model=\"gpt-4.1\",\n    context_limit=1_000_000,\n    review_policy=\"Needs review by Reasoning or Efficiency\",\n)\n\nEFFICIENCY_PROFILE = AgentProfile(\n    name=\"efficiency\",\n    role=\"lint-summarise\",\n    model=\"o4-mini\",\n    context_limit=200_000,\n    review_policy=\"Reviews Execution unless diff is non-code\",\n)\n\n# Convenience lookup\nBUILTIN_PROFILES = {\n    \"reasoning\": REASONING_PROFILE,\n    \"execution\": EXECUTION_PROFILE,\n    \"efficiency\": EFFICIENCY_PROFILE,\n}",
  "src/cadence/agents/__init__.py": "\n",
  "src/cadence/agents/registry.py": "# src/cadence/agents/registry.py\n\"\"\"\nSingle place to obtain a Core Agent or Profile.\n\nAvoids hard-coding classes throughout the codebase.\n\"\"\"\n\nfrom typing import Type\n\nfrom .reasoning import ReasoningAgent\nfrom .execution import ExecutionAgent\nfrom .efficiency import EfficiencyAgent\nfrom .profile import BUILTIN_PROFILES, AgentProfile\n\n_CORE_AGENTS: dict[str, Type] = {\n    \"reasoning\": ReasoningAgent,\n    \"execution\": ExecutionAgent,\n    \"efficiency\": EfficiencyAgent,\n}\n\n\ndef get_agent(agent_type: str, **kwargs):\n    \"\"\"\n    Instantiate a Core Agent by `agent_type`.\n\n    Example:\n        agent = get_agent(\"execution\")\n    \"\"\"\n    if agent_type not in _CORE_AGENTS:\n        raise ValueError(f\"Unknown agent_type '{agent_type}'. Valid: {list(_CORE_AGENTS)}\")\n    return _CORE_AGENTS[agent_type](**kwargs)\n\n\ndef get_profile(profile_name: str) -> AgentProfile:\n    if profile_name not in BUILTIN_PROFILES:\n        raise ValueError(f\"Unknown profile '{profile_name}'. Valid: {list(BUILTIN_PROFILES)}\")\n    return BUILTIN_PROFILES[profile_name]",
  "src/cadence/agents/execution.py": "# src/cadence/agents/execution.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EXECUTION_PROFILE, AgentProfile\n\n\nclass ExecutionAgent(BaseAgent):\n    \"\"\"\n    Final class: generates or refactors significant portions of the codebase.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EXECUTION_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)"
}