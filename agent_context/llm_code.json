{
  "src/cadence/llm/client.py": "# src/cadence/llm/client.py\nfrom __future__ import annotations\n\nimport os, logging, time\nfrom typing import List, Dict, Any, Optional, cast\n\nfrom openai import AsyncOpenAI, OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom dotenv import load_dotenv\nimport tiktoken\nimport hashlib\nfrom cadence.audit.llm_call_log import LLMCallLogger\n\n# one-time env expansion\nload_dotenv()\n\nlogger = logging.getLogger(\"cadence.llm.client\")\nif not logger.handlers:\n    h = logging.StreamHandler()\n    h.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\"))\n    logger.addHandler(h)\nlogger.setLevel(logging.INFO)\n\n_DEFAULT_MODELS = {\n    \"reasoning\": \"o3-2025-04-16\",\n    \"execution\": \"gpt-4.1\",\n    \"efficiency\": \"o4-mini\",\n}\n\n\ndef _count_tokens(model: str, messages: List[Dict[str, str]]) -> int:\n    enc = tiktoken.get_encoding(\"o200k_base\")\n    return sum(len(enc.encode(m[\"role\"])) + len(enc.encode(m[\"content\"])) for m in messages)\n\n\nclass LLMClient:\n    \"\"\"\n    Central sync/async wrapper with:\n\n    • stub-mode when no API key\n    • optional json_mode   → OpenAI “response_format={type:json_object}”\n    • optional function_spec → OpenAI “tools=[…]”\n    \"\"\"\n\n    _warned_stub = False\n\n    def __init__(\n        self,\n        *,\n        api_key: Optional[str] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        default_model: Optional[str] = None,\n    ):\n        key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        self.stub = not bool(key)\n        self.api_key = key\n        self.api_base = api_base or os.getenv(\"OPENAI_API_BASE\")\n        self.api_version = api_version or os.getenv(\"OPENAI_API_VERSION\")\n        self.default_model = default_model or _DEFAULT_MODELS[\"execution\"]\n\n        if self.stub:\n            if not LLMClient._warned_stub:\n                logger.warning(\n                    \"[Cadence] LLMClient stub-mode — OPENAI_API_KEY missing; \"\n                    \".call()/ .acall() return canned message.\"\n                )\n                LLMClient._warned_stub = True\n            self._sync_client = None\n            self._async_client = None\n        else:\n            try:\n                self._sync_client  = OpenAI(api_key=self.api_key,\n                                            base_url=self.api_base)\n                self._async_client = AsyncOpenAI(api_key=self.api_key,\n                                                 base_url=self.api_base)\n                # If the test-suite monkey-patched OpenAI to a stub that\n                # returns None we must still fall back to stub-mode.\n                if self._sync_client is None or not hasattr(self._sync_client,\n                                                            \"chat\"):\n                    raise AttributeError\n            except Exception:                      # noqa: BLE001\n                self.stub = True\n                self._sync_client = self._async_client = None\n                if not LLMClient._warned_stub:\n                    logger.warning(\"[Cadence] LLMClient stub-mode (auto)\")\n                    LLMClient._warned_stub = True\n\n    # ------------------------------------------------------------------ #\n    def _resolve_model(self, model: Optional[str], agent_type: Optional[str]) -> str:\n        if model:\n            return model\n        if agent_type and agent_type in _DEFAULT_MODELS:\n            return _DEFAULT_MODELS[agent_type]\n        return self.default_model\n\n    # ------------------------------------------------------------------ #\n    def call(\n        self,\n        messages: List[Dict[str, Any]],\n        *,\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        json_mode: bool = False,\n        function_spec: Optional[List[Dict[str, Any]]] = None,\n        **kwargs,\n    ) -> str:\n        if self.stub:\n            return \"LLM unavailable — Cadence stub-mode\"\n\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        prompt_tokens = _count_tokens(used_model, msgs)\n        t0 = time.perf_counter()\n\n        # -- wrap tools if present --------------------------------------\n        tools_arg = None\n        tool_choice_arg = None\n        if function_spec:\n            tools_arg = [{\"type\": \"function\", \"function\": fs}\n                         for fs in function_spec]\n            tool_choice_arg = {\n                \"type\": \"function\",\n                \"function\": {               # <- nest correctly\n                    \"name\": function_spec[0][\"name\"]\n                }\n            }\n\n        response = self._sync_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            # Never send response_format if we are already in tool-call mode\n            response_format=None if function_spec else (\n                {\"type\": \"json_object\"} if json_mode else None\n            ),\n            tools=tools_arg,\n            tool_choice=tool_choice_arg,\n            **kwargs,\n        )\n\n        # ------------------------------------------------------------ #\n        # OpenAI mutually-excludes  “tools=…”   and   “response_format”.\n        # If we supplied  tools=function_spec, the assistant returns\n        # the result in   message.tool_calls[0].function.arguments\n        # and leaves   message.content == None.\n        # ------------------------------------------------------------ #\n        if response.choices[0].message.content is None and response.choices[0].message.tool_calls:\n            # We requested exactly ONE function; grab its arguments.\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        else:\n            content = (response.choices[0].message.content or \"\").strip()\n\n        latency = time.perf_counter() - t0\n        completion_tokens = getattr(response.usage, \"completion_tokens\", None)\n\n        LLMCallLogger().log({\n            \"ts\": time.time(),\n            \"agent_id\": kwargs.get(\"agent_id\", \"n/a\"),\n            \"model\": used_model,\n            \"temperature\": kwargs.get(\"temperature\"),\n            \"top_p\": kwargs.get(\"top_p\"),\n            \"prompt_tokens\": prompt_tokens,\n            \"completion_tokens\": completion_tokens,\n            \"latency_s\": latency,\n            \"result_sha\": hashlib.sha1(content.encode()).hexdigest(),\n        })\n\n        logger.info(\n            \"LLM call %s → %.2fs  prompt≈%d  completion≈%d\",\n            used_model,\n            latency,\n            prompt_tokens,\n            completion_tokens,\n        )\n        return content\n\n    # async version (rarely used by Cadence core)\n    async def acall(\n        self,\n        messages: List[Dict[str, Any]],\n        *,\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        json_mode: bool = False,\n        function_spec: Optional[List[Dict[str, Any]]] = None,\n        **kwargs,\n    ) -> str:\n        if self.stub:\n            return \"LLM unavailable — Cadence stub-mode\"\n\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        prompt_tokens = _count_tokens(used_model, msgs)\n        t0 = time.perf_counter()\n\n        # -- wrap tools if present --------------------------------------\n        tools_arg = None\n        tool_choice_arg = None\n        if function_spec:\n            tools_arg = [{\"type\": \"function\", \"function\": fs}\n                         for fs in function_spec]\n            tool_choice_arg = {\n                \"type\": \"function\",\n                \"function\": {               # <- nest correctly\n                    \"name\": function_spec[0][\"name\"]\n                }\n            }\n\n        response = await self._async_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            # Never send response_format if we are already in tool-call mode\n            response_format=None if function_spec else (\n                {\"type\": \"json_object\"} if json_mode else None\n            ),\n            tools=tools_arg,\n            tool_choice=tool_choice_arg,\n            **kwargs,\n        )\n\n        # ------------------------------------------------------------ #\n        # OpenAI mutually-excludes  “tools=…”   and   “response_format”.\n        # If we supplied  tools=function_spec, the assistant returns\n        # the result in   message.tool_calls[0].function.arguments\n        # and leaves   message.content == None.\n        # ------------------------------------------------------------ #\n        if response.choices[0].message.content is None and response.choices[0].message.tool_calls:\n            # We requested exactly ONE function; grab its arguments.\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        else:\n            content = (response.choices[0].message.content or \"\").strip()\n\n        latency = time.perf_counter() - t0\n        completion_tokens = getattr(response.usage, \"completion_tokens\", None)\n\n        LLMCallLogger().log({\n            \"ts\": time.time(),\n            \"agent_id\": kwargs.get(\"agent_id\", \"n/a\"),\n            \"model\": used_model,\n            \"temperature\": kwargs.get(\"temperature\"),\n            \"top_p\": kwargs.get(\"top_p\"),\n            \"prompt_tokens\": prompt_tokens,\n            \"completion_tokens\": completion_tokens,\n            \"latency_s\": latency,\n            \"result_sha\": hashlib.sha1(content.encode()).hexdigest(),\n        })\n\n        logger.info(\n            \"LLM call %s → %.2fs  prompt≈%d  completion≈%d\",\n            used_model,\n            latency,\n            prompt_tokens,\n            completion_tokens,\n        )\n        return content\n\n\n# helper for callers that want the singleton\ndef get_default_client() -> LLMClient:\n    return _DEFAULT_CLIENT\n\n\n_DEFAULT_CLIENT = LLMClient()\n",
  "src/cadence/llm/json_call.py": "# src/cadence/llm/json_call.py\n\"\"\"\nLLMJsonCaller – ask the model for strictly-typed JSON via function-calling.\nRetries automatically on validation failure and normalises legacy shapes.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport re\nimport time\nfrom typing import Any, Dict\n\nimport jsonschema\n\nfrom cadence.llm.client import get_default_client\nfrom cadence.dev.schema import CHANGE_SET_V1\n\nlogger = logging.getLogger(\"cadence.llm.json_call\")\nif not logger.handlers:\n    logger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n\n_MAX_RETRIES = 3\n\n\nclass LLMJsonCaller:\n    \"\"\"\n    Generic wrapper: validate any JSON object returned via the OpenAI\n    *function-calling* pathway.\n\n    Parameters\n    ----------\n    schema        – Draft-07 JSON-schema the assistant must satisfy.\n    function_name – Name exposed to the OpenAI tools array (defaults to\n                    “create_change_set” for backward-compat).\n    \"\"\"\n    def __init__(\n        self,\n        *,\n        schema: Dict = CHANGE_SET_V1,\n        function_name: str = \"create_change_set\",\n        model: str | None = None,\n    ):\n        self.schema = schema\n        self.model = model\n        self.llm = get_default_client()\n\n        self.func_spec = [\n            {\n                \"name\": function_name,\n                \"description\": \"Return an object that satisfies the supplied schema\",\n                \"parameters\": self.schema,\n            }\n        ]\n\n    # ------------------------------------------------------------------ #\n    def ask(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:\n        # Off-line / CI guard – bail out immediately\n        if getattr(self.llm, \"stub\", False):\n            raise RuntimeError(\"LLM unavailable — stub-mode\")\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n\n        for attempt in range(1, _MAX_RETRIES + 1):\n            resp = self.llm.call(\n                messages,\n                model=self.model,\n                json_mode=True,\n                function_spec=self.func_spec,\n            )\n\n            try:\n                # resp may be str *or* dict (when tool-call path chosen)\n                obj = resp if isinstance(resp, dict) else _parse_json(resp)\n                # Change-set helper no-op for other schemas\n                if self.schema is CHANGE_SET_V1:\n                    obj = _normalise_legacy(obj)\n                jsonschema.validate(obj, self.schema)\n                return obj\n\n            except Exception as exc:  # noqa: BLE001\n                logger.warning(\n                    \"JSON validation failed (%d/%d): %s\", attempt, _MAX_RETRIES, exc\n                )\n\n                # When parsing/validation fails, fall back to the raw response\n                assistant_output = (\n                    resp if isinstance(resp, str) else json.dumps(resp)\n                )[:4000]\n\n                # Inject the invalid output so the model can self-correct\n                messages.append({\"role\": \"assistant\", \"content\": assistant_output})\n                messages.append(\n                    {\n                        \"role\": \"user\",\n                        \"content\": (\n                            \"The JSON object is invalid. \"\n                            \"Return ONLY a corrected JSON object.\"\n                        ),\n                    }\n                )\n                time.sleep(1)\n\n        raise RuntimeError(\"LLM gave invalid JSON after multiple retries\")\n\n\n# --------------------------------------------------------------------------- #\n# helpers\n# --------------------------------------------------------------------------- #\ndef _parse_json(text: str) -> Dict[str, Any]:\n    \"\"\"\n    If OpenAI response_format works, `text` is already pure JSON.\n    Guard against accidental fencing.\n    \"\"\"\n    if text.strip().startswith(\"```\"):\n        m = re.search(r\"```json\\s*([\\s\\S]*?)```\", text, re.I)\n        if not m:\n            raise ValueError(\"Could not locate fenced JSON block\")\n        text = m.group(1)\n    return json.loads(text)\n\n\ndef _normalise_legacy(obj: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Accept LLM output that uses {\"changes\":[…]} instead of {\"edits\":[…]}.\n    \"\"\"\n    if \"changes\" in obj and \"edits\" not in obj:\n        obj[\"edits\"] = [\n            {\n                \"path\": c.get(\"file\") or c.get(\"path\"),\n                \"mode\": c.get(\"mode\", \"modify\"),\n                \"after\": c.get(\"after\"),\n                \"before_sha\": c.get(\"before_sha\"),\n            }\n            for c in obj[\"changes\"]\n        ]\n        obj.pop(\"changes\")\n    return obj",
  "src/cadence/llm/__init__.py": "\n"
}