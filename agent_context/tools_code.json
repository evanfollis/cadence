{
  "tools/collect_code.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\n\"\"\"\ncollect_code.py  –  Export Cadence source files to a single JSON payload.\n\nUsage\n-----\npython tools/collect_code.py \\\n       --root cadence              # package folder(s) to scan (repeatable)\n       --out  code_payload.json   # written JSON (stdout if omitted)\n       --ext .py .md              # file extensions to keep\n       --max-bytes 50000          # skip giant files (>50 kB)\n\nResult\n------\nA JSON dict   { \"relative/path/to/file\": \"UTF-8 text …\", ... }\n\"\"\"\n\nfrom pathlib import Path\nimport argparse\nimport json\nimport sys\n\nDEFAULT_EXT = (\".py\", \".md\", \".cfg\", \".toml\", \".ini\")\n\n\ndef collect(\n    roots: list[Path],\n    files: list[Path] = [],\n    *,\n    extensions: tuple[str, ...] = DEFAULT_EXT,\n    max_bytes: int | None = None,\n) -> dict[str, str]:\n    \"\"\"\n    Walk *roots* and return {relative_path: code_text}.\n    Skips __pycache__, hidden folders, and files larger than *max_bytes*.\n    \"\"\"\n    out: dict[str, str] = {}\n    for root in roots:\n        for path in root.rglob(\"*\"):\n            if (\n                path.is_file()\n                and path.suffix in extensions\n                and \"__pycache__\" not in path.parts\n                and not any(p.startswith(\".\") for p in path.parts)\n            ):\n                if max_bytes and path.stat().st_size > max_bytes:\n                    continue\n                try:\n                    text = path.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[str(path.relative_to(Path.cwd()))] = text\n    for file in files:\n        if (\n            file.is_file()\n            and file.suffix in extensions\n            and file.stat().st_size <= (max_bytes or float(\"inf\"))\n        ):\n            rel = str(file.relative_to(Path.cwd()))\n            if rel not in out:\n                try:\n                    text = file.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = file.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[rel] = text\n    return out\n\n\ndef parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Collect source files into JSON.\")\n    p.add_argument(\n        \"--root\",\n        nargs=\"+\",\n        default=[\"cadence\"],\n        help=\"Directories to scan (repeatable).\",\n    )\n    p.add_argument(\n        \"--ext\",\n        nargs=\"+\",\n        default=DEFAULT_EXT,\n        help=\"File extensions to include (repeatable).\",\n    )\n    p.add_argument(\n        \"--max-bytes\",\n        type=int,\n        default=50000,\n        help=\"Skip files larger than this size (bytes).\",\n    )\n    p.add_argument(\n        \"--out\",\n        type=str,\n        default=\"-\",\n        help=\"Output JSON file path or '-' for stdout.\",\n    )\n    p.add_argument(\n        \"--file\",\n        nargs=\"+\",\n        default=[],\n        help=\"Individual files to include (repeatable).\",\n    )\n    return p.parse_args(argv)\n\n\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = parse_args(argv)\n    payload = collect(\n        [Path(r).resolve() for r in args.root],\n        files=[Path(f).resolve() for f in args.file],\n        extensions=tuple(args.ext),\n        max_bytes=args.max_bytes,\n    )\n    if args.out == \"-\":\n        json.dump(payload, sys.stdout, indent=2, ensure_ascii=False)\n    else:\n        out_path = Path(args.out)\n        out_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False))\n        print(f\"Wrote {len(payload)} files → {out_path}\")\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    main()\n",
  "tools/gen_prompt.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\n\"\"\"\ngen_prompt.py  –  Assemble a mega-prompt that contains\n\n  • Ground-truth docs (blueprint, progress logs, etc.)\n  • Full source snapshot (or whatever roots you point at)\n  • A header that gives the LLM an explicit NEXT TASK and ENV context\n\nUsage\n-----\npython tools/gen_prompt.py \\\n       --code-root cadence \\\n       --docs-dir docs \\\n       --task \"Implement FactorRegistry API and unit tests\" \\\n       --env  \"Python 3.11, pandas 2.2, scikit-learn 1.4\" \\\n       --out  prompt.txt\n\"\"\"\n\nfrom pathlib import Path\nimport argparse\nimport sys\nimport textwrap\n\n# --------------------------------------------------------------------------- #\n#  Config\n# --------------------------------------------------------------------------- #\nDEFAULT_CODE_EXT = (\".py\", \".md\", \".toml\", \".ini\", \".cfg\")\n\n\n# --------------------------------------------------------------------------- #\n#  Helpers\n# --------------------------------------------------------------------------- #\ndef _collect_files(\n    roots: list[Path],\n    *,\n    include_ext: tuple[str, ...],\n    max_bytes: int | None = None,\n) -> list[tuple[str, str]]:\n    \"\"\"Return [(relative_path, text), …] for all files matching *include_ext*.\"\"\"\n    records: list[tuple[str, str]] = []\n    cwd = Path.cwd()\n\n    for root in roots:\n        root = Path(root).resolve()\n        if not root.exists():\n            print(f\"WARNING: directory not found → {root}\", file=sys.stderr)\n            continue\n\n        for p in root.rglob(\"*\"):\n            if (\n                p.is_file()\n                and p.suffix in include_ext\n                and \"__pycache__\" not in p.parts\n                and not any(part.startswith(\".\") for part in p.parts)\n            ):\n                if max_bytes and p.stat().st_size > max_bytes:\n                    continue\n                try:\n                    txt = p.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n                records.append((str(p.relative_to(cwd)), txt))\n\n    records.sort()\n    return records\n\n\ndef _build_prompt(\n    docs: list[tuple[str, str]],\n    code: list[tuple[str, str]],\n    *,\n    header: str,\n) -> str:\n    parts: list[str] = [header]\n\n    # -- docs ---------------------------------------------------------------\n    parts.append(\"\\n## 1. Ground-Truth Documents\")\n    if not docs:\n        parts.append(\"\\n_No Markdown / text documents found in docs directory._\")\n    for path, txt in docs:\n        parts.append(f\"\\n### {path}\\n```markdown\\n{txt}\\n```\")\n\n    # -- code ---------------------------------------------------------------\n    parts.append(\"\\n## 2. Source Code Snapshot\")\n    if not code:\n        parts.append(\"\\n_No source files found in code roots._\")\n    for path, txt in code:\n        fence = \"```python\" if path.endswith(\".py\") else \"```text\"\n        parts.append(f\"\\n### {path}\\n{fence}\\n{txt}\\n```\")\n\n    return \"\\n\".join(parts)\n\n\n# --------------------------------------------------------------------------- #\n#  CLI\n# --------------------------------------------------------------------------- #\ndef _parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Generate mega-prompt for LLM.\")\n    p.add_argument(\"--code-root\", nargs=\"+\", default=[\"cadence\"],\n                   help=\"Package directories to scan (repeatable).\")\n    p.add_argument(\"--docs-dir\", default=\"docs\",\n                   help=\"Directory holding NORTH_STAR.md, progress logs, etc.\")\n    p.add_argument(\"--ext\", nargs=\"+\", default=DEFAULT_CODE_EXT,\n                   help=\"File extensions to include from code roots.\")\n    p.add_argument(\"--max-bytes\", type=int, default=100_000,\n                   help=\"Skip individual files larger than this size (bytes).\")\n    p.add_argument(\"--skip-code\", action=\"store_true\",\n               help=\"Omit source snapshot (tasks only).\")\n    p.add_argument(\"--task\", default=\"Tell me the next highest-leverage step and write the code.\",\n                   help=\"Explicit next task instruction injected into header.\")\n    p.add_argument(\"--env\", default=\"Python 3.11, pandas 2.2, scikit-learn 1.4\",\n                   help=\"Runtime environment string added to header.\")\n    p.add_argument(\"--out\", default=\"-\",\n                   help=\"Output file path or '-' for stdout.\")\n    return p.parse_args(argv)\n\n\n# --------------------------------------------------------------------------- #\n#  Main\n# --------------------------------------------------------------------------- #\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = _parse_args(argv)\n\n    docs = _collect_files(\n        [Path(args.docs_dir).resolve()],\n        include_ext=(\".md\", \".txt\"),\n        max_bytes=args.max_bytes,\n    )\n\n    if args.skip_code:\n        code = []\n    else:\n        code = _collect_files(\n            [Path(r).resolve() for r in args.code_root],\n            include_ext=tuple(args.ext),\n            max_bytes=args.max_bytes,\n        )\n\n    header = textwrap.dedent(\n        f\"\"\"\\\n        ### CADENCE STATUS / ALIGNMENT REQUEST  (auto-generated)\n\n        **Task**: {args.task}\n        **Environment**: {args.env}\n\n        You are an expert reviewer. Read ALL content below — docs first, then full\n        code — and report:\n\n          1. Alignment gaps between implementation and blueprint  \n          2. Missing risk / compliance safeguards  \n          3. Highest-leverage next actions  \n\n        Be brutally honest. No cheerleading. Return your analysis **only**.\n\n        ---\n        \"\"\"\n    )\n\n    prompt = _build_prompt(docs, code, header=header)\n\n    if args.out == \"-\":\n        sys.stdout.write(prompt)\n    else:\n        Path(args.out).write_text(prompt, encoding=\"utf-8\")\n        print(f\"Wrote prompt to {args.out}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
  "tools/lint_docs.py": "# tools/lint_docs.py\n\"\"\"\nLint the CADENCE docs for drift in phase table, agents, and OKRs.\nEnsures doc ↔ code synchrony on phase ordering and canonical identifiers.\nExtended for Failure-Diagnose phase.\n\"\"\"\nimport re, sys\nfrom pathlib import Path\n\nPHASE_ENUM = [\n    \"Backlog\",\n    \"Generate\",\n    \"Execute\",\n    \"Review-Reasoning\",\n    \"Failure-Diagnose\",\n    \"Review-Efficiency\",\n    \"Branch-Isolate\",\n    \"Test (pre-merge)\",\n    \"Commit\",\n    \"Merge Queue\",\n    \"Record\",\n    \"Meta\",\n]\n\ndef lint_dev_process_phases():\n    \"\"\"\n    Ensure phase table in docs/DEV_PROCESS.md matches PHASE_ENUM, including 04-b Failure-Diagnose\n    \"\"\"\n    path = Path(\"docs/DEV_PROCESS.md\")\n    lines = path.read_text(encoding=\"utf8\").splitlines()\n    in_table = False\n    found = []\n    for line in lines:\n        if line.startswith(\"| Seq \"): in_table = True\n        if in_table and line.startswith(\"|\") and \"Phase\" not in line and \"-----\" not in line:\n            cells = [x.strip() for x in line.split(\"|\")[1:]]\n            phase = cells[1]\n            found.append(phase)\n        if in_table and line.strip() == \"\": break\n    if found != PHASE_ENUM:\n        print(\"\u001b[31mPhase table drift detected!\u001b[0m\")\n        print(\"doc table:   \", found)\n        print(\"PHASE_ENUM:  \", PHASE_ENUM)\n        sys.exit(1)\n    print(\"Phase table matches code.\")\n\ndef main():\n    lint_dev_process_phases()\n    print(\"All lint checks passed.\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "tools/module_contexts.py": "\nimport os\nimport json\nimport ast\nimport re\n\nEXCLUDES = {'archive', 'temp', 'code_payloads', '.git', '.pytest_cache', '__pycache__'}\nROOT = os.getcwd()\nCONTEXT_JSON = \"module_contexts.json\"\n\nDEFAULT_CONTEXT = dict(\n    purpose=\"\",\n    public_api=[],\n    depends_on=[],\n    used_by=[],\n    direct_imports=[],\n    related_schemas=[],\n    context_window_expected=\"\",\n    escalation_review=\"\",\n)\n\n# future context-aware tasks (skeleton) -- algorithm to select context:\n# seed = set(target_modules)\n# context = []\n# budget = max_tokens\n# frontier = seed\n# visited = set()\n\n# while frontier and budget > 0:\n#     next_frontier = set()\n#     for mod in frontier:\n#         if mod in visited:\n#             continue\n#         snippet = module_contexts[mod][\"source\"]     # or summarised source\n#         tok = token_estimate(snippet)\n#         if tok > budget:\n#             break   # budget exhausted\n#         context.append(snippet)\n#         budget -= tok\n#         visited.add(mod)\n#         next_frontier |= set(module_contexts[mod][\"imports\"])\n#     frontier = next_frontier - visited\n# return \"\\n\\n\".join(context)\n\ndef relpath(path):\n    return os.path.relpath(path, ROOT).replace(os.sep, \"/\")\n\ndef get_module_import_path(rel_path):\n    # \"cadence/dev/executor.py\" -> \"cadence.dev.executor\"\n    p = rel_path\n    if p.endswith(\".py\"):\n        p = p[:-3]\n    if p.endswith(\"/__init__\"):\n        p = p[:-9]\n    return p.replace(\"/\", \".\")\n\ndef extract_and_strip_shebang_and_futures(lines):\n    shebang = None\n    futures = []\n    body = []\n    for line in lines:\n        if shebang is None and line.startswith(\"#!\"):\n            shebang = line\n            continue\n        m = re.match(r\"\\s*from __future__ import\", line)\n        if m:\n            # Avoid duplicates, but preserve order\n            if line not in futures:\n                futures.append(line)\n            continue\n        body.append(line)\n    return shebang, futures, body\n\ndef strip_duplicate_headers_at_top(lines):\n    \"\"\"Remove all context summary header blocks at the file top (before any code).\"\"\"\n    out = []\n    i = 0\n    n = len(lines)\n    while i < n:\n        line = lines[i]\n        # Allow blank lines and comments to stay at top\n        if line.strip() == \"\" or line.strip().startswith(\"#\"):\n            out.append(line)\n            i += 1\n            continue\n        # Remove all context headers at the top\n        if \"# MODULE CONTEXT SUMMARY\" in line:\n            while i < n and \"# END MODULE CONTEXT SUMMARY\" not in lines[i]:\n                i += 1\n            if i < n:\n                i += 1  # Skip END marker\n            # Keep going in case of further headers\n            continue\n        break  # Non-header, non-blank, non-comment: stop removing\n    out.extend(lines[i:])\n    # Remove extra blank lines at the start\n    while len(out) > 1 and out[0].strip() == \"\" and out[1].strip() == \"\":\n        out = out[1:]\n    return out\n\n\ndef find_existing_context(lines):\n    start = None\n    end = None\n    for i, line in enumerate(lines):\n        if \"MODULE CONTEXT SUMMARY\" in line:\n            start = i\n        if start is not None and \"END MODULE CONTEXT SUMMARY\" in line:\n            end = i\n            break\n    return (start, end) if start is not None and end is not None else (None, None)\n\ndef render_pretty_list(lst, indent=4):\n    if not lst:\n        return \"[]\"\n    pad = \" \" * indent\n    return \"[\\n\" + \"\".join(f\"{pad}{repr(x)},\\n\" for x in lst) + \"]\"\n\ndef render_context_block(rel, context):\n    def pretty(key):\n        val = context[key]\n        if isinstance(val, list):\n            return f\"{key}: {render_pretty_list(val)}\"\n        return f'{key}: \"{val}\"' if isinstance(val, str) else f\"{key}: {val}\"\n\n    lines = [\n        '\"\"\"# MODULE CONTEXT SUMMARY',\n        f'filepath: {rel}',\n        pretty(\"purpose\"),\n        pretty(\"public_api\"),\n        pretty(\"depends_on\"),\n        pretty(\"used_by\"),\n        pretty(\"direct_imports\"),\n        pretty(\"related_schemas\"),\n        pretty(\"context_window_expected\"),\n        pretty(\"escalation_review\"),\n        '# END MODULE CONTEXT SUMMARY\"\"\"',\n        ''\n    ]\n    return \"\\n\".join(lines) + \"\\n\"\n\ndef load_all_contexts():\n    if os.path.exists(CONTEXT_JSON):\n        with open(CONTEXT_JSON, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    else:\n        return {}\n\ndef write_all_contexts(contexts):\n    with open(CONTEXT_JSON, \"w\", encoding=\"utf-8\") as f:\n        json.dump(contexts, f, indent=2, ensure_ascii=False)\n\ndef scan_python_modules():\n    for dirpath, dirnames, filenames in os.walk(ROOT):\n        dirnames[:] = [d for d in dirnames if d not in EXCLUDES]\n        for fname in filenames:\n            if fname.endswith(\".py\") and fname not in EXCLUDES:\n                abspath = os.path.join(dirpath, fname)\n                yield relpath(abspath), abspath\n\ndef scan_all_internal_modules(root_dir):\n    internal = set()\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        for fname in filenames:\n            if fname.endswith(\".py\"):\n                abs_path = os.path.join(dirpath, fname)\n                rel = os.path.relpath(abs_path, root_dir).replace(os.sep, \"/\")\n                mod_path = get_module_import_path(rel)\n                internal.add(mod_path)\n    return internal\n\ndef parse_module(path, rel_path, all_internal_modules):\n    \"\"\"Returns (public_api, depends_on, direct_imports) for a python module.\n       - public_api: list of fully qualified names for top-level defs/classes in this file\n       - depends_on: internal modules imported (as import paths)\n       - direct_imports: all directly imported packages/modules (raw names, incl. external)\n    \"\"\"\n    public_api = []\n    depends_on = set()\n    direct_imports = set()\n\n    module_import_path = get_module_import_path(rel_path)\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            node = ast.parse(f.read(), filename=path)\n    except Exception:\n        return public_api, depends_on, direct_imports\n\n    # Top-level functions/classes\n    for n in node.body:\n        if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            public_api.append(f\"{module_import_path}.{n.name}\")\n\n    # Imports\n    for n in ast.walk(node):\n        if isinstance(n, ast.Import):\n            for alias in n.names:\n                direct_imports.add(alias.name.split(\".\")[0])\n        elif isinstance(n, ast.ImportFrom):\n            mod = n.module\n            if mod:\n                mod_path = mod.replace(\".\", \"/\") + \".py\"\n                mod_import_path = mod.replace(\"/\", \".\")\n                direct_imports.add(mod.split(\".\")[0])\n                # Internal module dependency as import path (e.g. cadence.dev.executor)\n                if mod_import_path in all_internal_modules:\n                    depends_on.add(mod_import_path)\n    return sorted(public_api), sorted(depends_on), sorted(direct_imports)\n\ndef sync_contexts():\n    all_internal_modules = scan_all_internal_modules(ROOT)\n    all_contexts = load_all_contexts()\n    updated_contexts = {}\n    modified = 0\n    for rel, abspath in scan_python_modules():\n        context = dict(DEFAULT_CONTEXT)\n        context.update(all_contexts.get(rel, {}))\n        context['filepath'] = rel\n        public_api, depends_on, direct_imports = parse_module(abspath, rel, all_internal_modules)\n        context['public_api'] = public_api\n        context['depends_on'] = depends_on\n        context['direct_imports'] = direct_imports\n        with open(abspath, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        # Extract and remove all shebang/future imports anywhere in the file\n        shebang, futures, lines_no_shebang = extract_and_strip_shebang_and_futures(lines)\n        # Remove all context header blocks at the top\n        code_body = strip_duplicate_headers_at_top(lines_no_shebang)\n        block = render_context_block(rel, context)\n        new_lines = []\n        if shebang:\n            new_lines.append(shebang)\n        if futures:\n            new_lines.extend(futures)\n        new_lines.append(block)\n        new_lines.extend(code_body)\n        # Ensure only one blank line after header\n        i = 1\n        while i < len(new_lines) and new_lines[i].strip() == \"\":\n            i += 1\n        if i > 2:\n            new_lines = [new_lines[0], \"\\n\"] + new_lines[i:]\n        with open(abspath, \"w\", encoding=\"utf-8\") as f:\n            f.writelines(new_lines)\n        updated_contexts[rel] = context\n        modified += 1\n    write_all_contexts(updated_contexts)\n    print(f\"Updated {modified} file(s) and wrote {CONTEXT_JSON}.\")\n\n\n\ndef print_context(module):\n    contexts = load_all_contexts()\n    ctx = contexts.get(module)\n    if not ctx:\n        print(f\"No context found for {module}\")\n        return\n    for k, v in ctx.items():\n        print(f\"{k}: {v}\")\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) == 2 and sys.argv[1] == \"sync\":\n        sync_contexts()\n    elif len(sys.argv) == 3 and sys.argv[1] == \"show\":\n        print_context(sys.argv[2])\n    else:\n        print(\"Usage:\")\n        print(\"  python module_context.py sync         # Update headers and JSON for all modules\")\n        print(\"  python module_context.py show path/to/module.py   # Print context for a module\")\n"
}