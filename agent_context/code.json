{
  "src/cadence/__init__.py": "\n",
  "src/cadence/llm/json_call.py": "# src/cadence/llm/json_call.py\n\"\"\"\nLLMJsonCaller – ask the model for strictly-typed JSON via function-calling.\nRetries automatically on validation failure and normalises legacy shapes.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport re\nimport time\nfrom typing import Any, Dict\n\nimport jsonschema\n\nfrom cadence.llm.client import get_default_client\nfrom cadence.dev.schema import CHANGE_SET_V1\n\nlogger = logging.getLogger(\"cadence.llm.json_call\")\nif not logger.handlers:\n    logger.addHandler(logging.StreamHandler())\nlogger.setLevel(logging.INFO)\n\n_MAX_RETRIES = 3\n\n\nclass LLMJsonCaller:\n    def __init__(self, *, schema: Dict = CHANGE_SET_V1, model: str | None = None):\n        self.schema = schema\n        self.model = model\n        self.llm = get_default_client()\n\n        self.func_spec = [\n            {\n                \"name\": \"create_change_set\",\n                \"description\": \"Return a ChangeSet that implements the blueprint\",\n                \"parameters\": self.schema,\n            }\n        ]\n\n    # ------------------------------------------------------------------ #\n    def ask(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:\n        # Off-line / CI guard – bail out immediately\n        if getattr(self.llm, \"stub\", False):\n            raise RuntimeError(\"LLM unavailable — stub-mode\")\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ]\n\n        for attempt in range(1, _MAX_RETRIES + 1):\n            resp = self.llm.call(\n                messages,\n                model=self.model,\n                json_mode=True,\n                function_spec=self.func_spec,\n            )\n\n            try:\n                # resp may be str *or* dict (when tool-call path chosen)\n                obj = resp if isinstance(resp, dict) else _parse_json(resp)\n                obj = _normalise_legacy(obj)\n                jsonschema.validate(obj, self.schema)\n                return obj\n\n            except Exception as exc:  # noqa: BLE001\n                logger.warning(\n                    \"JSON validation failed (%d/%d): %s\", attempt, _MAX_RETRIES, exc\n                )\n\n                # When parsing/validation fails, fall back to the raw response\n                assistant_output = (\n                    resp if isinstance(resp, str) else json.dumps(resp)\n                )[:4000]\n\n                # Inject the invalid output so the model can self-correct\n                messages.append({\"role\": \"assistant\", \"content\": assistant_output})\n                messages.append(\n                    {\n                        \"role\": \"user\",\n                        \"content\": (\n                            \"The JSON object is invalid. \"\n                            \"Return ONLY a corrected JSON object.\"\n                        ),\n                    }\n                )\n                time.sleep(1)\n\n        raise RuntimeError(\"LLM gave invalid JSON after multiple retries\")\n\n\n# --------------------------------------------------------------------------- #\n# helpers\n# --------------------------------------------------------------------------- #\ndef _parse_json(text: str) -> Dict[str, Any]:\n    \"\"\"\n    If OpenAI response_format works, `text` is already pure JSON.\n    Guard against accidental fencing.\n    \"\"\"\n    if text.strip().startswith(\"```\"):\n        m = re.search(r\"```json\\s*([\\s\\S]*?)```\", text, re.I)\n        if not m:\n            raise ValueError(\"Could not locate fenced JSON block\")\n        text = m.group(1)\n    return json.loads(text)\n\n\ndef _normalise_legacy(obj: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Accept LLM output that uses {\"changes\":[…]} instead of {\"edits\":[…]}.\n    \"\"\"\n    if \"changes\" in obj and \"edits\" not in obj:\n        obj[\"edits\"] = [\n            {\n                \"path\": c.get(\"file\") or c.get(\"path\"),\n                \"mode\": c.get(\"mode\", \"modify\"),\n                \"after\": c.get(\"after\"),\n                \"before_sha\": c.get(\"before_sha\"),\n            }\n            for c in obj[\"changes\"]\n        ]\n        obj.pop(\"changes\")\n    return obj",
  "src/cadence/llm/client.py": "# src/cadence/llm/client.py\nfrom __future__ import annotations\n\nimport os, logging, time\nfrom typing import List, Dict, Any, Optional, cast\n\nfrom openai import AsyncOpenAI, OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom dotenv import load_dotenv\nimport tiktoken\n\n# one-time env expansion\nload_dotenv()\n\nlogger = logging.getLogger(\"cadence.llm.client\")\nif not logger.handlers:\n    h = logging.StreamHandler()\n    h.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\"))\n    logger.addHandler(h)\nlogger.setLevel(logging.INFO)\n\n_DEFAULT_MODELS = {\n    \"reasoning\": \"o3-2025-04-16\",\n    \"execution\": \"gpt-4.1\",\n    \"efficiency\": \"o4-mini\",\n}\n\n\ndef _count_tokens(model: str, messages: List[Dict[str, str]]) -> int:\n    enc = tiktoken.get_encoding(\"o200k_base\")\n    return sum(len(enc.encode(m[\"role\"])) + len(enc.encode(m[\"content\"])) for m in messages)\n\n\nclass LLMClient:\n    \"\"\"\n    Central sync/async wrapper with:\n\n    • stub-mode when no API key\n    • optional json_mode   → OpenAI “response_format={type:json_object}”\n    • optional function_spec → OpenAI “tools=[…]”\n    \"\"\"\n\n    _warned_stub = False\n\n    def __init__(\n        self,\n        *,\n        api_key: Optional[str] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        default_model: Optional[str] = None,\n    ):\n        key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        self.stub = not bool(key)\n        self.api_key = key\n        self.api_base = api_base or os.getenv(\"OPENAI_API_BASE\")\n        self.api_version = api_version or os.getenv(\"OPENAI_API_VERSION\")\n        self.default_model = default_model or _DEFAULT_MODELS[\"execution\"]\n\n        if self.stub:\n            if not LLMClient._warned_stub:\n                logger.warning(\n                    \"[Cadence] LLMClient stub-mode — OPENAI_API_KEY missing; \"\n                    \".call()/ .acall() return canned message.\"\n                )\n                LLMClient._warned_stub = True\n            self._sync_client = None\n            self._async_client = None\n        else:\n            try:\n                self._sync_client  = OpenAI(api_key=self.api_key,\n                                            base_url=self.api_base)\n                self._async_client = AsyncOpenAI(api_key=self.api_key,\n                                                 base_url=self.api_base)\n                # If the test-suite monkey-patched OpenAI to a stub that\n                # returns None we must still fall back to stub-mode.\n                if self._sync_client is None or not hasattr(self._sync_client,\n                                                            \"chat\"):\n                    raise AttributeError\n            except Exception:                      # noqa: BLE001\n                self.stub = True\n                self._sync_client = self._async_client = None\n                if not LLMClient._warned_stub:\n                    logger.warning(\"[Cadence] LLMClient stub-mode (auto)\")\n                    LLMClient._warned_stub = True\n\n    # ------------------------------------------------------------------ #\n    def _resolve_model(self, model: Optional[str], agent_type: Optional[str]) -> str:\n        if model:\n            return model\n        if agent_type and agent_type in _DEFAULT_MODELS:\n            return _DEFAULT_MODELS[agent_type]\n        return self.default_model\n\n    # ------------------------------------------------------------------ #\n    def call(\n        self,\n        messages: List[Dict[str, Any]],\n        *,\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        json_mode: bool = False,\n        function_spec: Optional[List[Dict[str, Any]]] = None,\n        **kwargs,\n    ) -> str:\n        if self.stub:\n            return \"LLM unavailable — Cadence stub-mode\"\n\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        prompt_tokens = _count_tokens(used_model, msgs)\n        t0 = time.perf_counter()\n\n        # -- wrap tools if present --------------------------------------\n        tools_arg = None\n        tool_choice_arg = None\n        if function_spec:\n            tools_arg = [{\"type\": \"function\", \"function\": fs}\n                         for fs in function_spec]\n            tool_choice_arg = {\n                \"type\": \"function\",\n                \"function\": {               # <- nest correctly\n                    \"name\": function_spec[0][\"name\"]\n                }\n            }\n\n        response = self._sync_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            # Never send response_format if we are already in tool-call mode\n            response_format=None if function_spec else (\n                {\"type\": \"json_object\"} if json_mode else None\n            ),\n            tools=tools_arg,\n            tool_choice=tool_choice_arg,\n            **kwargs,\n        )\n\n        # ------------------------------------------------------------ #\n        # OpenAI mutually-excludes  “tools=…”   and   “response_format”.\n        # If we supplied  tools=function_spec, the assistant returns\n        # the result in   message.tool_calls[0].function.arguments\n        # and leaves   message.content == None.\n        # ------------------------------------------------------------ #\n        if response.choices[0].message.content is None and response.choices[0].message.tool_calls:\n            # We requested exactly ONE function; grab its arguments.\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        else:\n            content = (response.choices[0].message.content or \"\").strip()\n\n        logger.info(\n            \"LLM call %s → %.2fs  prompt≈%d  completion≈%d\",\n            used_model,\n            time.perf_counter() - t0,\n            prompt_tokens,\n            len(content) // 4,\n        )\n        return content\n\n    # async version (rarely used by Cadence core)\n    async def acall(\n        self,\n        messages: List[Dict[str, Any]],\n        *,\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        json_mode: bool = False,\n        function_spec: Optional[List[Dict[str, Any]]] = None,\n        **kwargs,\n    ) -> str:\n        if self.stub:\n            return \"LLM unavailable — Cadence stub-mode\"\n\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        prompt_tokens = _count_tokens(used_model, msgs)\n        t0 = time.perf_counter()\n\n        # -- wrap tools if present --------------------------------------\n        tools_arg = None\n        tool_choice_arg = None\n        if function_spec:\n            tools_arg = [{\"type\": \"function\", \"function\": fs}\n                         for fs in function_spec]\n            tool_choice_arg = {\n                \"type\": \"function\",\n                \"function\": {               # <- nest correctly\n                    \"name\": function_spec[0][\"name\"]\n                }\n            }\n\n        response = await self._async_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            # Never send response_format if we are already in tool-call mode\n            response_format=None if function_spec else (\n                {\"type\": \"json_object\"} if json_mode else None\n            ),\n            tools=tools_arg,\n            tool_choice=tool_choice_arg,\n            **kwargs,\n        )\n\n        # ------------------------------------------------------------ #\n        # OpenAI mutually-excludes  “tools=…”   and   “response_format”.\n        # If we supplied  tools=function_spec, the assistant returns\n        # the result in   message.tool_calls[0].function.arguments\n        # and leaves   message.content == None.\n        # ------------------------------------------------------------ #\n        if response.choices[0].message.content is None and response.choices[0].message.tool_calls:\n            # We requested exactly ONE function; grab its arguments.\n            content = response.choices[0].message.tool_calls[0].function.arguments\n        else:\n            content = (response.choices[0].message.content or \"\").strip()\n\n        logger.info(\n            \"LLM call %s → %.2fs  prompt≈%d  completion≈%d\",\n            used_model,\n            time.perf_counter() - t0,\n            prompt_tokens,\n            len(content) // 4,\n        )\n        return content\n\n\n# helper for callers that want the singleton\ndef get_default_client() -> LLMClient:\n    return _DEFAULT_CLIENT\n\n\n_DEFAULT_CLIENT = LLMClient()\n",
  "src/cadence/llm/__init__.py": "\n",
  "src/cadence/context/provider.py": "# src/cadence/context/provider.py\nimport subprocess, sys, json\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nclass ContextProvider(ABC):\n    @abstractmethod\n    def get_context(self, *roots: Path, exts=(\".py\", \".md\"), max_bytes=50_000) -> str: ...\nclass SnapshotContextProvider(ContextProvider):\n    def get_context(self, *roots, exts=(\".py\", \".md\"), max_bytes=50_000, out=\"-\") -> str:\n        args = [sys.executable, \"tools/collect_code.py\"]\n        for r in roots: args += [\"--root\", str(r)]\n        for e in exts:  args += [\"--ext\", e]\n        args += [\"--max-bytes\", str(max_bytes), \"--out\", out]\n        return subprocess.run(args, capture_output=True, text=True, check=True).stdout\n",
  "src/cadence/context/__init__.py": "",
  "src/cadence/context/select.py": "# src/cadence/context/select.py\n\ndef select_context(target_paths: list[str], max_tokens: int = 50_000) -> str:\n    \"\"\"\n    Return BFS-ranked source blobs whose cumulative size ≤ max_tokens.\n    \"\"\"\n    ...",
  "src/cadence/agents/efficiency.py": "# src/cadence/agents/efficiency.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EFFICIENCY_PROFILE, AgentProfile\n\n\nclass EfficiencyAgent(BaseAgent):\n    \"\"\"\n    Final class: fast, low-cost linting & summarisation.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EFFICIENCY_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/agents/execution.py": "# src/cadence/agents/execution.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EXECUTION_PROFILE, AgentProfile\n\n\nclass ExecutionAgent(BaseAgent):\n    \"\"\"\n    Final class: generates or refactors significant portions of the codebase.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EXECUTION_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/agents/registry.py": "# src/cadence/agents/registry.py\n\"\"\"\nSingle place to obtain a Core Agent or Profile.\n\nAvoids hard-coding classes throughout the codebase.\n\"\"\"\n\nfrom typing import Type\n\nfrom .reasoning import ReasoningAgent\nfrom .execution import ExecutionAgent\nfrom .efficiency import EfficiencyAgent\nfrom .profile import BUILTIN_PROFILES, AgentProfile\n\n_CORE_AGENTS: dict[str, Type] = {\n    \"reasoning\": ReasoningAgent,\n    \"execution\": ExecutionAgent,\n    \"efficiency\": EfficiencyAgent,\n}\n\n\ndef get_agent(agent_type: str, **kwargs):\n    \"\"\"\n    Instantiate a Core Agent by `agent_type`.\n\n    Example:\n        agent = get_agent(\"execution\")\n    \"\"\"\n    if agent_type not in _CORE_AGENTS:\n        raise ValueError(f\"Unknown agent_type '{agent_type}'. Valid: {list(_CORE_AGENTS)}\")\n    return _CORE_AGENTS[agent_type](**kwargs)\n\n\ndef get_profile(profile_name: str) -> AgentProfile:\n    if profile_name not in BUILTIN_PROFILES:\n        raise ValueError(f\"Unknown profile '{profile_name}'. Valid: {list(BUILTIN_PROFILES)}\")\n    return BUILTIN_PROFILES[profile_name]",
  "src/cadence/agents/profile.py": "# src/cadence/agents/profile.py\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any\n\n\n@dataclass(frozen=True, slots=True)\nclass AgentProfile:\n    \"\"\"\n    Immutable definition of an agent’s operational contract.\n\n    Nothing here executes code; it is pure data that can be validated,\n    serialised, or inspected by the Meta-agent and CI tooling.\n    \"\"\"\n    name: str\n    role: str\n    model: str\n    context_limit: int\n    review_policy: str = \"\"\n    default_system_prompt: str = \"\"\n    extra: Dict[str, Any] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n# Canonical profiles – these are the ONLY ones that Core Agents will default to\n# --------------------------------------------------------------------------- #\nREASONING_PROFILE = AgentProfile(\n    name=\"reasoning\",\n    role=\"plan-review\",\n    model=\"o3-2025-04-16\",\n    context_limit=200_000,\n    review_policy=\"Cannot commit code; must review Execution diff\",\n)\n\nEXECUTION_PROFILE = AgentProfile(\n    name=\"execution\",\n    role=\"implement\",\n    model=\"gpt-4.1\",\n    context_limit=1_000_000,\n    review_policy=\"Needs review by Reasoning or Efficiency\",\n)\n\nEFFICIENCY_PROFILE = AgentProfile(\n    name=\"efficiency\",\n    role=\"lint-summarise\",\n    model=\"o4-mini\",\n    context_limit=200_000,\n    review_policy=\"Reviews Execution unless diff is non-code\",\n)\n\n# Convenience lookup\nBUILTIN_PROFILES = {\n    \"reasoning\": REASONING_PROFILE,\n    \"execution\": EXECUTION_PROFILE,\n    \"efficiency\": EFFICIENCY_PROFILE,\n}",
  "src/cadence/agents/__init__.py": "\n",
  "src/cadence/agents/sidekick.py": "# src/cadence/agents/sidekick.py\n\"\"\"\nPersona agent that *delegates* to a ReasoningAgent but presents a\nhuman-centric mentor/advisor interface.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nfrom .profile import AgentProfile, REASONING_PROFILE\nfrom .reasoning import ReasoningAgent\n\n\n_SIDEKICK_PROMPT = \"\"\"\nYou are an AI-enhanced co-developer and mentor. Your primary goal is to\nextract the most creative, high-leverage ideas from the human user and\ntransform them into actionable improvements for the Cadence platform.\nAvoid tactical implementation details unless asked; focus on vision,\narchitecture, and pragmatic next steps.\n\"\"\"\n\n\nclass Sidekick:\n    \"\"\"\n    Thin wrapper: exposes `run_interaction` but delegates work to an\n    internal ReasoningAgent instance configured with a custom prompt.\n    \"\"\"\n\n    def __init__(self):\n        profile = AgentProfile(\n            name=\"sidekick\",\n            role=\"advisor\",\n            model=REASONING_PROFILE.model,\n            context_limit=REASONING_PROFILE.context_limit,\n            review_policy=REASONING_PROFILE.review_policy,\n            default_system_prompt=REASONING_PROFILE.default_system_prompt,\n            extra=REASONING_PROFILE.extra.copy() if REASONING_PROFILE.extra else {},\n        )\n        self._agent = ReasoningAgent(profile=profile, system_prompt=_SIDEKICK_PROMPT)\n        self._inject_seed_context()\n\n    # ------------------------------------------------------------------ #\n    # Public façade\n    # ------------------------------------------------------------------ #\n    def run_interaction(self, user_input: str, **kwargs) -> str:\n        return self._agent.run_interaction(user_input, **kwargs)\n\n    async def async_run_interaction(self, user_input: str, **kwargs) -> str:\n        return await self._agent.async_run_interaction(user_input, **kwargs)\n\n    # ------------------------------------------------------------------ #\n    # Private helpers\n    # ------------------------------------------------------------------ #\n    def _inject_seed_context(self):\n        docs = self._agent.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n\n        modules_path = Path(\"agent_context/module_contexts.json\")\n        modules = {}\n        if modules_path.exists():\n            modules = json.loads(modules_path.read_text())\n\n        self._agent.append_message(\n            \"user\",\n            f\"DOCS:\\n{docs}\\n---\\nMODULE_CONTEXTS:\\n{json.dumps(modules)[:10_000]}\",\n        )",
  "src/cadence/agents/reasoning.py": "# src/cadence/agents/reasoning.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import REASONING_PROFILE, AgentProfile\n\n\nclass ReasoningAgent(BaseAgent):\n    \"\"\"\n    Final class: provides deep, chain-of-thought reasoning and architectural review.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = REASONING_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)\n\n    # Automatically inject a fresh code snapshot on each reset\n    def reset_context(self, system_prompt: str | None = None):\n        super().reset_context(system_prompt)\n        docs = self.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n        self.append_message(\"user\", f\"REFERENCE_DOCUMENTS:\\n{docs}\\n---\\nYou are cleared for deep reasoning.\")",
  "src/cadence/agents/base.py": "# src/cadence/agents/base.py\nfrom __future__ import annotations\n\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\n\nfrom src.cadence.llm.client import LLMClient, get_default_client\nfrom src.cadence.context.provider import ContextProvider, SnapshotContextProvider\nfrom .profile import AgentProfile\n\n\nclass BaseAgent:\n    \"\"\"\n    The one true superclass for *all* Cadence agents.\n\n    An agent = (profile) + (conversation state) + (LLM client) [+ optional helpers]\n\n    Subclasses SHOULD NOT hard-code models; they inherit that from the supplied\n    `AgentProfile`.  Core agents (Reasoning / Execution / Efficiency) simply\n    pass the canonical profile; personas may inject a custom one.\n    \"\"\"\n\n    def __init__(\n        self,\n        profile: AgentProfile,\n        *,\n        llm_client: Optional[LLMClient] = None,\n        system_prompt: Optional[str] = None,\n        context_provider: Optional[ContextProvider] = None,\n    ):\n        self.profile = profile\n        self.llm_client = llm_client or get_default_client()\n        self.system_prompt = system_prompt or profile.default_system_prompt\n        self.context_provider = context_provider or SnapshotContextProvider()\n        self.messages: List[Dict[str, Any]] = []\n        self.reset_context()\n\n    # --------------------------------------------------------------------- #\n    # Conversation helpers\n    # --------------------------------------------------------------------- #\n    def reset_context(self, system_prompt: Optional[str] = None):\n        \"\"\"Clear history and (re)set the system prompt.\"\"\"\n        self.messages = []\n        sys_prompt = system_prompt or self.system_prompt\n        if sys_prompt:\n            self.append_message(\"system\", sys_prompt)\n\n    def append_message(self, role: str, content: str):\n        self.messages.append({\"role\": role, \"content\": content})\n\n    # --------------------------------------------------------------------- #\n    # LLM calls\n    # --------------------------------------------------------------------- #\n    def run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        response = self.llm_client.call(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,  # already injected\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        return response\n\n    async def async_run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        response = await self.llm_client.acall(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        return response\n\n    # --------------------------------------------------------------------- #\n    # Persistence\n    # --------------------------------------------------------------------- #\n    def save_history(self, path: str):\n        import json\n        Path(path).write_text(json.dumps(self.messages, indent=2, ensure_ascii=False))\n\n    def load_history(self, path: str):\n        import json\n        self.messages = json.loads(Path(path).read_text())\n\n    # --------------------------------------------------------------------- #\n    # Context helpers\n    # --------------------------------------------------------------------- #\n    def gather_codebase_context(\n        self,\n        root: Tuple[str, ...] = (\"cadence\", \"docs\"),\n        ext: Tuple[str, ...] = (\".py\", \".md\", \".json\", \".mermaid\"),\n        **kwargs,\n    ) -> str:\n        \"\"\"Return repo/docs snapshot via the injected ContextProvider.\"\"\"\n        return self.context_provider.get_context(*(Path(r) for r in root), exts=ext, **kwargs)\n",
  "src/cadence/utils/add.py": "def add(x: int, y: int) -> int:\n    \"\"\"Intentionally wrong implementation for MVP red→green demo.\"\"\"\n    return x + y\n",
  "src/cadence/utils/mvp_loop.py": "# src/cadence/utils/mvp_loop.py\n\nimport pytest\nfrom src.cadence.dev.executor import TaskExecutor\nfrom src.cadence.dev.shell import ShellRunner\n\ndef manual_test():\n    result = pytest.main([\"tests\"])\n    if result != 0:\n        print(\"Tests failed.\")\n        # Read before\n        before = open(\"cadence/utils/add.py\").read()\n        print(\"Paste fixed implementation for utils/add.py below. End input with EOF (Ctrl+D):\")\n        after = []\n        try:\n            while True:\n                after.append(input())\n        except EOFError:\n            pass\n        after = \"\\n\".join(after)\n        # build diff\n        task = {\"diff\": {\"file\": \"cadence/utils/add.py\", \"before\": before, \"after\": after}}\n        patch = TaskExecutor(\"cadence/utils\").build_patch(task)\n        print(\"---Proposed Diff---\")\n        print(patch)\n\ndef OOP_test():\n    executor = TaskExecutor(src_root=\".\")\n    shell = ShellRunner(repo_dir=\".\")\n\n    # Dynamically read and patch the file\n    with open(\"cadence/utils/add.py\") as f:\n        before = f.read()\n    if \"return x + y\" not in before:\n        after = before.replace(\"return x - 1 + y\", \"return x + y\")\n    else:\n        print(\"Already correct: no patch needed.\")\n        return\n\n    task = {\n        \"diff\": {\n            \"file\": \"cadence/utils/add.py\",\n            \"before\": before,\n            \"after\": after\n        }\n    }\n\n    patch = executor.build_patch(task)\n    try:\n        shell.git_apply(patch)\n        # Run tests via ShellRunner\n        result = shell.run_pytest()\n        if result[\"success\"]:\n            sha = shell.git_commit(\"Fix add(): correct return expression\")\n            print(f\"Patch applied and tests passed. Commit SHA: {sha}\")\n        else:\n            print(\"Tests failed after patch:\\n\", result[\"output\"])\n    except Exception as e:\n        print(\"Patch failed:\", e)\n\n\n\nif __name__ == \"__main__\":\n    OOP_test()",
  "src/cadence/dev/command_center.py": "\n# src/cadence/dev/command_center.py\n\nimport streamlit as st\n\n# You may need to adjust the import path according to your setup\nfrom src.cadence.dev.orchestrator import DevOrchestrator\n\n# ---- Basic Config (map to your dev environment) ----\nCONFIG = dict(\n    backlog_path=\"dev_backlog.json\",\n    template_file=\"dev_templates.json\",\n    src_root=\"cadence\",\n    ruleset_file=None,\n    repo_dir=\".\",\n    record_file=\"dev_record.json\"\n)\norch = DevOrchestrator(CONFIG)\n\n# ---- Session State Initialization ----\nif \"selected_task_id\" not in st.session_state:\n    st.session_state[\"selected_task_id\"] = None\nif \"phase\" not in st.session_state:\n    st.session_state[\"phase\"] = \"Backlog\"\n\n# ---- Sidebar: Phase Navigation ----\nst.sidebar.title(\"Cadence Dev Center\")\nphase = st.sidebar.radio(\n    \"Workflow phase\",\n    [\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"],\n    index=[\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"].index(st.session_state[\"phase\"])\n)\nst.session_state[\"phase\"] = phase\n\n# ---- Main: Backlog View ----\nif phase == \"Backlog\":\n    st.title(\"Task Backlog\")\n    open_tasks = orch.backlog.list_items(status=\"open\")\n    if not open_tasks:\n        st.info(\"No open tasks! Add tasks via CLI/Notebook.\")\n    else:\n        import pandas as pd\n        df = pd.DataFrame(open_tasks)\n        st.dataframe(df[[\"id\", \"title\", \"type\", \"status\", \"created_at\"]])\n        selected = st.selectbox(\n            \"Select a task to work on\",\n            options=[t[\"id\"] for t in open_tasks],\n            format_func=lambda tid: f'{tid[:8]}: {next(t[\"title\"] for t in open_tasks if t[\"id\"] == tid)}'\n        )\n        if st.button(\"Continue to task detail\"):\n            st.session_state[\"selected_task_id\"] = selected\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Task Detail View ----\nelif phase == \"Task Detail\":\n    st.title(\"Task Details\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    st.markdown(f\"**Title:** {task['title']}\\n\\n**Type:** {task['type']}\\n\\n**Status:** {task['status']}\\n\\n**Created:** {task['created_at']}\")\n    st.code(task.get(\"description\", \"\"), language=\"markdown\")\n    st.json(task)\n    if st.button(\"Proceed to Patch Review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()\n\n# ---- Patch Review ----\nelif phase == \"Patch Review\":\n    st.title(\"Patch Review & Approval\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    try:\n        patch = orch.executor.build_patch(task)\n        st.code(patch, language=\"diff\")\n        review = orch.reviewer.review_patch(patch, context=task)\n        st.markdown(\"### Review Comments\")\n        st.markdown(review[\"comments\"] or \"_No issues detected._\")\n        if review[\"pass\"]:\n            if st.button(\"Approve and Apply Patch\"):\n                # Apply patch, save, and proceed\n                orch.shell.git_apply(patch)\n                orch._record(task, \"patch_applied\")\n                st.success(\"Patch applied.\")\n                st.session_state[\"phase\"] = \"Run Test\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Patch failed review; please revise before continuing.\")\n            if st.button(\"Back to task detail\"):\n                st.session_state[\"phase\"] = \"Task Detail\"\n                st.experimental_rerun()\n    except Exception as ex:\n        st.error(f\"Patch build/review failed: {ex}\")\n        if st.button(\"Back to task detail\"):\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Run Test ----\nelif phase == \"Run Test\":\n    st.title(\"Run Pytest\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    st.markdown(\"Apply code patch complete. Run tests to confirm correctness.\")\n    if st.button(\"Run tests now\"):\n        test_result = orch.shell.run_pytest()\n        st.text_area(\"Test Output\", test_result[\"output\"], height=200)\n        if test_result[\"success\"]:\n            st.success(\"Tests passed!\")\n            if st.button(\"Proceed to Archive/Done\"):\n                # Commit and archive task\n                task = orch.backlog.get_item(task_id)\n                sha = orch.shell.git_commit(f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\")\n                orch.backlog.update_item(task_id, {\"status\": \"done\"})\n                orch.backlog.archive_completed()\n                # commit snapshot (task is still 'done' here)\n                orch.record.save(task, state=\"committed\", extra={\"commit_sha\": sha})\n                # refresh snapshot so we accurately log 'archived'\n                updated_task = orch.backlog.get_item(task_id)\n                orch.record.save(updated_task, state=\"archived\", extra={})\n                st.session_state[\"phase\"] = \"Archive\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Tests failed, fix required before progressing.\")\n    if st.button(\"Back to patch review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n\n# ---- Archive / Task Complete ----\nelif phase == \"Archive\":\n    st.title(\"Task Archived\")\n    st.success(\"Task flow completed. You may return to the backlog.\")\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"selected_task_id\"] = None\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()",
  "src/cadence/dev/change_set.py": "# src/cadence/dev/change_set.py\n\"\"\"\nStructured representation of a code change.\n\nExecution-agents (LLMs or humans) now produce **ChangeSet** JSON instead of\nhand-written diffs.  A single PatchBuilder later converts the ChangeSet into a\nvalid git patch, eliminating fragile string-diff manipulation.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field, asdict\nfrom pathlib import Path\nfrom typing import List, Optional, Dict, Any\nimport json\nimport hashlib\n\n\n# --------------------------------------------------------------------------- #\n# Dataclasses\n# --------------------------------------------------------------------------- #\n@dataclass(slots=True)\nclass FileEdit:\n    \"\"\"\n    One logical modification to a file.\n\n    • `path`  – repository-relative path using POSIX slashes.\n    • `after` – full new file contents (None for deletions).\n    • `before_sha` – optional SHA-1 of the *current* file to protect\n                     against stale edits; raise if it no longer matches.\n    • `mode` –  \"add\" | \"modify\" | \"delete\"\n    \"\"\"\n\n    path: str\n    after: Optional[str] = None\n    before_sha: Optional[str] = None\n    mode: str = \"modify\"\n\n    # --- helpers --------------------------------------------------------- #\n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n\n    @staticmethod\n    def from_dict(obj: Dict[str, Any]) -> \"FileEdit\":\n        content = obj.get(\"after\")\n        if content is None and obj.get(\"after_file\"):\n            content = Path(obj[\"after_file\"]).read_text(encoding=\"utf-8\")\n        return FileEdit(\n            path=obj[\"path\"],\n            after=content,\n            before_sha=obj.get(\"before_sha\"),\n            mode=obj.get(\"mode\", \"modify\"),\n        )\n\n\n@dataclass(slots=True)\nclass ChangeSet:\n    \"\"\"\n    A collection of FileEdits plus commit metadata.\n    \"\"\"\n\n    edits: List[FileEdit] = field(default_factory=list)\n    message: str = \"\"\n    author: str = \"\"\n    meta: Dict[str, Any] = field(default_factory=dict)\n\n    # --- helpers --------------------------------------------------------- #\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"edits\": [e.to_dict() for e in self.edits],\n            \"message\": self.message,\n            \"author\": self.author,\n            \"meta\": self.meta,\n        }\n\n    @staticmethod\n    def from_dict(obj: Dict[str, Any]) -> \"ChangeSet\":\n        return ChangeSet(\n            edits=[FileEdit.from_dict(ed) for ed in obj.get(\"edits\", [])],\n            message=obj.get(\"message\", \"\"),\n            author=obj.get(\"author\", \"\"),\n            meta=obj.get(\"meta\", {}),\n        )\n\n    # Convenient JSON helpers -------------------------------------------- #\n    def to_json(self, *, indent: int | None = 2) -> str:\n        return json.dumps(self.to_dict(), indent=indent, ensure_ascii=False)\n\n    @staticmethod\n    def from_json(text: str | bytes) -> \"ChangeSet\":\n        return ChangeSet.from_dict(json.loads(text))\n\n    # -------------------------------------------------------------------- #\n    # Validation helpers\n    # -------------------------------------------------------------------- #\n    def validate_against_repo(self, repo_path: Path) -> None:\n        \"\"\"\n        Raises RuntimeError if any `before_sha` no longer matches current file.\n        \"\"\"\n        for e in self.edits:\n            if e.before_sha:\n                file_path = repo_path / e.path\n                if not file_path.exists():\n                    raise RuntimeError(f\"{e.path} missing – SHA check impossible.\")\n                sha = _sha1_of_file(file_path)\n                if sha != e.before_sha:\n                    raise RuntimeError(\n                        f\"{e.path} SHA mismatch (expected {e.before_sha}, got {sha})\"\n                    )\n\n\n# --------------------------------------------------------------------------- #\n# Internal helpers\n# --------------------------------------------------------------------------- #\ndef _sha1_of_file(p: Path) -> str:\n    buf = p.read_bytes()\n    return hashlib.sha1(buf).hexdigest()",
  "src/cadence/dev/shell.py": "# src/cadence/dev/shell.py\n\"\"\"\nCadence ShellRunner\n-------------------\n\nAdditions in this revision\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n1. **Phase-order enforcement**\n   • `git_apply`, `run_pytest`, and `git_commit` now cooperate with a\n     lightweight tracker that guarantees commits cannot occur unless a\n     patch has been applied *and* the test suite has passed.\n2. **Patch pre-check**\n   • `git_apply` performs `git apply --check` before mutating the\n     working tree, aborting early if the diff’s *before* image does not\n     match the current file contents.\n\nEnforced invariants\n-------------------\n• patch_applied   – set automatically after a successful `git_apply`\n• tests_passed    – set automatically after a green `run_pytest`\n• committed       – set after `git_commit`\n\nCommit is refused (ShellCommandError) unless **both**\n`patch_applied` *and* `tests_passed` are present for the task.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Optional, Dict, List, Set\n\nfrom .record import TaskRecord\nfrom .phase_guard import enforce_phase, PhaseOrderError\n\n\nclass ShellCommandError(Exception):\n    \"\"\"Raised when a shell/git/pytest command fails.\"\"\"\n\n\nclass ShellRunner:\n    \"\"\"\n    Wrapper around common git / pytest commands **with automatic failure\n    persistence** *and* runtime phase-order guarantees.\n    \"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction / context helpers\n    # ------------------------------------------------------------------ #\n    def __init__(self, repo_dir: str = \".\", *, task_record: TaskRecord | None = None):\n        self.repo_dir = os.path.abspath(repo_dir)\n        if not os.path.isdir(self.repo_dir):\n            raise ValueError(\n                f\"repo_dir '{self.repo_dir}' does not exist or is not a directory.\"\n            )\n\n        # Recording context (may be None for stand-alone usage)\n        self._record: TaskRecord | None = task_record\n        self._current_task: dict | None = None\n\n        # Phase-tracking:  task_id → {phase labels}\n        self._phase_flags: Dict[str, Set[str]] = {}\n\n    # ---- phase-tracking helpers ---------------------------------------\n    def _init_phase_tracking(self, task_id: str) -> None:\n        self._phase_flags.setdefault(task_id, set())\n\n    def _mark_phase(self, task_id: str, phase: str) -> None:\n        self._phase_flags.setdefault(task_id, set()).add(phase)\n\n    def _has_phase(self, task_id: str, phase: str) -> bool:\n        return phase in self._phase_flags.get(task_id, set())\n\n    # ------------------------------------------------------------------ #\n    def attach_task(self, task: dict | None):\n        \"\"\"\n        Attach the *current* task dict so that failures inside any shell\n        call can be persisted and phase order can be enforced.\n        \"\"\"\n        self._current_task = task\n        if task:\n            self._init_phase_tracking(task[\"id\"])\n\n    # ------------------------------------------------------------------ #\n    # Internal helper – persist failure snapshot (best-effort)\n    # ------------------------------------------------------------------ #\n    def _record_failure(\n        self,\n        *,\n        state: str,\n        error: Exception | str,\n        output: str = \"\",\n        cmd: List[str] | None = None,\n    ):\n        if not (self._record and self._current_task):\n            return  # runner used outside orchestrated flow\n        extra = {\"error\": str(error)}\n        if output:\n            extra[\"output\"] = output.strip()\n        if cmd:\n            extra[\"cmd\"] = \" \".join(cmd)\n        try:\n            self._record.save(self._current_task, state=state, extra=extra)\n        except Exception:  # noqa: BLE001 – failure recording must not raise\n            pass\n\n    # ------------------------------------------------------------------ #\n    # Git patch helpers\n    # ------------------------------------------------------------------ #\n    @enforce_phase(mark=\"patch_applied\")\n    def git_apply(self, patch: str, *, reverse: bool = False) -> bool:\n        \"\"\"\n        Apply a unified diff to the working tree *after* ensuring the\n        patch cleanly applies via `git apply --check`.\n        \"\"\"\n        stage = \"git_apply_reverse\" if reverse else \"git_apply\"\n\n        if not patch or not isinstance(patch, str):\n            err = ShellCommandError(\"No patch supplied to apply.\")\n            self._record_failure(state=f\"failed_{stage}\", error=err)\n            raise err\n\n        # Write patch to temporary file\n        with tempfile.NamedTemporaryFile(\n            mode=\"w+\", suffix=\".patch\", delete=False\n        ) as tf:\n            tf.write(patch)\n            tf.flush()\n            tf_path = tf.name\n\n        # --- pre-check --------------------------------------------------\n        check_cmd: List[str] = [\"git\", \"apply\", \"--check\"]\n        if reverse:\n            check_cmd.append(\"-R\")\n        check_cmd.append(tf_path)\n        result = subprocess.run(\n            check_cmd,\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=False,\n        )\n        if result.returncode != 0:\n            err = ShellCommandError(\n                f\"Patch pre-check failed: {result.stderr.strip() or result.stdout.strip()}\"\n            )\n            self._record_failure(\n                state=f\"failed_{stage}\",\n                error=err,\n                output=(result.stderr or result.stdout),\n                cmd=check_cmd,\n            )\n            os.remove(tf_path)\n            raise err\n\n        # --- actual apply ----------------------------------------------\n        cmd: List[str] = [\"git\", \"apply\"]\n        if reverse:\n            cmd.append(\"-R\")\n        cmd.append(tf_path)\n\n        try:\n            result = subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n\n            if result.returncode != 0:\n                raise ShellCommandError(\n                    f\"git apply failed: {result.stderr.strip() or result.stdout.strip()}\"\n                )\n            return True\n\n        except Exception as ex:  # noqa: BLE001 – blanket to ensure capture\n            output = \"\"\n            if \"result\" in locals():\n                output = (result.stdout or \"\") + \"\\n\" + (result.stderr or \"\")\n            self._record_failure(\n                state=f\"failed_{stage}\",\n                error=ex,\n                output=output,\n                cmd=cmd,\n            )\n            raise\n        finally:\n            os.remove(tf_path)\n\n    # ------------------------------------------------------------------ #\n    # Testing helpers\n    # ------------------------------------------------------------------ #\n    def run_pytest(self, test_path: Optional[str] = None) -> Dict:\n        \"\"\"\n        Run pytest on the given path (default: ./tests).\n\n        Success automatically marks the *tests_passed* phase.\n        Returns {'success': bool, 'output': str}\n        \"\"\"\n        stage = \"pytest\"\n        path = test_path or os.path.join(self.repo_dir, \"tests\")\n        if not os.path.exists(path):\n            err = ShellCommandError(f\"Tests path '{path}' does not exist.\")\n            self._record_failure(state=f\"failed_{stage}\", error=err)\n            raise err\n\n        cmd = [\"pytest\", \"-q\", path]\n        try:\n            result = subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n            passed = result.returncode == 0\n            output = (result.stdout or \"\") + \"\\n\" + (result.stderr or \"\")\n\n            if passed and self._current_task:\n                self._mark_phase(self._current_task[\"id\"], \"tests_passed\")\n\n            if not passed:\n                # Persist *test* failure even though we don't raise here\n                self._record_failure(\n                    state=\"failed_pytest\", error=\"pytest failed\", output=output, cmd=cmd\n                )\n            return {\"success\": passed, \"output\": output.strip()}\n\n        except Exception as ex:\n            self._record_failure(state=f\"failed_{stage}\", error=ex)\n            raise\n\n    # ------------------------------------------------------------------ #\n    # Commit helper\n    # ------------------------------------------------------------------ #\n    def git_commit(self, message: str) -> str:\n        \"\"\"\n        Commit **all** staged/changed files with the given commit message.\n\n        Phase-guard: refuses to commit unless *patch_applied* **and**\n        *tests_passed* are recorded for the current task.\n        Returns the new commit SHA string.\n        \"\"\"\n        stage = \"git_commit\"\n\n        # ---- phase-order enforcement -----------------------------------\n        if self._current_task:\n            tid = self._current_task[\"id\"]\n            missing: List[str] = []\n            if not self._has_phase(tid, \"patch_applied\"):\n                missing.append(\"patch_applied\")\n            if not self._has_phase(tid, \"tests_passed\"):\n                missing.append(\"tests_passed\")\n            if missing:\n                err = ShellCommandError(\n                    f\"Cannot commit – missing prerequisite phase(s): {', '.join(missing)}\"\n                )\n                self._record_failure(state=f\"failed_{stage}\", error=err)\n                raise err\n\n        def _run(cmd: List[str]):\n            return subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n\n        try:\n            # Stage all changes\n            add_cmd = [\"git\", \"add\", \"-A\"]\n            result = _run(add_cmd)\n            if result.returncode != 0:\n                raise ShellCommandError(f\"git add failed: {result.stderr.strip()}\")\n\n            # Commit\n            commit_cmd = [\"git\", \"commit\", \"-m\", message]\n            result = _run(commit_cmd)\n            if result.returncode != 0:\n                if \"nothing to commit\" in (result.stderr + result.stdout).lower():\n                    raise ShellCommandError(\"git commit: nothing to commit.\")\n                raise ShellCommandError(f\"git commit failed: {result.stderr.strip()}\")\n\n            # Retrieve last commit SHA\n            sha_cmd = [\"git\", \"rev-parse\", \"HEAD\"]\n            result = subprocess.run(\n                sha_cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=True,\n            )\n\n            # Mark phase completed\n            if self._current_task:\n                self._mark_phase(self._current_task[\"id\"], \"committed\")\n\n            return result.stdout.strip()\n\n        except Exception as ex:\n            self._record_failure(\n                state=f\"failed_{stage}\",\n                error=ex,\n                output=(result.stderr if \"result\" in locals() else \"\"),\n            )\n            raise\n\n\n# --------------------------------------------------------------------------- #\n# Dev-only sanity CLI\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    runner = ShellRunner(\".\", task_record=None)  # no persistence\n    print(\"ShellRunner loaded. No CLI demo.\")",
  "src/cadence/dev/__init__.py": "\n",
  "src/cadence/dev/phase_guard.py": "# src/cadence/dev/phase_guard.py\n\"\"\"cadence.dev.phase_guard\n\nRuntime enforcement of Cadence workflow-phase ordering.\n\nA lightweight decorator (enforce_phase) raises PhaseOrderError\nwhenever a caller tries to execute a phase whose required predecessors\nhave not yet been completed for the current task.  The decorator is\ngeneric: any object that exposes\n\n· self._current_task   – dict with an “id” key\n· self._has_phase(id, phase) -> bool\n· self._mark_phase(id, phase)\ncan use it.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Any, Callable, Tuple\n\nclass PhaseOrderError(RuntimeError):\n    \"\"\"Raised when workflow phases are executed out of order.\"\"\"\n\ndef enforce_phase(\n    *required_phases: str,\n    mark: str | None = None,\n) -> Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"\n    Decorate a method representing a phase transition.\n\n    Parameters\n    ----------\n    *required_phases :\n        Zero or more phase labels that **must already be complete**\n        for the current task before the wrapped method may run.\n\n    mark :\n        Optional phase label to record as *completed* automatically\n        **after** the wrapped method returns without raising.\n\n    Notes\n    -----\n    If the decorated object is used outside an agentic task context\n    (`self._current_task is None`) the decorator becomes a no-op.\n    \"\"\"\n\n    req: Tuple[str, ...] = tuple(required_phases)\n\n    def _decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n        @functools.wraps(func)\n        def _wrapper(self, *args, **kwargs):\n            task = getattr(self, \"_current_task\", None)\n            if task and req:\n                tid = task.get(\"id\")\n                missing = [p for p in req if not self._has_phase(tid, p)]\n                if missing:\n                    raise PhaseOrderError(\n                        f\"{func.__name__} cannot run – unmet phase(s): \"\n                        f\"{', '.join(missing)}\"\n                    )\n            # --- execute wrapped method -----------------------------------\n            result = func(self, *args, **kwargs)\n\n            # --- auto-mark completion ------------------------------------\n            if task and mark:\n                self._mark_phase(task[\"id\"], mark)\n            return result\n\n        return _wrapper\n\n    return _decorator",
  "src/cadence/dev/backlog.py": "# src/cadence/dev/backlog.py\n\n\"\"\"\nCadence BacklogManager\n---------------------\nThread-safe CRUD on the task backlog.\n\nKey changes (2025-06-21)\n• Introduced a process-local re-entrant lock (`threading.RLock`) named\n  `_lock`.  ALL public mutators and any internal helpers that touch shared\n  state or disk are now executed under `with self._lock: …`.\n• Read helpers (`list_items`, `get_item`, `export`, `__str__`) also acquire\n  the lock to guarantee a coherent snapshot even while writers operate.\n• Nested calls (e.g. `archive_completed()` → `save()`) are safe because\n  RLock is re-entrant.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport json\nimport uuid\nimport threading\nimport copy\nfrom typing import List, Dict, Optional\n\n# --------------------------------------------------------------------------- #\n# Exceptions\n# --------------------------------------------------------------------------- #\nclass BacklogEmptyError(Exception):\n    \"\"\"Raised if attempting to pop or select from an empty backlog.\"\"\"\n\n\nclass TaskStructureError(Exception):\n    \"\"\"Raised if a task dict doesn't conform to required structure.\"\"\"\n\n\nclass TaskNotFoundError(Exception):\n    \"\"\"Raised if a requested task_id is not in the backlog.\"\"\"\n\n\n# --------------------------------------------------------------------------- #\n# Constants / helpers\n# --------------------------------------------------------------------------- #\nREQUIRED_FIELDS = (\"id\", \"title\", \"type\", \"status\", \"created_at\")\n\nVALID_STATUSES = (\"open\", \"in_progress\", \"done\", \"archived\", \"blocked\")\n\n# --------------------------------------------------------------------------- #\n# BacklogManager\n# --------------------------------------------------------------------------- #\nclass BacklogManager:\n    \"\"\"\n    Manages Cadence backlog: micro-tasks, stories, and epics.\n    State is persisted to JSON.  All mutating operations are guarded\n    by an *instance-local* RLock to avoid intra-process race conditions.\n    \"\"\"\n\n    # ------------------------------- #\n    # Construction / loading\n    # ------------------------------- #\n    def __init__(self, backlog_path: str):\n        self.path = backlog_path\n        self._lock = threading.RLock()\n        self._items: List[Dict] = []\n        # load() already acquires the lock – safe to call here\n        self.load()\n\n    # ------------------------------- #\n    # Public API – READ\n    # ------------------------------- #\n    def list_items(self, status: str = \"open\") -> List[Dict]:\n        \"\"\"\n        Return a list of tasks filtered by status.\n        status: \"open\", \"in_progress\", \"done\", \"archived\" or \"all\"\n        * Items with status \"blocked\" are never included in list_items(\"open\")\n        \"\"\"\n        with self._lock:\n            if status == \"open\":\n                data = [item for item in self._items if item.get(\"status\", \"open\") == \"open\"]\n                # explicit: blocked tasks are NOT returned for \"open\":\n                data = [item for item in data if item.get(\"status\") != \"blocked\"]\n            elif status == \"all\":\n                data = list(self._items)\n            else:\n                data = [item for item in self._items if item.get(\"status\", \"open\") == status]\n            # Shallow-copy so caller cannot mutate our internal state.\n            return [dict(item) for item in data]\n\n    def get_item(self, task_id: str) -> Dict:\n        \"\"\"Retrieve a single task by id (defensive copy).\"\"\"\n        with self._lock:\n            idx = self._task_index(task_id)\n            return dict(self._items[idx])\n\n    def export(self) -> List[Dict]:\n        \"\"\"Return a deep copy of *all* backlog items.\"\"\"\n        with self._lock:\n            return copy.deepcopy(self._items)\n\n    # ------------------------------- #\n    # Public API – WRITE / MUTATE\n    # ------------------------------- #\n    def add_item(self, task: Dict) -> None:\n        \"\"\"Add a new task to backlog (enforces structure & unique id).\"\"\"\n        with self._lock:\n            task = self._normalize_task(task)\n            if any(t[\"id\"] == task[\"id\"] for t in self._items):\n                raise TaskStructureError(f\"Duplicate task id: {task['id']}\")\n            self._items.append(task)\n            self.save()\n\n    def remove_item(self, task_id: str) -> None:\n        \"\"\"Soft-delete: mark a task as archived.\"\"\"\n        with self._lock:\n            idx = self._task_index(task_id)\n            self._items[idx][\"status\"] = \"archived\"\n            self.save()\n\n    def update_item(self, task_id: str, updates: Dict) -> None:\n        \"\"\"Update arbitrary fields of a task (e.g. assign, progress).\"\"\"\n        with self._lock:\n            idx = self._task_index(task_id)\n            self._items[idx].update(updates)\n            self.save()\n\n    def archive_completed(self) -> None:\n        \"\"\"Mark all tasks with status 'done' as 'archived'.\"\"\"\n        with self._lock:\n            changed = False\n            for item in self._items:\n                if item.get(\"status\") == \"done\":\n                    item[\"status\"] = \"archived\"\n                    changed = True\n            if changed:\n                self.save()\n\n    # ------------------------------- #\n    # Disk persistence (internal)\n    # ------------------------------- #\n    def save(self) -> None:\n        \"\"\"Persist backlog state atomically (under lock).\"\"\"\n        with self._lock:\n            tmp_path = self.path + \".tmp\"\n            with open(tmp_path, \"w\", encoding=\"utf8\") as f:\n                json.dump(self._items, f, indent=2)\n            os.replace(tmp_path, self.path)\n\n    def load(self) -> None:\n        \"\"\"Load backlog state from disk (gracefully handles missing file).\"\"\"\n        with self._lock:\n            if not os.path.exists(self.path):\n                self._items = []\n                return\n            with open(self.path, \"r\", encoding=\"utf8\") as f:\n                data = json.load(f)\n            if not isinstance(data, list):\n                raise ValueError(\"Backlog JSON must be a list of tasks\")\n            self._items = [self._normalize_task(t) for t in data]\n\n    # ------------------------------- #\n    # Internal helpers\n    # ------------------------------- #\n    def _task_index(self, task_id: str) -> int:\n        for ix, t in enumerate(self._items):\n            if t[\"id\"] == task_id:\n                return ix\n        raise TaskNotFoundError(f\"No task found with id={task_id}\")\n\n    @staticmethod\n    def _normalize_task(task: Dict) -> Dict:\n        \"\"\"Ensure mandatory fields are present; fill sensible defaults.\"\"\"\n        t = dict(task)  # shallow copy\n        for field in REQUIRED_FIELDS:\n            if field not in t:\n                if field == \"id\":\n                    t[\"id\"] = str(uuid.uuid4())\n                elif field == \"created_at\":\n                    import datetime\n\n                    t[\"created_at\"] = datetime.datetime.utcnow().isoformat()\n                elif field == \"status\":\n                    t[\"status\"] = \"open\"\n                elif field == \"type\":\n                    t[\"type\"] = \"micro\"\n                else:\n                    raise TaskStructureError(f\"Missing required field: {field}\")\n        if not isinstance(t[\"id\"], str):\n            t[\"id\"] = str(t[\"id\"])\n        # Validate status field:\n        if t[\"status\"] not in VALID_STATUSES:\n            raise TaskStructureError(f\"Invalid status: {t['status']}. Valid: {VALID_STATUSES}\")\n        return t\n\n    # ------------------------------- #\n    # Convenience string representation\n    # ------------------------------- #\n    def __str__(self) -> str:\n        from tabulate import tabulate\n\n        with self._lock:\n            if not self._items:\n                return \"(Backlog empty)\"\n            rows = [\n                (t[\"id\"][:8], t[\"title\"], t[\"type\"], t.get(\"status\", \"open\"), t.get(\"created_at\", \"\"))\n                for t in self._items\n                if t.get(\"status\") != \"archived\"\n            ]\n            headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n            return tabulate(rows, headers, tablefmt=\"github\")\n\n\n# --------------------------------------------------------------------------- #\n# Development-only smoke-test\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":  # pragma: no cover\n    mgr = BacklogManager(\"dev_backlog.json\")\n    print(mgr)\n",
  "src/cadence/dev/generator.py": "\n# src/cadence/dev/generator.py\n\n\"\"\"\nCadence TaskGenerator\n-------------------\nSingle Responsibility: Propose/generate well-formed tasks, optionally from template or rules/LLM seed.\nNever applies code or diffs. Future extensible to LLM/human agent.\n\"\"\"\n\nimport os, json, uuid, datetime, warnings\nfrom typing import List, Dict, Optional\n\nclass TaskTemplateError(Exception):\n    \"\"\"Raised if template file is not valid or incomplete.\"\"\"\n    pass\n\nREQUIRED_FIELDS = (\"title\", \"type\", \"status\", \"created_at\")\n\n\nclass TaskGenerator:\n    def __init__(self, template_file: str | None = None, *, strict: bool = False):\n        \"\"\"\n        Optionally supply a JSON / MD template file.  \n        If `strict` is False (default) and the file does **not** exist, we\n        continue with an empty template dictionary and merely warn.\n        \"\"\"\n        self.template_file = template_file\n        self._template_cache: Dict = {}\n        if template_file:\n            if os.path.exists(template_file):\n                self._template_cache = self._load_template(template_file)\n            elif strict:\n                # Original behaviour – hard-fail\n                raise TaskTemplateError(f\"Template file not found: {template_file}\")\n            else:\n                warnings.warn(\n                    f\"Template file '{template_file}' not found; \"\n                    \"proceeding with minimal fallback templates.\",\n                    RuntimeWarning,\n                )\n    \n    def generate_tasks(self, mode: str = \"micro\", count: int = 1, human_prompt: Optional[str]=None) -> List[Dict]:\n        \"\"\"\n        Return a list of well-formed tasks. \n        - mode: \"micro\", \"story\", \"epic\", etc.\n        - count: number of tasks to generate\n        - human_prompt: if provided, use as summary/title for each (e.g., \"Add new test\", for human CLI prompt workflow)\n        If template_file is used, will fill in mode-related templates.\n        \"\"\"\n        tasks = []\n        base_tpl = self._get_template_for_mode(mode)\n        now = datetime.datetime.utcnow().isoformat()\n        for i in range(count):\n            task = dict(base_tpl)\n            # Minimal fields: id, title, type, status, created_at\n            task[\"id\"] = str(uuid.uuid4())\n            task[\"type\"] = mode\n            task.setdefault(\"status\", \"open\")\n            task.setdefault(\"created_at\", now)\n            if human_prompt:\n                # Provide a default/barebones title/desc from human input\n                task[\"title\"] = human_prompt if count == 1 else f\"{human_prompt} [{i+1}]\"\n                task.setdefault(\"description\", human_prompt)\n            else:\n                # Fallback: title must be present; if not, use template/title from mode or 'Untitled'\n                task[\"title\"] = task.get(\"title\", f\"{mode.capitalize()} Task {i+1}\")\n                task.setdefault(\"description\", \"\")\n            self._validate_task(task)\n            tasks.append(task)\n        return tasks\n\n    def overwrite_tasks(self, new_tasks: List[Dict], output_path: Optional[str]=None) -> None:\n        \"\"\"\n        Replace all backlog tasks with given well-formed list (writes to output_path, else self.template_file).\n        \"\"\"\n        path = output_path or self.template_file\n        if not path:\n            raise TaskTemplateError(\"No output path specified to write tasks.\")\n        with open(path, \"w\", encoding=\"utf8\") as f:\n            json.dump([self._validate_task(t) for t in new_tasks], f, indent=2)\n\n    def _get_template_for_mode(self, mode: str) -> Dict:\n        \"\"\"\n        Get template for the given mode; falls back to default/minimal template.\n        \"\"\"\n        if self._template_cache and mode in self._template_cache:\n            return dict(self._template_cache[mode])  # deep copy\n        # Fallback: minimal template\n        return {\n            \"title\": \"\",\n            \"type\": mode,\n            \"status\": \"open\",\n            \"created_at\": \"\",\n            \"description\": \"\",\n        }\n\n    def _load_template(self, path: str) -> Dict:\n        \"\"\"\n        Loads a JSON template file mapping mode→template-dict.\n        If Markdown file with front-matter, parse the JSON front-matter.\n        \"\"\"\n        if not os.path.exists(path):\n            raise TaskTemplateError(f\"Template file not found: {path}\")\n        if path.endswith(\".md\"):\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                lines = f.readlines()\n            start, end = None, None\n            for i, line in enumerate(lines):\n                if line.strip() == \"```json\":\n                    start = i + 1\n                elif line.strip().startswith(\"```\") and start is not None and end is None:\n                    end = i\n                    break\n            if start is not None and end is not None:\n                json_str = \"\".join(lines[start:end])\n                tpl = json.loads(json_str)\n            else:\n                raise TaskTemplateError(\"Markdown template missing ```json ... ``` block.\")\n        else:\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                tpl = json.load(f)\n        if not isinstance(tpl, dict):\n            raise TaskTemplateError(\"Task template must be a dict mapping mode->template.\")\n        return tpl\n\n    def _validate_task(self, task: Dict) -> Dict:\n        \"\"\"\n        Ensures task has all required fields and correct types/formats.\n        Throws TaskTemplateError if not.\n        \"\"\"\n        for field in REQUIRED_FIELDS:\n            if field not in task or (field == \"title\" and not task[\"title\"].strip()):\n                raise TaskTemplateError(f\"Task missing required field: '{field}'\")\n        if not isinstance(task[\"type\"], str):\n            raise TaskTemplateError(\"Task type must be str.\")\n        if \"id\" in task and not isinstance(task[\"id\"], str):\n            task[\"id\"] = str(task[\"id\"])\n        # Optionally: check status value, etc.\n        return task\n\n    # For future agentic/LLM/human input: accept strings, call LLM API, etc.\n    # Extend here with agent hooks.\n\n# Standalone/test CLI example (not for production)\nif __name__ == \"__main__\":\n    # Example: generate 2 microtasks from default, print as JSON:\n    g = TaskGenerator()\n    tasks = g.generate_tasks(mode=\"micro\", count=2, human_prompt=\"Example user-initiated task\")\n    print(json.dumps(tasks, indent=2))",
  "src/cadence/dev/reviewer.py": "\n# src/cadence/dev/reviewer.py\n\n\"\"\"\nCadence TaskReviewer\n-------------------\nSingle Responsibility: Adjudicates patch/diff quality via rules/LLM/manual. Never applies code or diffs.\nFuture extensible: can host local ruleset, shell out to LLM agent, or use human-in-the-loop.\n\"\"\"\n\nimport os\nimport json\nfrom typing import Optional, Dict\n\nclass PatchReviewError(Exception):\n    \"\"\"Raised if review input is malformed or review fails outright (e.g. ruleset not found/valid).\"\"\"\n    pass\n\nclass TaskReviewer:\n    def __init__(self, ruleset_file: str = None):\n        \"\"\"\n        Optionally specify path to ruleset file (JSON list of rules),\n        or leave blank to use default built-in rules.\n        \"\"\"\n        self.ruleset_file = ruleset_file\n        self.rules = self._load_ruleset(ruleset_file) if ruleset_file else self._default_ruleset()\n\n    def review_patch(self, patch: str, context: Optional[dict] = None) -> Dict:\n        \"\"\"\n        Review a diff/patch string (unapplied) and optional context (task, commit message, etc).\n        Returns dict {'pass': bool, 'comments': str}\n        This uses static (offline) heuristics but can be swapped for agent/LLM in future.\n        \"\"\"\n        # Guard: Patch required\n        if not patch or not isinstance(patch, str):\n            return {'pass': False, 'comments': 'Patch missing or not a string.'}\n\n        # Apply rules in order. If any hard-fail, review fails.\n        comments = []\n        passed = True\n\n        for rule in self.rules:\n            ok, msg = rule(patch, context)\n            if not ok:\n                passed = False\n            if msg:\n                comments.append(msg)\n            if not ok:\n                # For now, fail-hard (but comment all)\n                break\n\n        return {'pass': passed, 'comments': \"\\n\".join(comments).strip()}\n\n    def _default_ruleset(self):\n        \"\"\"\n        Returns a list of static rule functions: (patch, context) → (bool, str)\n        \"\"\"\n        def not_empty_rule(patch, _):\n            if not patch.strip():\n                return False, \"Patch is empty.\"\n            return True, \"\"\n        def startswith_rule(patch, _):\n            if not patch.startswith((\"---\", \"diff \", \"@@ \")):\n                return False, \"Patch does not appear to be a valid unified diff.\"\n            return True, \"\"\n        def contains_todo_rule(patch, _):\n            if \"TODO\" in patch:\n                return False, \"Patch contains 'TODO'—code review must not introduce placeholders.\"\n            return True, \"\"\n\n        # Optionally check for too-huge diffs, or forbidden patterns, via rules below.\n        def size_limit_rule(patch, _):\n            line_count = patch.count(\"\\n\")\n            if line_count > 5000:  # Arbitrary large patch guard\n                return False, f\"Patch too large for standard review ({line_count} lines).\"\n            return True, \"\"\n        return [\n            not_empty_rule, \n            startswith_rule,\n            contains_todo_rule,\n            size_limit_rule,\n        ]\n\n    def _load_ruleset(self, path: str):\n        \"\"\"\n        Loads a simple external ruleset (for human/agent extension), e.g. as list of forbidden strings.\n        For extensibility only; advanced policies/LLMs should be subclassed onto this interface.\n        \"\"\"\n        if not os.path.exists(path):\n            raise PatchReviewError(f\"Ruleset file '{path}' not found.\")\n        with open(path, \"r\", encoding=\"utf8\") as f:\n            obj = json.load(f)\n        # Expect a list of {'type':..., 'pattern':..., ...} dicts for pattern rules\n        rules = []\n        def make_rule(ruleobj):\n            typ = ruleobj.get('type')\n            pattern = ruleobj.get('pattern')\n            msg = ruleobj.get('message', f\"Patch contains forbidden pattern: {pattern}\")\n            if typ == 'forbid':\n                def _inner(patch, _):\n                    if pattern in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            elif typ == 'require':\n                def _inner(patch, _):\n                    if pattern not in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            else:\n                # Ignore unknown rule types\n                def _inner(patch, _):\n                    return True, \"\"\n                return _inner\n        for ruleobj in obj:\n            rules.append(make_rule(ruleobj))\n        # Default rules always included\n        return self._default_ruleset() + rules\n\n# Standalone/example/test run\nif __name__ == \"__main__\":\n    reviewer = TaskReviewer()\n    # Good patch\n    patch = \"\"\"--- sample.py\n+++ sample.py\n@@ -1 +1,2 @@\n-print('hello')\n+print('hello world')\n\"\"\"\n    result = reviewer.review_patch(patch)\n    print(\"Result (should pass):\", result)\n\n    bad_patch = \"TODO: refactor\\n\"\n    result = reviewer.review_patch(bad_patch)\n    print(\"Result (should fail):\", result)",
  "src/cadence/dev/orchestrator.py": "# src/cadence/dev/orchestrator.py\n\"\"\"\nCadence DevOrchestrator\n-----------------------\nIntegrated union of all prior versions.\n\nKey capabilities\n~~~~~~~~~~~~~~~~\n• Auto-replenishes an empty backlog with micro-tasks.  \n• Persists *every* state transition to TaskRecord; ShellRunner\n  self-records failures after `.attach_task()`.  \n• Two-stage human-style review:\n    1. **Reasoning** review via `TaskReviewer`.\n    2. **Efficiency** review via `EfficiencyAgent` (LLM).  \n• Safe patch application with automatic rollback on test/commit failure.  \n• **MetaAgent** governance layer records post-cycle telemetry for audit /\n  policy-checking (gated by `config['enable_meta']`, default =True).  \n\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\nfrom typing import Any, Dict, Optional\n\nimport tabulate  # noqa: F401 – needed by _format_backlog\n\nfrom cadence.agents.registry import get_agent  # EfficiencyAgent\nfrom .backlog import BacklogManager\nfrom .executor import PatchBuildError, TaskExecutor, TaskExecutorError\nfrom .generator import TaskGenerator\nfrom .record import TaskRecord, TaskRecordError\nfrom .reviewer import TaskReviewer\nfrom .shell import ShellRunner, ShellCommandError\n\n# --------------------------------------------------------------------------- #\n# Meta-governance stub\n# --------------------------------------------------------------------------- #\nclass MetaAgent:\n    \"\"\"Light-weight governance / analytics layer (MVP stub).\"\"\"\n\n    def __init__(self, task_record: TaskRecord):\n        self.task_record = task_record\n\n    def analyse(self, run_summary: dict) -> dict:  # noqa: D401\n        \"\"\"Return minimal telemetry; insert richer checks later.\"\"\"\n        return {\n            \"telemetry\": run_summary.copy(),\n            \"policy_check\": \"stub\",\n            \"meta_ok\": True,\n        }\n\n\n# --------------------------------------------------------------------------- #\n# Orchestrator\n# --------------------------------------------------------------------------- #\nclass DevOrchestrator:\n    def __init__(self, config: dict):\n        # Core collaborators -------------------------------------------------\n        self.backlog = BacklogManager(config[\"backlog_path\"])\n        self.generator = TaskGenerator(config.get(\"template_file\"))\n        self.record = TaskRecord(config[\"record_file\"])\n        self.shell = ShellRunner(config[\"repo_dir\"], task_record=self.record)\n        self.executor = TaskExecutor(config[\"src_root\"])\n        self.reviewer = TaskReviewer(config.get(\"ruleset_file\"))\n\n        # Agents -------------------------------------------------------------\n        self.efficiency = get_agent(\"efficiency\")\n        self._enable_meta: bool = config.get(\"enable_meta\", True)\n        self.meta_agent: Optional[MetaAgent] = (\n            MetaAgent(self.record) if self._enable_meta else None\n        )\n\n        # Behaviour toggles --------------------------------------------------\n        self.backlog_autoreplenish_count: int = config.get(\n            \"backlog_autoreplenish_count\", 3\n        )\n\n    # ------------------------------------------------------------------ #\n    # Back-log auto-replenishment\n    # ------------------------------------------------------------------ #\n    def _ensure_backlog(self, count: Optional[int] = None) -> None:\n        if self.backlog.list_items(\"open\"):\n            return\n        n = count if count is not None else self.backlog_autoreplenish_count\n        for t in self.generator.generate_tasks(mode=\"micro\", count=n):\n            self.backlog.add_item(t)\n        self._record(\n            {\"id\": \"auto-backlog-replenish\", \"title\": \"Auto-replenish\"},\n            state=\"backlog_replenished\",\n            extra={\"count\": n},\n        )\n\n    # ------------------------------------------------------------------ #\n    # Record helper – ALWAYS log, never raise\n    # ------------------------------------------------------------------ #\n    def _record(\n        self, task: dict, state: str, extra: Dict[str, Any] | None = None\n    ) -> None:\n        try:\n            self.record.save(task, state=state, extra=extra or {})\n        except TaskRecordError as e:\n            print(f\"[Record-Error] {e}\", file=sys.stderr)\n\n    # ------------------------------------------------------------------ #\n    # Pretty-printing helpers\n    # ------------------------------------------------------------------ #\n    def show(self, status: str = \"open\", printout: bool = True):\n        items = self.backlog.list_items(status)\n        if printout:\n            print(self._format_backlog(items))\n        return items\n\n    def _format_backlog(self, items):\n        if not items:\n            return \"(Backlog empty)\"\n        rows = [\n            (\n                t[\"id\"][:8],\n                t.get(\"title\", \"\")[:48],\n                t.get(\"type\", \"\"),\n                t.get(\"status\", \"\"),\n                t.get(\"created_at\", \"\")[:19],\n            )\n            for t in items\n            if t.get(\"status\") != \"archived\"\n        ]\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n        return tabulate.tabulate(rows, headers, tablefmt=\"github\")\n\n    # ------------------------------------------------------------------ #\n    # Main workflow\n    # ------------------------------------------------------------------ #\n    def run_task_cycle(\n        self, select_id: str | None = None, *, interactive: bool = False\n    ):\n        \"\"\"\n        Run **one** micro-task end-to-end with:\n\n        • auto-replenish ⟶ dual Reasoning+Efficiency reviews ⟶ tests ⟶ commit  \n        • auto-rollback on failure  \n        • MetaAgent post-run analysis (non-blocking)  \n        \"\"\"\n        self._ensure_backlog()\n        rollback_patch: str | None = None\n        task: dict | None = None\n        run_result: Dict[str, Any] | None = None\n\n        try:\n            # 1️⃣  Select task ------------------------------------------------\n            open_tasks = self.backlog.list_items(\"open\")\n            if not open_tasks:\n                raise RuntimeError(\"No open tasks in backlog.\")\n\n            if select_id:\n                task = next((t for t in open_tasks if t[\"id\"] == select_id), None)\n                if not task:\n                    raise RuntimeError(f\"Task id '{select_id}' not found.\")\n            elif interactive:\n                print(self._format_backlog(open_tasks))\n                print(\"---\")\n                task = open_tasks[self._prompt_pick(len(open_tasks))]\n            else:\n                task = open_tasks[0]\n\n            print(f\"\\n[Selected task: {task['id'][:8]}] {task.get('title')}\\n\")\n            self.shell.attach_task(task)  # allow ShellRunner to self-record\n\n            # 2️⃣  Build patch -----------------------------------------------\n            self._record(task, \"build_patch\")\n            try:\n                patch = self.executor.build_patch(task)\n                rollback_patch = patch\n                self._record(task, \"patch_built\", {\"patch\": patch})\n                print(\"--- Patch built ---\\n\", patch)\n            except (PatchBuildError, TaskExecutorError) as ex:\n                self._record(task, \"failed_build_patch\", {\"error\": str(ex)})\n                print(f\"[X] Patch build failed: {ex}\")\n                return {\"success\": False, \"stage\": \"build_patch\", \"error\": str(ex)}\n\n            # 3️⃣  Review #1 – Reasoning ------------------------------------\n            review1 = self.reviewer.review_patch(patch, context=task)\n            # keep legacy state for the test-suite\n            self._record(task, \"patch_reviewed\",             {\"review\": review1})\n            self._record(task, \"patch_reviewed_reasoning\",   {\"review\": review1})\n            print(\"--- Review 1 (Reasoning) ---\")\n            print(review1[\"comments\"] or \"(no comments)\")\n            if not review1[\"pass\"]:\n                self._record(task, \"failed_patch_review_reasoning\", {\"review\": review1})\n                print(\"[X] Patch failed REASONING review, aborting.\")\n                return {\n                    \"success\": False,\n                    \"stage\": \"patch_review_reasoning\",\n                    \"review\": review1,\n                }\n\n            # 4️⃣  Review #2 – Efficiency ------------------------------------\n            # Skip hard-LLM step in stub-mode so CI remains offline-safe\n            if getattr(self.efficiency.llm_client, \"stub\", False):\n                eff_raw  = \"LLM stub-mode: efficiency review skipped.\"\n                eff_pass = True\n                if eff_pass and hasattr(self.shell, \"_mark_phase\"):\n                    self.shell._mark_phase(task[\"id\"], \"efficiency_passed\")\n            else:\n                eff_prompt = (\n                    \"You are the EfficiencyAgent for the Cadence workflow.\\n\"\n                    \"Review the diff below for best-practice, lint, and summarisation.\\n\"\n                    f\"DIFF:\\n{patch}\\n\\nTASK CONTEXT:\\n{task}\"\n                )\n                eff_raw  = self.efficiency.run_interaction(eff_prompt)\n                eff_pass = \"fail\" not in eff_raw.lower() #and \"pass\" in eff_raw.lower()\n\n            # Record flag for downstream phase-guards\n            if eff_pass and hasattr(self.shell, \"_mark_phase\") and task.get(\"id\"):\n                self.shell._mark_phase(task[\"id\"], \"efficiency_passed\")\n            eff_review = {\"pass\": eff_pass, \"comments\": eff_raw}\n            self._record(task, \"patch_reviewed_efficiency\", {\"review\": eff_review})\n            print(\"--- Review 2 (Efficiency) ---\")\n            print(eff_review[\"comments\"] or \"(no comments)\")\n            if not eff_pass:\n                self._record(task, \"failed_patch_review_efficiency\", {\"review\": eff_review})\n                print(\"[X] Patch failed EFFICIENCY review, aborting.\")\n                return {\n                    \"success\": False,\n                    \"stage\": \"patch_review_efficiency\",\n                    \"review\": eff_review,\n                }\n\n            # # Optional phase marker for advanced ShellRunner integrations ----\n            # if hasattr(self.shell, \"_mark_phase\") and task.get(\"id\"):\n            #     self.shell._mark_phase(task[\"id\"], \"efficiency_passed\")\n\n            # 5️⃣  Apply patch -----------------------------------------------\n            try:\n                self.shell.git_apply(patch)\n                self._record(task, \"patch_applied\")\n                print(\"[✔] Patch applied.\")\n            except ShellCommandError as ex:\n                self._record(task, \"failed_patch_apply\", {\"error\": str(ex)})\n                print(f\"[X] git apply failed: {ex}\")\n                return {\"success\": False, \"stage\": \"patch_apply\", \"error\": str(ex)}\n\n            # 6️⃣  Run tests --------------------------------------------------\n            test_result = self.shell.run_pytest()\n            self._record(task, \"pytest_run\", {\"pytest\": test_result})\n            print(\"--- Pytest ---\")\n            print(test_result[\"output\"])\n            if not test_result[\"success\"]:\n                print(\"[X] Tests FAILED. Initiating rollback.\")\n                self._record(task, \"failed_test\", {\"pytest\": test_result})\n                self._attempt_rollback(task, rollback_patch, src_stage=\"test\")\n                return {\"success\": False, \"stage\": \"test\", \"test_result\": test_result}\n\n            # 7️⃣  Commit -----------------------------------------------------\n            commit_msg = f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\"\n            try:\n                sha = self.shell.git_commit(commit_msg)\n                self._record(task, \"committed\", {\"commit_sha\": sha})\n                print(f\"[✔] Committed as {sha}\")\n            except ShellCommandError as ex:\n                self._record(task, \"failed_commit\", {\"error\": str(ex)})\n                print(f\"[X] git commit failed: {ex}\")\n                self._attempt_rollback(task, rollback_patch, src_stage=\"commit\")\n                return {\"success\": False, \"stage\": \"commit\", \"error\": str(ex)}\n\n            # 8️⃣  Mark done & archive ---------------------------------------\n            self.backlog.update_item(task[\"id\"], {\"status\": \"done\"})\n            task = self.backlog.get_item(task[\"id\"])\n            self._record(task, \"status_done\")\n            self.backlog.archive_completed()\n            task = self.backlog.get_item(task[\"id\"])\n            self._record(task, \"archived\")\n            print(\"[✔] Task marked done and archived.\")\n\n            run_result = {\"success\": True, \"commit\": sha, \"task_id\": task[\"id\"]}\n            return run_result\n\n        except Exception as ex:\n            if task and rollback_patch:\n                self._attempt_rollback(task, rollback_patch, src_stage=\"unexpected\", quiet=True)\n            print(f\"[X] Cycle failed: {ex}\")\n            run_result = {\"success\": False, \"error\": str(ex)}\n            return run_result\n\n        # ------------------------------------------------------------------ #\n        # MetaAgent post-cycle analysis (non-blocking)\n        # ------------------------------------------------------------------ #\n        finally:\n            if self._enable_meta and self.meta_agent and task:\n                try:\n                    meta_result = self.meta_agent.analyse(run_result or {})\n                    # append_iteration keeps the last history entry untouched\n                    self.record.append_iteration(task[\"id\"],\n                                                {\"phase\": \"meta_analysis\",\n                                                \"payload\": meta_result})\n                except Exception as meta_ex:   # pragma: no cover\n                    print(f\"[MetaAgent-Error] {meta_ex}\", file=sys.stderr)\n\n    # ------------------------------------------------------------------ #\n    # Rollback helper\n    # ------------------------------------------------------------------ #\n    def _attempt_rollback(\n        self, task: dict, patch: str | None, *, src_stage: str, quiet: bool = False\n    ):\n        if not patch:\n            self._record(task, \"rollback_skip_no_patch\")\n            return\n        try:\n            self.shell.git_apply(patch, reverse=True)\n            self._record(task, f\"failed_{src_stage}_and_rollback\")\n            if not quiet:\n                print(\"[↩] Rollback successful – working tree restored.\")\n        except ShellCommandError as rb_ex:\n            self._record(\n                task,\n                \"critical_rollback_failure\",\n                {\"trigger\": src_stage, \"rollback_error\": str(rb_ex)},\n            )\n            print(f\"[!!] Rollback FAILED – manual fix required: {rb_ex}\")\n\n    # ------------------------------------------------------------------ #\n    # CLI helpers\n    # ------------------------------------------------------------------ #\n    def cli_entry(self, command: str, **kwargs):\n        try:\n            if command in (\"backlog\", \"show\"):\n                return self.show(status=kwargs.get(\"status\", \"open\"))\n            if command in (\"start\", \"evaluate\"):\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            if command == \"done\":\n                if \"id\" not in kwargs:\n                    print(\"You must supply a task id for 'done'.\")\n                    return\n                self.backlog.update_item(kwargs[\"id\"], {\"status\": \"done\"})\n                self.backlog.archive_completed()\n                print(f\"Task {kwargs['id']} marked as done and archived.\")\n                return\n            print(f\"Unknown command: {command}\")\n        except Exception as ex:\n            print(f\"[X] CLI command '{command}' failed: {ex}\")\n\n    def _prompt_pick(self, n: int) -> int:\n        while True:\n            ans = input(f\"Select task [0-{n-1}]: \")\n            try:\n                ix = int(ans)\n                if 0 <= ix < n:\n                    return ix\n            except Exception:\n                pass\n            print(\"Invalid. Try again.\")\n\n\n# --------------------------------------------------------------------------- #\n# Stand-alone execution helper\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    CONFIG = dict(\n        backlog_path=\"dev_backlog.json\",\n        template_file=\"dev_templates.json\",\n        src_root=\"cadence\",\n        ruleset_file=None,\n        repo_dir=\".\",\n        record_file=\"dev_record.json\",\n        enable_meta=True,\n        backlog_autoreplenish_count=3,\n    )\n    orch = DevOrchestrator(CONFIG)\n\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command\", nargs=\"?\", help=\"show|start|evaluate|done\")\n    parser.add_argument(\"--id\", default=None, help=\"Task id to use\")\n    parser.add_argument(\n        \"--backlog-autoreplenish-count\",\n        type=int,\n        default=3,\n        help=\"Number of micro-tasks to auto-generate when backlog is empty.\",\n    )\n    parser.add_argument(\n        \"--disable-meta\",\n        action=\"store_true\",\n        help=\"Disable MetaAgent execution for this session.\",\n    )\n    args = parser.parse_args()\n\n    orch.backlog_autoreplenish_count = args.backlog_autoreplenish_count\n    if args.disable_meta:\n        orch._enable_meta = False\n        orch.meta_agent = None\n\n    orch.cli_entry(args.command or \"show\", id=args.id)\n",
  "src/cadence/dev/record.py": "# src/cadence/dev/record.py\n\n\"\"\"\nCadence TaskRecord\n------------------\nThread-safe, append-only persistence of task life-cycle history.\n\nKey upgrades (2025-06-21)\n• Replaced `threading.Lock` with **re-entrant** `threading.RLock` so\n  nested mutator calls (e.g., save() → _persist()) never dead-lock.\n• Every public mutator (save, append_iteration) and every private helper\n  that writes to disk now acquires the lock.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport json\nimport threading\nimport copy\nfrom typing import List, Dict, Optional\nfrom datetime import datetime, UTC\n\n# --------------------------------------------------------------------------- #\n# Exceptions\n# --------------------------------------------------------------------------- #\nclass TaskRecordError(Exception):\n    \"\"\"Custom error for task record issues.\"\"\"\n\n\n# --------------------------------------------------------------------------- #\n# TaskRecord\n# --------------------------------------------------------------------------- #\nclass TaskRecord:\n    def __init__(self, record_file: str):\n        self.record_file = record_file\n        self._lock = threading.RLock()  # <-- upgraded to RLock\n        self._records: List[Dict] = []\n        self._idmap: Dict[str, Dict] = {}\n        self._load()  # safe – _load() acquires the lock internally\n\n    # ------------------------------------------------------------------ #\n    # Public API – mutators\n    # ------------------------------------------------------------------ #\n    def save(self, task: dict, state: str, extra: dict | None = None) -> None:\n        \"\"\"\n        Append a new state snapshot for the given task_id.\n        \"\"\"\n        with self._lock:\n            record = self._find_or_create_record(task)\n            snapshot = {\n                \"state\": state,\n                \"timestamp\": self._now(),\n                \"task\": copy.deepcopy(task),\n                \"extra\": copy.deepcopy(extra) if extra else {},\n            }\n            record[\"history\"].append(snapshot)\n            self._sync_idmap()\n            self._persist()\n\n    def append_iteration(self, task_id: str, iteration: dict) -> None:\n        \"\"\"\n        Append a fine-grained iteration step (e.g. reviewer notes).\n        \"\"\"\n        with self._lock:\n            record = self._find_record(task_id)\n            if record is None:\n                raise TaskRecordError(f\"No record for task id={task_id}\")\n            iter_snapshot = {\"timestamp\": self._now(), **copy.deepcopy(iteration)}\n            record.setdefault(\"iterations\", []).append(iter_snapshot)\n            self._persist()\n\n    # ------------------------------------------------------------------ #\n    # Public API – read-only\n    # ------------------------------------------------------------------ #\n    def load(self) -> List[Dict]:\n        \"\"\"Return a deep copy of all records.\"\"\"\n        with self._lock:\n            return copy.deepcopy(self._records)\n\n    # ------------------------------------------------------------------ #\n    # Internal helpers (locking handled by callers)\n    # ------------------------------------------------------------------ #\n    def _find_or_create_record(self, task: dict) -> Dict:\n        tid = self._get_task_id(task)\n        rec = self._idmap.get(tid)\n        if rec is None:\n            rec = {\n                \"task_id\": tid,\n                \"created_at\": self._now(),\n                \"history\": [],\n                \"iterations\": [],\n            }\n            self._records.append(rec)\n            self._idmap[tid] = rec\n        return rec\n\n    def _find_record(self, task_id: str) -> Optional[Dict]:\n        return self._idmap.get(task_id)\n\n    @staticmethod\n    def _get_task_id(task: dict) -> str:\n        tid = task.get(\"id\")\n        if not tid:\n            raise TaskRecordError(\"Task dict missing 'id'. Cannot save record.\")\n        return tid\n\n    # ------------------------------------------------------------------ #\n    # Disk persistence & loading (always under lock)\n    # ------------------------------------------------------------------ #\n    def _persist(self) -> None:\n        with self._lock:\n            tmp = self.record_file + \".tmp\"\n            with open(tmp, \"w\", encoding=\"utf8\") as f:\n                json.dump(self._records, f, indent=2)\n            os.replace(tmp, self.record_file)\n\n    def _load(self) -> None:\n        with self._lock:\n            if not os.path.exists(self.record_file):\n                self._records = []\n                self._idmap = {}\n                return\n            with open(self.record_file, \"r\", encoding=\"utf8\") as f:\n                self._records = json.load(f)\n            self._sync_idmap()\n\n    def _sync_idmap(self):\n        self._idmap = {rec[\"task_id\"]: rec for rec in self._records}\n\n    @staticmethod\n    def _now():\n        return datetime.now(UTC).isoformat()\n\n\n# --------------------------------------------------------------------------- #\n# Dev-only sanity CLI\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":  # pragma: no cover\n    rec = TaskRecord(\"dev_record.json\")\n    tid = \"a1b2c3\"\n    task = {\"id\": tid, \"title\": \"Do something\", \"status\": \"open\"}\n    rec.save(task, state=\"patch_proposed\", extra={\"patch\": \"--- foo\"})\n    rec.append_iteration(tid, {\"reviewer\": \"alice\", \"opinion\": \"looks good\"})\n    import pprint\n\n    pprint.pp(rec.load())\n",
  "src/cadence/dev/executor.py": "# src/cadence/dev/executor.py\n\"\"\"\nCadence TaskExecutor\n\nNow consumes *structured* ChangeSets in addition to raw diffs.  Priority:\n\n    1. task[\"patch\"]         – already-built diff (legacy)\n    2. task[\"change_set\"]    – **new preferred path**\n    3. task[\"diff\"]          – legacy before/after dict (kept for tests)\n\nThe method still returns a unified diff string so downstream ShellRunner /\nReviewer require **zero** changes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nimport difflib\nimport os\n\nfrom .change_set import ChangeSet\nfrom .patch_builder import build_patch, PatchBuildError\n\n\nclass TaskExecutorError(RuntimeError):\n    \"\"\"Generic executor failure.\"\"\"\n\n\nclass TaskExecutor:\n    def __init__(self, src_root: str | Path):\n        self.src_root = Path(src_root).resolve()\n        if not self.src_root.is_dir():\n            raise ValueError(f\"src_root '{src_root}' is not a directory.\")\n\n    # ------------------------------------------------------------------ #\n    # Public\n    # ------------------------------------------------------------------ #\n    def build_patch(self, task: Dict[str, Any]) -> str:\n        \"\"\"\n        Return a unified diff string ready for `git apply`.\n\n        Accepted task keys (checked in this order):\n\n        • \"patch\"       – already-made diff → returned unchanged.\n        • \"change_set\"  – new structured format → converted via PatchBuilder.\n        • \"diff\"        – legacy single-file before/after dict.\n\n        Raises TaskExecutorError (wrapper) on failure so orchestrator callers\n        don’t have to know about PatchBuildError vs ValueError, etc.\n        \"\"\"\n        try:\n            # 1. already-built patch supplied?  --------------------------------\n            raw = task.get(\"patch\")\n            if isinstance(raw, str) and raw.strip():\n                return raw if raw.endswith(\"\\n\") else raw + \"\\n\"\n\n            # 2. new ChangeSet path  ------------------------------------------\n            if \"change_set\" in task:\n                cs_obj = ChangeSet.from_dict(task[\"change_set\"])\n                return build_patch(cs_obj, self.src_root)\n\n            # 3. legacy single-file diff dict  --------------------------------\n            return self._build_one_file_diff(task)\n\n        except PatchBuildError as exc:\n            raise TaskExecutorError(str(exc)) from exc\n        except Exception as exc:\n            raise TaskExecutorError(f\"Failed to build patch: {exc}\") from exc\n\n    # ------------------------------------------------------------------ #\n    # Legacy helper – keep old diff path working\n    # ------------------------------------------------------------------ #\n    def _build_one_file_diff(self, task: Dict[str, Any]) -> str:\n        diff_info = task.get(\"diff\")\n        if not diff_info:\n            raise TaskExecutorError(\n                \"Task missing 'change_set' or 'diff' or already-built 'patch'.\"\n            )\n\n        file_rel = diff_info.get(\"file\", \"\")\n        before = diff_info.get(\"before\")\n        after = diff_info.get(\"after\")\n\n        if not file_rel or before is None or after is None:\n            raise TaskExecutorError(\n                \"diff dict must contain 'file', 'before', and 'after'.\"\n            )\n\n        # --- normalise line endings ------------------------------------- #\n        if before and not before.endswith(\"\\n\"):\n            before += \"\\n\"\n        if after and not after.endswith(\"\\n\"):\n            after += \"\\n\"\n\n        before_lines: List[str] = before.splitlines(keepends=True) if before else []\n        after_lines: List[str] = after.splitlines(keepends=True) if after else []\n\n        new_file = len(before_lines) == 0 and len(after_lines) > 0\n        delete_file = len(before_lines) > 0 and len(after_lines) == 0\n\n        fromfile = \"/dev/null\" if new_file else f\"a/{file_rel}\"\n        tofile = \"/dev/null\" if delete_file else f\"b/{file_rel}\"\n\n        diff_lines = difflib.unified_diff(\n            before_lines,\n            after_lines,\n            fromfile=fromfile,\n            tofile=tofile,\n            lineterm=\"\\n\",\n        )\n        patch = \"\".join(diff_lines)\n        if not patch.strip():\n            raise TaskExecutorError(\"Generated patch is empty.\")\n        if not patch.endswith(\"\\n\"):\n            patch += \"\\n\"\n        return patch",
  "src/cadence/dev/schema.py": "# src/cadence/dev/schema.py\n\"\"\"\nRuntime JSON-Schema definitions that agents *must* follow.\n\"\"\"\n\nCHANGE_SET_V1 = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"title\": \"CadenceChangeSet\",\n    \"type\": \"object\",\n    \"required\": [\"message\", \"edits\"],\n    \"properties\": {\n        \"message\": {\"type\": \"string\", \"minLength\": 1},\n        \"author\":  {\"type\": \"string\"},\n        \"meta\":    {\"type\": \"object\"},\n        \"edits\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"path\", \"mode\"],\n                \"properties\": {\n                    \"path\": {\"type\": \"string\", \"minLength\": 1},\n                    \"mode\": {\"type\": \"string\", \"enum\": [\"add\", \"modify\", \"delete\"]},\n                    \"after\": {\"type\": [\"string\", \"null\"]},\n                    \"before_sha\": {\"type\": [\"string\", \"null\"]},\n                },\n            },\n        },\n    },\n}",
  "src/cadence/dev/patch_builder.py": "# src/cadence/dev/patch_builder.py\n\"\"\"\nPatchBuilder – convert a ChangeSet into a git-compatible unified diff.\n\nGuarantees:\n• Only repository-relative paths (`a/<path>`, `b/<path>`).\n• Trailing newline (git apply requirement).\n• Patch passes `git apply --check`.\n\n2025-06-24 fix\n──────────────\nEliminate `/var/.../shadow/...` leakage and the `./` path prefix by\nrewriting *all* header lines emitted by `git diff`.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nfrom pathlib import Path\nfrom shutil import copytree\nfrom tempfile import TemporaryDirectory\n\nfrom .change_set import ChangeSet, FileEdit\n\n\nclass PatchBuildError(RuntimeError):\n    \"\"\"Bad ChangeSet → diff generation failed.\"\"\"\n\n\n# ────────────────────────────────────────────────────────────────────────────\n# Public API\n# ────────────────────────────────────────────────────────────────────────────\ndef build_patch(change_set: ChangeSet, repo_dir: str | Path) -> str:\n    \"\"\"\n    Return a validated unified diff for *change_set* relative to *repo_dir*.\n    \"\"\"\n    repo_dir = Path(repo_dir).resolve()\n    change_set.validate_against_repo(repo_dir)\n\n    with TemporaryDirectory() as tmp:\n        shadow = Path(tmp) / \"shadow\"\n        copytree(repo_dir, shadow, dirs_exist_ok=True)\n\n        for edit in change_set.edits:\n            _apply_edit_to_shadow(edit, shadow)\n\n        # git diff runs inside repo → left side \".\" (no absolute path leakage)\n        proc = subprocess.run(\n            [\n                \"git\",\n                \"diff\",\n                \"--no-index\",\n                \"--binary\",\n                \"--relative\",\n                \"--src-prefix=a/\",\n                \"--dst-prefix=b/\",\n                \"--\",\n                \".\",          # ← repo root\n                str(shadow),  # ← modified copy\n            ],\n            cwd=repo_dir,\n            capture_output=True,\n            text=True,\n        )\n\n        if proc.returncode not in (0, 1):  # 0 = identical, 1 = diff produced\n            raise PatchBuildError(proc.stderr.strip())\n\n        patch = _rewrite_headers(proc.stdout, shadow)\n\n        if not patch.strip():\n            raise PatchBuildError(\"ChangeSet produced an empty diff.\")\n        if not patch.endswith(\"\\n\"):\n            patch += \"\\n\"\n\n        _ensure_patch_applies(patch, repo_dir)\n        return patch\n\n\n# ────────────────────────────────────────────────────────────────────────────\n# Helpers\n# ────────────────────────────────────────────────────────────────────────────\ndef _apply_edit_to_shadow(edit: FileEdit, shadow_root: Path) -> None:\n    target = shadow_root / edit.path\n    if edit.mode == \"delete\":\n        target.unlink(missing_ok=True)\n        return\n\n    target.parent.mkdir(parents=True, exist_ok=True)\n    if edit.after is None:\n        raise PatchBuildError(f\"`after` content required for mode={edit.mode}\")\n    target.write_text(edit.after, encoding=\"utf-8\")\n\n\ndef _rewrite_headers(raw: str, shadow_root: Path) -> str:\n    \"\"\"\n    Fix header lines emitted by `git diff`:\n\n        diff --git a/./src/foo.py b/<tmp>/shadow/src/foo.py\n        --- a/./src/foo.py\n        +++ b/<tmp>/shadow/src/foo.py\n\n    becomes\n\n        diff --git a/src/foo.py b/src/foo.py\n        --- a/src/foo.py\n        +++ b/src/foo.py\n    \"\"\"\n    shadow_prefix = str(shadow_root) + os.sep\n    fixed: list[str] = []\n\n    for line in raw.splitlines():\n        if line.startswith(\"diff --git \"):\n            _, _, paths = line.partition(\"diff --git \")\n            left, right = paths.split(\" \", maxsplit=1)\n            left = left.replace(\"a/./\", \"a/\")  # drop './'\n            right = _strip_shadow(right, shadow_prefix)\n            fixed.append(f\"diff --git {left} {right}\")\n        elif line.startswith(\"--- \"):\n            fixed.append(\"\".join((\"--- \", line[4:].replace(\"a/./\", \"a/\"))))\n        elif line.startswith(\"+++ \"):\n            cleaned = line[4:]\n            cleaned = cleaned.replace(\"b/./\", \"b/\")\n            cleaned = _strip_shadow(cleaned, shadow_prefix, prefix=\"b/\")\n            fixed.append(\"+++ \" + cleaned)\n        else:\n            fixed.append(line)\n    return \"\\n\".join(fixed)\n\n\ndef _strip_shadow(path: str, shadow_prefix: str, *, prefix: str = \"b/\") -> str:\n    \"\"\"\n    Normalise any header path that still contains the absolute TemporaryDirectory\n    copy of the repo (the “…/shadow/…” component) so that **only repository-relative\n    paths remain**::\n\n         b/var/folders/.../shadow/src/foo.py  ->  b/src/foo.py\n         a/var/folders/.../shadow/src/foo.py  ->  a/src/foo.py\n    \"\"\"\n    # 1. Peel off the a/ or b/ token so the search is position-agnostic\n    leading = \"\"\n    rest = path\n    if path.startswith((\"a/\", \"b/\")):\n        leading, rest = path[:2], path[2:]        # keep 'a/' or 'b/' for later\n\n    # 2. Find the *shadow* directory irrespective of how the tmp path is prefixed\n    #    Examples that must all be normalised:\n    #      var/folders/…/tmpabcd/shadow/src/foo.py\n    #      private/var/…/tmpabcd/shadow/src/foo.py\n    shadow_marker = f\"{os.sep}shadow{os.sep}\"\n\n    if shadow_prefix in rest:\n        _, _, tail = rest.partition(shadow_prefix)\n    elif shadow_marker in rest:\n        _, _, tail = rest.partition(shadow_marker)\n    else:\n        # Nothing to rewrite – re-attach original leading token and return\n        return leading + rest\n\n    # Drop any leading slash the partition may have preserved\n    if tail.startswith(os.sep):\n        tail = tail[len(os.sep):]\n\n    new_prefix = prefix if leading == \"b/\" else \"a/\"\n    return new_prefix + tail\n\n\ndef _ensure_patch_applies(patch: str, repo: Path) -> None:\n    \"\"\"Raise PatchBuildError if the patch would not apply cleanly.\"\"\"\n    proc = subprocess.run(\n        [\"git\", \"apply\", \"--check\", \"-\"],\n        input=patch,\n        text=True,\n        cwd=repo,\n        capture_output=True,\n    )\n    if proc.returncode != 0:\n        raise PatchBuildError(f\"Generated patch does not apply: {proc.stderr.strip()}\")",
  "tests/backlog_blocked_filtering.py": "# tests/backlog_blocked_filtering.py\nfrom src.cadence.dev.backlog import BacklogManager, TaskStructureError\nimport os, uuid\n\ndef test_blocked_tasks_are_filtered():\n    path = f\".test_backlog_{uuid.uuid4().hex}.json\"\n    try:\n        mgr = BacklogManager(path)\n        # Add an open task\n        t1 = {\"title\": \"Task open\", \"type\": \"micro\", \"status\": \"open\", \"created_at\": \"2024-07-01T12:00\", \"id\": \"T1\"}\n        mgr.add_item(t1)\n        # Add a blocked task\n        t2 = {\"title\": \"Task blocked\", \"type\": \"micro\", \"status\": \"blocked\", \"created_at\": \"2024-07-01T12:01\", \"id\": \"T2\"}\n        mgr.add_item(t2)\n        # Add another open task\n        t3 = {\"title\": \"Task open2\", \"type\": \"micro\", \"status\": \"open\", \"created_at\": \"2024-07-01T12:02\", \"id\": \"T3\"}\n        mgr.add_item(t3)\n        open_tasks = mgr.list_items(\"open\")\n        ids = {t[\"id\"] for t in open_tasks}\n        assert \"T2\" not in ids, \"Blocked tasks must not appear in open list\"\n        assert \"T1\" in ids and \"T3\" in ids\n        # Also test list_items(\"all\")\n        all_tasks = mgr.list_items(\"all\")\n        all_ids = {t[\"id\"] for t in all_tasks}\n        assert \"T2\" in all_ids\n    finally:\n        if os.path.exists(path):\n            os.remove(path)\n\ndef test_invalid_status_rejected():\n    path = f\".test_backlog_{uuid.uuid4().hex}.json\"\n    try:\n        mgr = BacklogManager(path)\n        bad_task = {\"title\": \"Bad status\", \"type\": \"micro\", \"status\": \"nonsense\", \"created_at\": \"2024-07-01T12:03\", \"id\": \"T4\"}\n        try:\n            mgr.add_item(bad_task)\n            assert False, \"TaskStructureError should have been raised for invalid status\"\n        except TaskStructureError:\n            pass\n    finally:\n        if os.path.exists(path):\n            os.remove(path)\n",
  "tests/test_file_builder_relative_paths.py": "from __future__ import annotations\nimport subprocess, uuid, textwrap\nfrom pathlib import Path\nfrom cadence.dev.change_set import ChangeSet, FileEdit\nfrom cadence.dev.patch_builder import build_patch\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal git repo with one python file.\"\"\"\n    repo = tmp_path / \"repo\"\n    (repo / \"src\").mkdir(parents=True)\n    target = repo / \"src\" / \"demo.py\"\n    target.write_text(\"def foo():\\n    return 1\\n\", encoding=\"utf8\")\n\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    return repo\n\ndef test_patch_builder_generates_relative_paths(tmp_path: Path):\n    repo = _init_repo(tmp_path)\n\n    # Build a ChangeSet that modifies src/demo.py\n    new_code = \"def foo():\\n    return 42\\n\"\n    cs = ChangeSet(\n        edits=[\n            FileEdit(\n                path=\"src/demo.py\",\n                mode=\"modify\",\n                after=new_code,\n            )\n        ],\n        message=\"change return value\",\n    )\n\n    patch = build_patch(cs, repo)\n\n    # --- Assertions -------------------------------------------------\n    # 1. No absolute /var/…/shadow path left in the diff\n    assert \"/shadow/\" not in patch, \"shadow path still present in patch\"\n\n    # 2. Headers are repository-relative\n    assert patch.startswith(\"--- a/src/demo.py\"), \"unexpected from-file header\"\n    assert \"\\n+++ b/src/demo.py\" in patch, \"unexpected to-file header\"\n\n    # 3. Patch applies cleanly to the working tree\n    subprocess.run([\"git\", \"apply\", \"--check\", \"-\"], cwd=repo, input=patch,\n                   text=True, check=True)",
  "tests/test_shell_failure_persistence.py": "\"\"\"\nRegression tests — Shell failure persistence\n============================================\n\nGoal\n----\nAssert that *every* failing shell operation executed through\n`cadence.dev.shell.ShellRunner` writes an explicit `failed_<stage>`\nsnapshot to the provided `TaskRecord` **before** the error propagates.\n\nWe stub `subprocess.run` so the tests are hermetic (no real git/pytest\ninvocations) and execute in milliseconds.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport subprocess\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import List, Tuple\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Helpers / stubs\n# --------------------------------------------------------------------------- #\nclass _FakeTaskRecord:\n    \"\"\"Minimal in-memory stand-in for cadence.dev.record.TaskRecord.\"\"\"\n\n    def __init__(self) -> None:\n        self.calls: List[dict] = []\n\n    # Signature matches real .save()\n    def save(self, task, state: str, extra: dict | None = None):\n        self.calls.append({\"task\": task, \"state\": state, \"extra\": extra or {}})\n\n\n@pytest.fixture(autouse=True)\ndef _ensure_importable(monkeypatch):\n    \"\"\"\n    Make the repository root (containing ``src/``) importable **everywhere**\n    so the tests run from any working directory or CI container.\n    \"\"\"\n    proj_root = Path(__file__).resolve().parents[1]\n    if (proj_root / \"src\").exists():\n        monkeypatch.syspath_prepend(str(proj_root))\n    yield\n\n\ndef _proc(rc=1, *, stdout: str = \"\", stderr: str = \"\") -> SimpleNamespace:\n    \"\"\"Return a dummy CompletedProcess-like object.\"\"\"\n    return SimpleNamespace(returncode=rc, stdout=stdout, stderr=stderr)\n\n\ndef _patch_subprocess(monkeypatch, mapping: dict[Tuple[str, str], SimpleNamespace]):\n    \"\"\"\n    Replace ``subprocess.run`` so that:\n\n        key = tuple(cmd[:2])   # e.g. (“git”, “apply”)\n\n    If *key* is in *mapping* → return that DummyProcess.\n    Otherwise → succeed (rc=0).\n    \"\"\"\n\n    def _fake_run(cmd, **_kwargs):\n        key = tuple(cmd[:2])\n        return mapping.get(key, _proc(rc=0))\n\n    monkeypatch.setattr(subprocess, \"run\", _fake_run)\n\n\ndef _make_runner(tmp_path: Path, record: _FakeTaskRecord):\n    \"\"\"Set up a ShellRunner pointed at an empty temp repo dir.\"\"\"\n    from src.cadence.dev.shell import ShellRunner\n\n    repo_dir = tmp_path / \"repo\"\n    repo_dir.mkdir()\n    runner = ShellRunner(repo_dir=str(repo_dir), task_record=record)\n    runner.attach_task({\"id\": \"task-1\", \"title\": \"demo\", \"status\": \"open\"})\n    return runner, repo_dir\n\n\n# --------------------------------------------------------------------------- #\n# Tests\n# --------------------------------------------------------------------------- #\ndef test_git_apply_failure_persists(monkeypatch, tmp_path: Path):\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _ = _make_runner(tmp_path, record)\n\n    # Simulate `git apply` failing\n    _patch_subprocess(\n        monkeypatch,\n        {(\"git\", \"apply\"): _proc(stderr=\"boom\")},\n    )\n\n    with pytest.raises(ShellCommandError):\n        runner.git_apply(\"--- broken diff\")\n\n    assert record.calls, \"TaskRecord.save was not called on failure\"\n    snapshot = record.calls[-1]\n    assert snapshot[\"state\"] == \"failed_git_apply\"\n    assert \"boom\" in snapshot[\"extra\"].get(\"error\", \"\") or \"boom\" in snapshot[\"extra\"].get(\n        \"output\", \"\"\n    )\n\n\ndef test_pytest_failure_persists(monkeypatch, tmp_path: Path):\n    record = _FakeTaskRecord()\n    runner, repo_dir = _make_runner(tmp_path, record)\n\n    # Ensure ./tests exists so run_pytest() doesn't raise path-missing error\n    (repo_dir / \"tests\").mkdir()\n\n    _patch_subprocess(\n        monkeypatch,\n        {(\"pytest\", \"-q\"): _proc(stdout=\"F..\", stderr=\"1 failed\")},\n    )\n\n    result = runner.run_pytest()\n    assert result[\"success\"] is False, \"stubbed pytest should fail\"\n\n    snapshot = record.calls[-1]\n    assert snapshot[\"state\"] == \"failed_pytest\"\n    assert \"1 failed\" in snapshot[\"extra\"][\"output\"]\n\n\ndef test_git_commit_failure_persists(monkeypatch, tmp_path: Path):\n    \"\"\"\n    Commit may now fail **either** because prerequisites were not met\n    (*phase-guard short-circuit*) **or** because `git commit` itself\n    returns a non-zero exit code.  Both paths must record a snapshot\n    with state ``failed_git_commit``.\n    \"\"\"\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _ = _make_runner(tmp_path, record)\n\n    # `git add` succeeds, `git commit` fails with \"nothing to commit\"\n    mapping = {\n        (\"git\", \"add\"): _proc(rc=0),\n        (\"git\", \"commit\"): _proc(rc=1, stderr=\"nothing to commit\"),\n    }\n    _patch_subprocess(monkeypatch, mapping)\n\n    with pytest.raises(ShellCommandError):\n        runner.git_commit(\"empty commit\")\n\n    snapshot = record.calls[-1]\n    assert snapshot[\"state\"] == \"failed_git_commit\"\n    # Accept either the original git-level error or the new phase-guard msg\n    err_msg = snapshot[\"extra\"][\"error\"]\n    assert (\n        \"nothing to commit\" in err_msg\n        or \"missing prerequisite phase\" in err_msg\n        or \"missing prerequisite phase(s)\" in err_msg\n    )",
  "tests/test_add.py": "from cadence.utils.add import add\n\ndef test_add():\n    assert add(2, 3) == 5",
  "tests/test_concurrency_locking.py": "# tests/test_concurrency_locking.py\n\"\"\"\nConcurrency / locking integration tests\n=======================================\n\nObjective\n---------\nEnsure that the new RLock-based protection in BacklogManager and TaskRecord\nprevents race-condition corruption when many threads mutate the same\nobjects *simultaneously*.\n\nThe test intentionally shares a single instance across  multiple threads\nto stress intra-process locking.  Cross-process safety (file-level\nlocking) is out-of-scope for this change-set.\n\"\"\"\n\nfrom __future__ import annotations\nimport json\nimport sys\nimport threading\nimport uuid\nfrom pathlib import Path\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Helper – ensure the repo \"src/\" folder is importable inside the test run\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _ensure_importable(monkeypatch):\n    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n    if (PROJECT_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJECT_ROOT))\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# BacklogManager concurrency test\n# --------------------------------------------------------------------------- #\ndef test_backlog_thread_safety(tmp_path: Path):\n    from src.cadence.dev.backlog import BacklogManager\n\n    backlog_path = tmp_path / \"backlog.json\"\n    mgr = BacklogManager(str(backlog_path))\n\n    THREADS = 10\n    TASKS_PER_THREAD = 100\n\n    def _worker(tid: int):\n        for i in range(TASKS_PER_THREAD):\n            mgr.add_item(\n                {\n                    \"title\": f\"task {tid}-{i}\",\n                    \"type\": \"micro\",\n                    \"status\": \"open\",\n                    \"created_at\": \"2025-06-21T00:00:00Z\",\n                }\n            )\n\n    threads = [threading.Thread(target=_worker, args=(n,)) for n in range(THREADS)]\n    for th in threads:\n        th.start()\n    for th in threads:\n        th.join(timeout=10)\n        assert not th.is_alive(), \"thread hung – possible deadlock\"\n\n    # Validate in-memory state\n    items = mgr.list_items(status=\"all\")\n    assert len(items) == THREADS * TASKS_PER_THREAD, \"missing or duplicate tasks in memory\"\n\n    # Validate on-disk JSON integrity\n    on_disk = json.loads(backlog_path.read_text())\n    assert len(on_disk) == len(items), \"disk state differs from memory state\"\n\n\n# --------------------------------------------------------------------------- #\n# TaskRecord concurrency test\n# --------------------------------------------------------------------------- #\ndef test_taskrecord_thread_safety(tmp_path: Path):\n    from src.cadence.dev.record import TaskRecord\n\n    record_path = tmp_path / \"record.json\"\n    tr = TaskRecord(str(record_path))\n\n    THREADS = 8\n    SAVES_PER_THREAD = 75\n\n    def _worker():\n        for _ in range(SAVES_PER_THREAD):\n            task_id = str(uuid.uuid4())\n            task = {\"id\": task_id, \"title\": \"concurrency\", \"status\": \"open\"}\n            tr.save(task, state=\"init\")\n\n    threads = [threading.Thread(target=_worker) for _ in range(THREADS)]\n    for th in threads:\n        th.start()\n    for th in threads:\n        th.join(timeout=10)\n        assert not th.is_alive(), \"thread hung – possible deadlock\"\n\n    # Verify integrity: unique task_id for each record\n    data = tr.load()\n    ids = [rec[\"task_id\"] for rec in data]\n    assert len(ids) == len(set(ids)), \"duplicate task_id detected – race condition?\"\n    assert len(ids) == THREADS * SAVES_PER_THREAD, \"missing records – some saves lost\"\n",
  "tests/test_phase_ordering_and_precheck.py": "\"\"\"\nTests for ShellRunner: diff pre-check & phase-ordering\n=====================================================\n\nThese tests verify that\n\n1.  A patch whose *before* image does **not** match the working tree\n    fails during the *pre-check* stage and records the correct snapshot.\n\n2.  `git_commit` is refused unless both *patch_applied* **and**\n    *tests_passed* phases are already recorded for the current task.\n\n3.  When phases are executed in the correct order\n    (apply → tests → commit) the commit succeeds and the *committed*\n    phase flag is set.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport subprocess\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import Dict, List, Tuple\n\nimport pytest\n\n# --------------------------------------------------------------------------- #\n# Helper – fake in-memory TaskRecord\n# --------------------------------------------------------------------------- #\nclass _FakeTaskRecord:\n    def __init__(self) -> None:\n        self.calls: List[dict] = []\n\n    def save(self, task, state: str, extra: dict | None = None):\n        self.calls.append({\"task\": task, \"state\": state, \"extra\": extra or {}})\n\n\n# --------------------------------------------------------------------------- #\n# Pytest fixtures / stubs\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _ensure_importable(monkeypatch):\n    \"\"\"\n    Ensure ``src/`` is import-searchable regardless of the cwd that the\n    test runner happens to use.\n    \"\"\"\n    proj_root = Path(__file__).resolve().parents[1]\n    if (proj_root / \"src\").exists():\n        monkeypatch.syspath_prepend(str(proj_root))\n    yield\n\n\ndef _proc(rc: int = 1, *, stdout: str = \"\", stderr: str = \"\") -> SimpleNamespace:\n    \"\"\"Return a dummy CompletedProcess-like object.\"\"\"\n    return SimpleNamespace(returncode=rc, stdout=stdout, stderr=stderr)\n\n\ndef _patch_subprocess(monkeypatch, mapping: Dict[Tuple[str, str], SimpleNamespace]):\n    \"\"\"\n    Monkey-patch ``subprocess.run`` so that the first two CLI tokens form a\n    lookup key.  If the key exists in *mapping* we return that fake\n    process; otherwise return a zero-exit stub.\n    \"\"\"\n\n    def _fake_run(cmd, **_kwargs):\n        key = tuple(cmd[:2])\n        return mapping.get(key, _proc(rc=0))\n\n    monkeypatch.setattr(subprocess, \"run\", _fake_run)\n\n\ndef _make_runner(tmp_path: Path, record: _FakeTaskRecord):\n    \"\"\"Return a (runner, repo_dir, task_id) triple.\"\"\"\n    from src.cadence.dev.shell import ShellRunner\n\n    repo_dir = tmp_path / \"repo\"\n    repo_dir.mkdir()\n    task = {\"id\": \"task-xyz\", \"title\": \"demo\", \"status\": \"open\"}\n    runner = ShellRunner(repo_dir=str(repo_dir), task_record=record)\n    runner.attach_task(task)\n    return runner, repo_dir, task[\"id\"]\n\n\n# --------------------------------------------------------------------------- #\n# Test 1 – diff pre-check failure\n# --------------------------------------------------------------------------- #\ndef test_patch_precheck_failure(monkeypatch, tmp_path: Path):\n    \"\"\"\n    git apply --check returns non-zero → ShellRunner must raise and record\n    ``failed_git_apply`` without setting *patch_applied*.\n    \"\"\"\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _repo_dir, tid = _make_runner(tmp_path, record)\n\n    # Pre-check fails\n    _patch_subprocess(monkeypatch, {(\"git\", \"apply\"): _proc(stderr=\"mismatch\")})\n\n    with pytest.raises(ShellCommandError):\n        runner.git_apply(\"--- broken diff\")\n\n    # Snapshot written\n    snap = record.calls[-1]\n    assert snap[\"state\"] == \"failed_git_apply\"\n    assert \"mismatch\" in snap[\"extra\"][\"error\"] or \"mismatch\" in snap[\"extra\"].get(\n        \"output\", \"\"\n    )\n\n    # Phase flag **not** set\n    assert not runner._has_phase(tid, \"patch_applied\")  # pylint: disable=protected-access\n\n\n# --------------------------------------------------------------------------- #\n# Test 2 – commit refused when prerequisites are missing\n# --------------------------------------------------------------------------- #\ndef test_commit_refused_without_prerequisites(monkeypatch, tmp_path: Path):\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _repo_dir, _tid = _make_runner(tmp_path, record)\n\n    # Underlying git commands would *succeed* but the phase guard should\n    # short-circuit first.\n    _patch_subprocess(\n        monkeypatch,\n        {\n            (\"git\", \"add\"): _proc(rc=0),\n            (\"git\", \"commit\"): _proc(rc=0),  # never reached\n        },\n    )\n\n    with pytest.raises(ShellCommandError) as exc:\n        runner.git_commit(\"should fail\")\n\n    assert \"missing prerequisite phase\" in str(exc.value)\n    snap = record.calls[-1]\n    assert snap[\"state\"] == \"failed_git_commit\"\n\n\n# --------------------------------------------------------------------------- #\n# Test 3 – happy-path: apply → tests → commit\n# --------------------------------------------------------------------------- #\ndef test_full_success_flow(monkeypatch, tmp_path: Path):\n    \"\"\"\n    Execute the correct phase sequence and assert that commit succeeds and\n    the internal *committed* flag is set.\n    \"\"\"\n    record = _FakeTaskRecord()\n    runner, repo_dir, tid = _make_runner(tmp_path, record)\n\n    # --- make an empty ./tests folder so ShellRunner.run_pytest() passes its\n    #     early path-existence guard.\n    (Path(repo_dir) / \"tests\").mkdir()\n\n    sha = \"abc123\"\n\n    _patch_subprocess(\n        monkeypatch,\n        {\n            # Patch pre-check OK, apply OK\n            (\"git\", \"apply\"): _proc(rc=0),\n            # Pytest green\n            (\"pytest\", \"-q\"): _proc(rc=0, stdout=\"\"),\n            # Git plumbing\n            (\"git\", \"add\"): _proc(rc=0),\n            (\"git\", \"commit\"): _proc(rc=0),\n            (\"git\", \"rev-parse\"): _proc(rc=0, stdout=f\"{sha}\\n\"),\n        },\n    )\n\n    # 1. apply\n    runner.git_apply(\"--- dummy diff\")\n\n    # 2. tests\n    py_res = runner.run_pytest()\n    assert py_res[\"success\"] is True\n\n    # 3. commit\n    out_sha = runner.git_commit(\"commit msg\")\n    assert out_sha == sha\n    assert runner._has_phase(tid, \"committed\")  # pylint: disable=protected-access",
  "tests/test_failed_rollback.py": "\"\"\"\nRegression-test — Atomic rollback on downstream failure\n=======================================================\n\nPurpose\n-------\nVerify that *any* failure **after** a patch is applied but **before**\ncommit triggers an automatic rollback that restores a pristine working\ntree **and** writes the correct snapshots to TaskRecord.\n\nStrategy\n--------\n1.  Start with a clean repo where utils.add() is *correct* and all tests\n    pass.\n\n2.  Backlog contains a task whose patch **adds a brand-new failing test\n    file** – this guarantees pytest will fail *if* the patch is applied,\n    regardless of implementation details.\n\n3.  Run a full `DevOrchestrator` cycle (non-interactive).\n\n4.  Assert:\n        ─ orchestrator reports failure at the *test* stage;\n        ─ TaskRecord contains both `\"failed_test\"` **and**\n          `\"failed_test_and_rollback\"` snapshots;\n        ─ the failing test file is gone (working tree restored);\n        ─ original tests pass again and git status is clean.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Global stubs – applied automatically\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _stub_external(monkeypatch):\n    \"\"\"Stub out optional / external deps so the test is hermetic.\"\"\"\n    # Fake OpenAI client (LLM not used by this path)\n    fake_openai = sys.modules[\"openai\"] = type(sys)(\"openai\")\n    fake_openai.AsyncOpenAI = lambda *a, **k: None  # type: ignore\n    fake_openai.OpenAI = lambda *a, **k: None       # type: ignore\n\n    # Fake tabulate (pretty-printer)\n    fake_tabulate = sys.modules[\"tabulate\"] = type(sys)(\"tabulate\")\n    fake_tabulate.tabulate = lambda *a, **k: \"\"\n\n    # Satisfy LLMClient env check\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"dummy\")\n\n    # Ensure repository *parent* (containing “src/”) is importable\n    PROJ_ROOT = Path(__file__).resolve().parents[1]\n    if (PROJ_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJ_ROOT))\n\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# Repo bootstrap helpers\n# --------------------------------------------------------------------------- #\nGOOD_IMPL = \"def add(x, y):\\n    return x + y\\n\"\nFAILING_TEST = (\n    \"def test_intentional_failure():\\n\"\n    \"    assert False, 'This test is added by the patch and must fail'\\n\"\n)\n\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal Cadence project inside a temporary git repo.\"\"\"\n    repo = tmp_path\n\n    # --- source package ----------------------------------------------------\n    pkg_root = repo / \"src\" / \"cadence\" / \"utils\"\n    pkg_root.mkdir(parents=True, exist_ok=True)\n    (repo / \"src\" / \"__init__.py\").write_text(\"\")\n    (repo / \"src\" / \"cadence\" / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"add.py\").write_text(GOOD_IMPL)\n\n    # --- baseline passing test --------------------------------------------\n    tests_dir = repo / \"tests\"\n    tests_dir.mkdir()\n    (tests_dir / \"test_add.py\").write_text(\n        \"import sys, pathlib, os\\n\"\n        \"sys.path.insert(0, os.fspath((pathlib.Path(__file__).resolve().parents[2] / 'src')))\\n\"\n        \"from cadence.utils.add import add\\n\"\n        \"\\n\"\n        \"def test_add():\\n\"\n        \"    assert add(2, 3) == 5\\n\"\n    )\n\n    # --- git init ----------------------------------------------------------\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run(\n        [\"git\", \"commit\", \"-m\", \"initial good implementation\"],\n        cwd=repo,\n        check=True,\n        stdout=subprocess.DEVNULL,\n    )\n    return repo\n\n\ndef _make_backlog(repo: Path, record_file: Path) -> Path:\n    \"\"\"Write backlog.json with one task that *adds* a failing test.\"\"\"\n    task = {\n        \"id\": \"task-add-failing-test\",\n        \"title\": \"Add failing test to trigger rollback\",\n        \"type\": \"micro\",\n        \"status\": \"open\",\n        \"created_at\": \"2025-06-21T00:00:00Z\",\n        \"diff\": {\n            # New file relative to repo root\n            \"file\": \"tests/test_break.py\",\n            \"before\": \"\",                 # new file → empty 'before'\n            \"after\":  FAILING_TEST,\n        },\n    }\n    backlog_path = repo / \"backlog.json\"\n    backlog_path.write_text(json.dumps([task], indent=2))\n    record_file.write_text(\"[]\")  # fresh record\n    return backlog_path\n\n\ndef _orch_cfg(repo: Path, backlog: Path, record: Path) -> dict:\n    \"\"\"Return minimal DevOrchestrator config.\"\"\"\n    return {\n        \"backlog_path\": str(backlog),\n        \"template_file\": None,\n        \"src_root\": str(repo),\n        \"ruleset_file\": None,\n        \"repo_dir\": str(repo),\n        \"record_file\": str(record),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# The actual test\n# --------------------------------------------------------------------------- #\ndef test_atomic_rollback_on_failed_tests(tmp_path: Path):\n    \"\"\"\n    Full DevOrchestrator run — must:\n        • fail at test phase,\n        • rollback applied patch,\n        • leave working tree clean.\n    \"\"\"\n    repo = _init_repo(tmp_path)\n    record_file = repo / \"dev_record.json\"\n    backlog_file = _make_backlog(repo, record_file)\n\n    # Import *after* stubs are in place\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator(_orch_cfg(repo, backlog_file, record_file))\n    result = orch.run_task_cycle(select_id=\"task-add-failing-test\", interactive=False)\n\n    # ---- orchestrator result ---------------------------------------------\n    assert result[\"success\"] is False\n    assert result[\"stage\"] == \"test\"\n\n    # ---- TaskRecord snapshots --------------------------------------------\n    history: List[dict] = json.loads(record_file.read_text())[0][\"history\"]\n    states = [snap[\"state\"] for snap in history]\n    assert \"failed_test\" in states, \"failure snapshot missing\"\n    assert \"failed_test_and_rollback\" in states, \"rollback snapshot missing\"\n\n    # ---- Working tree validation -----------------------------------------\n    # 1. The intentionally failing test must be *gone*\n    assert not (repo / \"tests\" / \"test_break.py\").exists(), \"rollback did not remove new file\"\n\n    # 2. Original add() implementation still correct\n    sys.path.insert(0, str(repo / \"src\"))\n    from cadence.utils.add import add  # noqa: E402  (delayed import)\n\n    assert add(2, 3) == 5\n\n    # 3. Git working tree clean (no tracked-file changes)\n    status = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=repo,\n        stdout=subprocess.PIPE,\n        encoding=\"utf-8\",\n        check=True,\n    ).stdout.strip()\n\n    # Ignore purely *untracked* lines (begin with '??')\n    tracked_changes = [line for line in status.splitlines() if not line.startswith(\"??\")]\n    assert tracked_changes == [], (\n        \"tracked files modified after rollback:\\n\" + \"\\n\".join(tracked_changes)\n    )",
  "tests/test_orchestrator_auto_replenish.py": "\"\"\"\nSmoke-test for DevOrchestrator._ensure_backlog()\n\"\"\"\n\nfrom __future__ import annotations\nimport pytest\n\n\nclass _DummyBacklog:\n    def __init__(self):\n        self.items = []\n\n    def list_items(self, status=\"open\"):\n        return [t for t in self.items if t.get(\"status\") == status]\n\n    def add_item(self, task):\n        self.items.append(dict(task))\n\n\nclass _DummyGenerator:\n    def __init__(self):\n        self.calls = []\n\n    def generate_tasks(self, mode: str, count: int):\n        assert mode == \"micro\"\n        self.calls.append(count)\n        return [\n            {\n                \"id\": f\"gen-{i}\",\n                \"title\": f\"auto-task {i}\",\n                \"type\": \"micro\",\n                \"status\": \"open\",\n                \"created_at\": \"now\",\n            }\n            for i in range(count)\n        ]\n\n\nclass _DummyRecord:\n    def __init__(self):\n        self.snapshots = []\n\n    def save(self, task, state, extra=None):\n        self.snapshots.append(state)\n\n\n@pytest.mark.parametrize(\"count\", [1, 4])\ndef test_auto_replenish(count):\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator.__new__(DevOrchestrator)  # bypass __init__\n    orch.backlog = _DummyBacklog()\n    orch.generator = _DummyGenerator()\n    orch.record = _DummyRecord()\n    orch.backlog_autoreplenish_count = count\n    orch._record = orch.record.save\n\n    orch._ensure_backlog()\n    assert len(orch.backlog.list_items(\"open\")) == count\n    assert \"backlog_replenished\" in orch.record.snapshots",
  "tests/test_state_recording.py": "# tests/test_state_recording.py\n\"\"\"\nIntegration test for TaskRecord integrity.\n\nRuns DevOrchestrator.run_task_cycle twice:\n\n1.  A green run where the patch fixes the bug and pytest passes.\n2.  A red run where the patch is a no-op so pytest fails.\n\nFor each run we assert that:\n    • mandatory state snapshots appear *in order* (allowing extra entries);\n    • `task.status` matches the state for *done* → *archived*;\n    • failure snapshots carry useful diagnostics.\n\nThe test is dependency-free: it stubs `openai` and `tabulate` before any\nCadence import so no network or extra wheels are required.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nfrom datetime import datetime, UTC\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\n# --------------------------------------------------------------------------- #\n# Global stubs – applied automatically by the autouse fixture\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _stub_external(monkeypatch):\n    \"\"\"Stub out optional / external deps so the test runs anywhere.\"\"\"\n    # Fake OpenAI client (LLM not used by DevOrchestrator path)\n    fake_openai = sys.modules[\"openai\"] = type(sys)(\"openai\")\n    fake_openai.AsyncOpenAI = lambda *a, **k: None  # type: ignore\n    fake_openai.OpenAI = lambda *a, **k: None       # type: ignore\n\n    # Fake tabulate to avoid CLI pretty-printer dependency\n    fake_tabulate = sys.modules[\"tabulate\"] = type(sys)(\"tabulate\")\n    fake_tabulate.tabulate = lambda *a, **k: \"\"\n\n    # Env var so LLMClient constructor is happy\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"dummy-key\")\n\n    # --- critical fix: ensure real Cadence code is importable ---------------\n    # We need the directory that CONTAINS the top-level “src/” package.\n    if (PROJECT_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJECT_ROOT))\n    # ----------------------------------------------------------------------- #\n\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# Repo bootstrap helpers\n# --------------------------------------------------------------------------- #\nBAD_IMPL = \"def add(x, y):\\n    return x - 1 + y\\n\"\nGOOD_IMPL = BAD_IMPL.replace(\"- 1 +\", \"+\")\n\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal Cadence project inside a temporary git repo.\"\"\"\n    repo = tmp_path\n\n    # Source package\n    pkg_root = repo / \"src\" / \"cadence\" / \"utils\"\n    pkg_root.mkdir(parents=True, exist_ok=True)\n    # PEP-420 implicit namespace would work, but an explicit file removes\n    # any ambiguity on Py<3.10 or odd tooling.\n    (repo / \"src\" / \"__init__.py\").write_text(\"\")\n    (repo / \"src\" / \"cadence\" / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"add.py\").write_text(BAD_IMPL)\n\n    # Unit test that will pass only if GOOD_IMPL is in place\n    tests_dir = repo / \"tests\"\n    tests_dir.mkdir()\n    (tests_dir / \"test_add.py\").write_text(\n        # Ensure repo/src is on import path _inside_ the pytest subprocess\n        \"import sys, pathlib, os\\n\"\n        \"sys.path.insert(0, os.fspath((pathlib.Path(__file__).resolve().parents[2] / 'src')))\\n\"\n        \"from cadence.utils.add import add\\n\"\n        \"\\n\"\n        \"def test_add():\\n\"\n        \"    assert add(2, 3) == 5\\n\"\n    )\n\n    # Initial git commit so `git apply` has a base tree\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n\n    return repo\n\n\ndef _make_backlog(repo: Path, record_file: Path, *, fix_bug: bool) -> Path:\n    \"\"\"Write backlog.json containing exactly one task and return the path.\"\"\"\n    # For the “red” path we still need a *non-empty* diff so the run\n    # proceeds through patch-apply and into pytest (where it will fail).\n    # - Green run: after_code fixes the defect.\n    # - Red  run: after_code == before_code  → diff is empty → PatchBuildError.\n    after_code = GOOD_IMPL if fix_bug else BAD_IMPL\n    task = {\n        \"id\": \"task-fix-add\",\n        \"title\": \"Fix utils.add bug\",\n        \"type\": \"micro\",\n        \"status\": \"open\",\n        \"created_at\": datetime.now(UTC).isoformat(),\n        \"diff\": {\n            \"file\": \"src/cadence/utils/add.py\",\n            \"before\": BAD_IMPL,\n            \"after\":  after_code,\n        },\n    }\n    backlog = repo / \"backlog.json\"\n    backlog.write_text(json.dumps([task], indent=2))\n    record_file.write_text(\"[]\")   # empty initial record\n    return backlog\n\n\ndef _orch_cfg(repo: Path, backlog: Path, record: Path) -> dict:\n    \"\"\"Return the minimal DevOrchestrator config dict.\"\"\"\n    return {\n        \"backlog_path\": str(backlog),\n        \"template_file\": None,\n        \"src_root\": str(repo),\n        \"ruleset_file\": None,\n        \"repo_dir\": str(repo),\n        \"record_file\": str(record),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# Parametrised integration test\n# --------------------------------------------------------------------------- #\n@pytest.mark.parametrize(\"fix_bug\", [True, False])\ndef test_task_record_snapshots(tmp_path: Path, fix_bug: bool):\n    \"\"\"\n    Ensure TaskRecord snapshots are written after every mutator or failure.\n    \"\"\"\n    repo = _init_repo(tmp_path)\n    record_file = repo / \"dev_record.json\"\n    backlog_file = _make_backlog(repo, record_file, fix_bug=fix_bug)\n\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator(_orch_cfg(repo, backlog_file, record_file))\n    result = orch.run_task_cycle(select_id=\"task-fix-add\", interactive=False)\n\n    # ----------------- Inspect TaskRecord ----------------- #\n    record: List[dict] = json.loads(record_file.read_text())\n    assert len(record) == 1, \"exactly one task record expected\"\n    history = record[0][\"history\"]\n    states = [snap[\"state\"] for snap in history]\n\n    common = [\n        \"build_patch\",\n        \"patch_built\",\n        \"patch_reviewed\",\n        \"patch_applied\",\n        \"pytest_run\",\n    ]\n    if fix_bug:\n        expected_seq = common + [\"committed\", \"status_done\", \"archived\"]\n\n        # Confirm green-path sequence\n        it = iter(states)\n        for label in expected_seq:\n            assert label in it, f\"missing or out-of-order state '{label}'\"\n    else:\n        # Red path: must terminate with some `failed_…` snapshot\n        assert not result[\"success\"], \"red run unexpectedly succeeded\"\n        assert states[-1].startswith(\"failed_\"), \"last snapshot must be a failure state\"\n        # And we still expect the initial 'build_patch' snapshot\n        assert states[0] == \"build_patch\"\n\n    # Semantic checks on snapshot contents\n    if fix_bug:\n        done_ix, arch_ix = states.index(\"status_done\"), states.index(\"archived\")\n        assert history[done_ix][\"task\"][\"status\"] == \"done\"\n        assert history[arch_ix][\"task\"][\"status\"] == \"archived\"\n    else:\n        extra = history[-1][\"extra\"]\n        assert extra, \"failure snapshot must include diagnostics\"\n        assert \"error\" in extra or \"pytest\" in extra",
  "tests/test_llm_json_call.py": "\"\"\"\ntests/test_llm_json_call.py\n===========================\n\nRegression tests for cadence.llm.json_call.LLMJsonCaller\n\nThe stubbed client means no real network traffic is made.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Any, List\n\nimport pytest\n\n# ------------------------------------------------------------------------- #\n# ensure  src/  is importable from any working dir\n# ------------------------------------------------------------------------- #\nPROJECT_ROOT = Path(__file__).resolve().parents[1]\nif (PROJECT_ROOT / \"src\").exists():\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom cadence.llm.json_call import LLMJsonCaller\nfrom cadence.dev.schema import CHANGE_SET_V1\n\n\n# ------------------------------------------------------------------------- #\n# helpers\n# ------------------------------------------------------------------------- #\ndef _minimal_changeset() -> dict[str, Any]:\n    \"\"\"Return the smallest ChangeSet that validates against CHANGE_SET_V1.\"\"\"\n    return {\n        \"message\": \"demo\",\n        \"edits\": [\n            {\n                \"path\": \"foo.py\",\n                \"mode\": \"add\",\n                \"after\": \"print('hi')\",\n                \"before_sha\": None,\n            }\n        ],\n        \"author\": \"\",\n        \"meta\": {},\n    }\n\n\nclass _StubLLM:\n    \"\"\"\n    Very small stand-in for cadence.llm.client.LLMClient.\n\n    • pops predefined responses off a queue;\n    • counts how many times .call() is invoked.\n    \"\"\"\n\n    def __init__(self, responses: List[Any]):\n        self._queue = list(responses)\n        self.call_count = 0\n        self.stub = False  # important – LLMJsonCaller checks this\n\n    # signature compatible with real .call()\n    def call(self, *_a, **_kw):\n        self.call_count += 1\n        if not self._queue:\n            raise RuntimeError(\"StubLLM queue exhausted\")\n        return self._queue.pop(0)\n\n    # async variant (unused here, but keeps interface parity)\n    async def acall(self, *_a, **_kw):\n        return self.call()\n\n\n# ------------------------------------------------------------------------- #\n# pytest fixture that patches get_default_client() for each test\n# ------------------------------------------------------------------------- #\n@pytest.fixture\ndef _patch_llm(monkeypatch):\n    \"\"\"\n    Provide a helper that installs a fresh StubLLM for the current test.\n    \"\"\"\n\n    def _install(responses: List[Any]) -> _StubLLM:\n        from cadence.llm import json_call as _jc_mod\n\n        stub = _StubLLM(responses)\n        monkeypatch.setattr(_jc_mod, \"get_default_client\", lambda: stub)\n        return stub\n\n    return _install\n\n\n# ------------------------------------------------------------------------- #\n# test-cases\n# ------------------------------------------------------------------------- #\ndef test_plain_json_string(_patch_llm):\n    \"\"\"Happy-path: assistant returns plain JSON text.\"\"\"\n    payload = _minimal_changeset()\n    _patch_llm([json.dumps(payload)])\n\n    caller = LLMJsonCaller(schema=CHANGE_SET_V1)\n    obj = caller.ask(\"sys\", \"user\")\n    assert obj == payload\n\n\ndef test_tool_call_dict_return(_patch_llm):\n    \"\"\"\n    Tool-call path: LLMClient.call() returns the parsed dict directly,\n    so LLMJsonCaller must accept the object unchanged.\n    \"\"\"\n    payload = _minimal_changeset()\n    _patch_llm([payload])  # already-parsed dict\n\n    caller = LLMJsonCaller(schema=CHANGE_SET_V1)\n    obj = caller.ask(\"system\", \"user\")\n    assert obj == payload\n\n\ndef test_retry_then_success(_patch_llm, monkeypatch):\n    \"\"\"\n    First response is invalid → LLMJsonCaller retries and\n    succeeds on the second round.\n    \"\"\"\n    bad = \"NOT-JSON\"\n    good = _minimal_changeset()\n    stub = _patch_llm([bad, json.dumps(good), json.dumps(good)])  # queue len ≥ _MAX_RETRIES\n\n    # Skip real waiting during retries\n    import time\n\n    monkeypatch.setattr(time, \"sleep\", lambda *_a, **_kw: None)\n\n    caller = LLMJsonCaller(schema=CHANGE_SET_V1)\n    obj = caller.ask(\"sys\", \"usr\")\n    assert obj == good\n\n    # One bad + one good response must have been consumed\n    assert stub.call_count == 2\n    assert len(stub._queue) == 1",
  "scripts/run_orchestrator.py": "# scripts/run_orchestrator.py\n# ---------------------------------------------------------------------+\n# Bootstrap: ensure repository ROOT (parent of this file’s directory)  +\n# is on sys.path so that 'src.*' namespace packages resolve correctly  +\n# ---------------------------------------------------------------------+\nimport pathlib, sys, os\nROOT = pathlib.Path(__file__).resolve().parents[1]\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\nfrom cadence.dev.orchestrator import DevOrchestrator\n\nCONFIG = {\n    \"backlog_path\": \"dev_backlog.json\",\n    \"template_file\": None,\n    \"src_root\": \"src\",          # <--- correct path\n    \"ruleset_file\": None,\n    \"repo_dir\": \".\",\n    \"record_file\": \"dev_record.json\",\n}\n\nif __name__ == \"__main__\":\n    orch = DevOrchestrator(CONFIG)\n    while True:\n        result = orch.run_task_cycle(interactive=False)\n        if not result.get(\"success\"):\n            break",
  "scripts/seed_round2_backlog.py": "# scripts/seed_round2_backlog.py\nfrom cadence.dev.generator import TaskGenerator\nfrom cadence.dev.backlog   import BacklogManager\n\n# ----- 2.1  create plain-language task shells -------------------------\nTASKS = [\n    {\"title\": \"TASK-1 Auto-replenish backlog\",            \"description\": \"\"\"Title: Auto-replenish backlog when empty\nGoal: Keep the pipeline perpetually flowing without human babysitting.\nImplementation Steps:\n\n1. Add **`DevOrchestrator._ensure_backlog()`** • If **`self.backlog.list_items(\"open\")`** is empty, call **`TaskGenerator.generate_tasks(mode=\"micro\", count=<N>)`** (N default = 3; expose CLI flag). • Persist the newly generated tasks with **`BacklogManager.add_item`**. • Record snapshot: **`state=\"backlog_replenished\"`**, extra={\"count\": N}.\n2. Call **`_ensure_backlog()`** at the very top of **`run_task_cycle()`**.\n3. Unit test: run an orchestrator in a temp repo with an empty backlog, assert it auto-populates.\n\nAcceptance: **`run_task_cycle(interactive=False)`** no longer raises **`RuntimeError`** when no tasks exist.\n\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-2 EfficiencyAgent second review\",     \"description\": \"\"\"Title: Wire EfficiencyAgent as mandatory second review\nGoal: Conform to DEV_PROCESS phase table (“Review” → Reasoning *and* Efficiency).\nImplementation Steps:\n\n1. In **`DevOrchestrator.__init__`** create **`self.efficiency = get_agent(\"efficiency\")`**.\n2. After **first** review passes, call **`eff_review = self.efficiency.run_interaction(<prompt_with_patch>)`** or, simpler for now, reuse **`TaskReviewer`** but tag the state **`\"efficiency_reviewed\"`**.\n3. Fail the task cycle unless both reviews pass.\n4. Record both review results with distinct states: **`\"patch_reviewed_reasoning\"`** / **`\"patch_reviewed_efficiency\"`**.\n5. Extend phase flags so **`git_commit`** requires **`\"efficiency_passed\"`** as well.\n\nAcceptance: A commit cannot occur unless *both* reviews have succeeded; tests updated accordingly.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-3 MetaAgent hook\",                    \"description\": \"\"\"Title: First-class MetaAgent hook\nGoal: Provide real-time governance / drift detection per DEV_PROCESS.\nImplementation Steps:\n\n1. Add simple **`MetaAgent.analyse(run_summary: dict)`** stub that just logs or appends to TaskRecord.\n2. Call it at the end of every **`run_task_cycle()`** (success *or* failure) with the full result dict.\n3. Record state **`\"meta_analysis\"`** plus whatever telemetry the MetaAgent returns.\n4. (Future-proof) Keep invocation behind **`config[\"enable_meta\"]`** flag (default True).\n\nAcceptance: TaskRecord shows a **`meta_analysis`** snapshot for every cycle; meta failures do not crash the run.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-4 Reviewer strict rule types\",        \"description\": \"\"\"Title: Harden TaskReviewer rule parsing\nGoal: Unknown rule types must never be ignored silently.\nImplementation Steps:\n\n1. In **`TaskReviewer._load_ruleset`** raise **`PatchReviewError`** **or** emit **`logger.warning`** when **`type`** is unrecognised.\n2. Provide **`strict`** constructor flag (default True).\n3. Add regression test loading a ruleset with an invalid type → expect exception or warning.\n\nAcceptance: CI fails (or logs) on an unrecognised rule type; no silent pass.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-5 Commit guard review flags\",         \"description\": \"\"\"Title: Expand enforce_phase → include review guards\nGoal: Prevent any commit unless **`\"review_passed\"`** *and* **`\"efficiency_passed\"`** flags exist.\nImplementation Steps:\n\n1. Add new decorator usage or explicit check in **`ShellRunner.git_commit`**: required = [\"patch_applied\", \"tests_passed\", \"review_passed\", \"efficiency_passed\"]\n2. Set those flags inside DevOrchestrator right after each successful review.\n3. Update tests in test_phase_ordering_and_precheck.py to assert commit fails without both review flags.\n\nAcceptance: New tests pass; existing tests updated to set the new flags on the happy path.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-6 Cross-process file locks\",          \"description\": \"\"\"Title: Cross-process file-locking for backlog & record\nGoal: Prevent two orchestrators on the same repo from racing.\nImplementation Steps:\n\n1. Add lightweight cross-process lock via **`filelock`** (pip-light) or portalocker.\n2. Acquire the lock in **`.save()`** and **`.load()`** of BacklogManager & TaskRecord *in addition* to the existing RLock. Lock file path = **`<jsonfile>.lock`**.\n3. Time-out (e.g., 10 s) then raise custom **`FileLockTimeoutError`**; caller should retry or alert.\n4. Add smoke test: spawn two **`multiprocessing.Process`** objects that hammer **`.add_item`**; assert no JSON corruption.\n\nAcceptance: Concurrency test passes; manual ctrl-C leaves **`.lock`** cleaned up.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-7 LLMClient stub mode\",               \"description\": \"\"\"Title: Graceful LLMClient fallback when env is missing\nGoal: Allow offline/CI runs without exporting OPENAI_API_KEY.\nImplementation Steps:\n\n1. In **`LLMClient.__init__`**, if api_key is missing: – log a **warning**; – enter “stub-mode”: **`.call()`** and **`.acall()`** return a canned message (e.g., **`\"LLM unavailable\"`**).\n2. Add **`self.stub = True`** flag; tests can assert behaviour.\n3. Update existing CI tests to expect stub-mode (they already monkey-patch OpenAI).\n\nAcceptance: Running orchestrator without the env var no longer crashes; warning is emitted exactly once per process.\"\"\", \"status\": \"open\"},\n]\n\ntg = TaskGenerator()\nwith_backfill = [*TASKS]            # TaskGenerator will fill id/created_at\nbm = BacklogManager(\"dev_backlog.json\")\nfor t in with_backfill:\n    bm.add_item(t)\n\nprint(f\"Backlog now contains {len(bm.list_items('open'))} open tasks.\")",
  "tools/module_contexts.py": "\nimport os\nimport json\nimport ast\nimport re\n\nEXCLUDES = {'archive', 'temp', 'code_payloads', '.git', '.pytest_cache', '__pycache__'}\nROOT = os.getcwd()\nCONTEXT_JSON = \"module_contexts.json\"\n\nDEFAULT_CONTEXT = dict(\n    purpose=\"\",\n    public_api=[],\n    depends_on=[],\n    used_by=[],\n    direct_imports=[],\n    related_schemas=[],\n    context_window_expected=\"\",\n    escalation_review=\"\",\n)\n\ndef relpath(path):\n    return os.path.relpath(path, ROOT).replace(os.sep, \"/\")\n\ndef get_module_import_path(rel_path):\n    # \"cadence/dev/executor.py\" -> \"cadence.dev.executor\"\n    p = rel_path\n    if p.endswith(\".py\"):\n        p = p[:-3]\n    if p.endswith(\"/__init__\"):\n        p = p[:-9]\n    return p.replace(\"/\", \".\")\n\ndef extract_and_strip_shebang_and_futures(lines):\n    shebang = None\n    futures = []\n    body = []\n    for line in lines:\n        if shebang is None and line.startswith(\"#!\"):\n            shebang = line\n            continue\n        m = re.match(r\"\\s*from __future__ import\", line)\n        if m:\n            # Avoid duplicates, but preserve order\n            if line not in futures:\n                futures.append(line)\n            continue\n        body.append(line)\n    return shebang, futures, body\n\ndef strip_duplicate_headers_at_top(lines):\n    \"\"\"Remove all context summary header blocks at the file top (before any code).\"\"\"\n    out = []\n    i = 0\n    n = len(lines)\n    while i < n:\n        line = lines[i]\n        # Allow blank lines and comments to stay at top\n        if line.strip() == \"\" or line.strip().startswith(\"#\"):\n            out.append(line)\n            i += 1\n            continue\n        # Remove all context headers at the top\n        if \"# MODULE CONTEXT SUMMARY\" in line:\n            while i < n and \"# END MODULE CONTEXT SUMMARY\" not in lines[i]:\n                i += 1\n            if i < n:\n                i += 1  # Skip END marker\n            # Keep going in case of further headers\n            continue\n        break  # Non-header, non-blank, non-comment: stop removing\n    out.extend(lines[i:])\n    # Remove extra blank lines at the start\n    while len(out) > 1 and out[0].strip() == \"\" and out[1].strip() == \"\":\n        out = out[1:]\n    return out\n\n\ndef find_existing_context(lines):\n    start = None\n    end = None\n    for i, line in enumerate(lines):\n        if \"MODULE CONTEXT SUMMARY\" in line:\n            start = i\n        if start is not None and \"END MODULE CONTEXT SUMMARY\" in line:\n            end = i\n            break\n    return (start, end) if start is not None and end is not None else (None, None)\n\ndef render_pretty_list(lst, indent=4):\n    if not lst:\n        return \"[]\"\n    pad = \" \" * indent\n    return \"[\\n\" + \"\".join(f\"{pad}{repr(x)},\\n\" for x in lst) + \"]\"\n\ndef render_context_block(rel, context):\n    def pretty(key):\n        val = context[key]\n        if isinstance(val, list):\n            return f\"{key}: {render_pretty_list(val)}\"\n        return f'{key}: \"{val}\"' if isinstance(val, str) else f\"{key}: {val}\"\n\n    lines = [\n        '\"\"\"# MODULE CONTEXT SUMMARY',\n        f'filepath: {rel}',\n        pretty(\"purpose\"),\n        pretty(\"public_api\"),\n        pretty(\"depends_on\"),\n        pretty(\"used_by\"),\n        pretty(\"direct_imports\"),\n        pretty(\"related_schemas\"),\n        pretty(\"context_window_expected\"),\n        pretty(\"escalation_review\"),\n        '# END MODULE CONTEXT SUMMARY\"\"\"',\n        ''\n    ]\n    return \"\\n\".join(lines) + \"\\n\"\n\ndef load_all_contexts():\n    if os.path.exists(CONTEXT_JSON):\n        with open(CONTEXT_JSON, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    else:\n        return {}\n\ndef write_all_contexts(contexts):\n    with open(CONTEXT_JSON, \"w\", encoding=\"utf-8\") as f:\n        json.dump(contexts, f, indent=2, ensure_ascii=False)\n\ndef scan_python_modules():\n    for dirpath, dirnames, filenames in os.walk(ROOT):\n        dirnames[:] = [d for d in dirnames if d not in EXCLUDES]\n        for fname in filenames:\n            if fname.endswith(\".py\") and fname not in EXCLUDES:\n                abspath = os.path.join(dirpath, fname)\n                yield relpath(abspath), abspath\n\ndef scan_all_internal_modules(root_dir):\n    internal = set()\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        for fname in filenames:\n            if fname.endswith(\".py\"):\n                abs_path = os.path.join(dirpath, fname)\n                rel = os.path.relpath(abs_path, root_dir).replace(os.sep, \"/\")\n                mod_path = get_module_import_path(rel)\n                internal.add(mod_path)\n    return internal\n\ndef parse_module(path, rel_path, all_internal_modules):\n    \"\"\"Returns (public_api, depends_on, direct_imports) for a python module.\n       - public_api: list of fully qualified names for top-level defs/classes in this file\n       - depends_on: internal modules imported (as import paths)\n       - direct_imports: all directly imported packages/modules (raw names, incl. external)\n    \"\"\"\n    public_api = []\n    depends_on = set()\n    direct_imports = set()\n\n    module_import_path = get_module_import_path(rel_path)\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            node = ast.parse(f.read(), filename=path)\n    except Exception:\n        return public_api, depends_on, direct_imports\n\n    # Top-level functions/classes\n    for n in node.body:\n        if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            public_api.append(f\"{module_import_path}.{n.name}\")\n\n    # Imports\n    for n in ast.walk(node):\n        if isinstance(n, ast.Import):\n            for alias in n.names:\n                direct_imports.add(alias.name.split(\".\")[0])\n        elif isinstance(n, ast.ImportFrom):\n            mod = n.module\n            if mod:\n                mod_path = mod.replace(\".\", \"/\") + \".py\"\n                mod_import_path = mod.replace(\"/\", \".\")\n                direct_imports.add(mod.split(\".\")[0])\n                # Internal module dependency as import path (e.g. cadence.dev.executor)\n                if mod_import_path in all_internal_modules:\n                    depends_on.add(mod_import_path)\n    return sorted(public_api), sorted(depends_on), sorted(direct_imports)\n\ndef sync_contexts():\n    all_internal_modules = scan_all_internal_modules(ROOT)\n    all_contexts = load_all_contexts()\n    updated_contexts = {}\n    modified = 0\n    for rel, abspath in scan_python_modules():\n        context = dict(DEFAULT_CONTEXT)\n        context.update(all_contexts.get(rel, {}))\n        context['filepath'] = rel\n        public_api, depends_on, direct_imports = parse_module(abspath, rel, all_internal_modules)\n        context['public_api'] = public_api\n        context['depends_on'] = depends_on\n        context['direct_imports'] = direct_imports\n        with open(abspath, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        # Extract and remove all shebang/future imports anywhere in the file\n        shebang, futures, lines_no_shebang = extract_and_strip_shebang_and_futures(lines)\n        # Remove all context header blocks at the top\n        code_body = strip_duplicate_headers_at_top(lines_no_shebang)\n        block = render_context_block(rel, context)\n        new_lines = []\n        if shebang:\n            new_lines.append(shebang)\n        if futures:\n            new_lines.extend(futures)\n        new_lines.append(block)\n        new_lines.extend(code_body)\n        # Ensure only one blank line after header\n        i = 1\n        while i < len(new_lines) and new_lines[i].strip() == \"\":\n            i += 1\n        if i > 2:\n            new_lines = [new_lines[0], \"\\n\"] + new_lines[i:]\n        with open(abspath, \"w\", encoding=\"utf-8\") as f:\n            f.writelines(new_lines)\n        updated_contexts[rel] = context\n        modified += 1\n    write_all_contexts(updated_contexts)\n    print(f\"Updated {modified} file(s) and wrote {CONTEXT_JSON}.\")\n\n\n\ndef print_context(module):\n    contexts = load_all_contexts()\n    ctx = contexts.get(module)\n    if not ctx:\n        print(f\"No context found for {module}\")\n        return\n    for k, v in ctx.items():\n        print(f\"{k}: {v}\")\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) == 2 and sys.argv[1] == \"sync\":\n        sync_contexts()\n    elif len(sys.argv) == 3 and sys.argv[1] == \"show\":\n        print_context(sys.argv[2])\n    else:\n        print(\"Usage:\")\n        print(\"  python module_context.py sync         # Update headers and JSON for all modules\")\n        print(\"  python module_context.py show path/to/module.py   # Print context for a module\")\n",
  "tools/collect_code.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\n\"\"\"\ncollect_code.py  –  Export Cadence source files to a single JSON payload.\n\nUsage\n-----\npython tools/collect_code.py \\\n       --root cadence              # package folder(s) to scan (repeatable)\n       --out  code_payload.json   # written JSON (stdout if omitted)\n       --ext .py .md              # file extensions to keep\n       --max-bytes 50000          # skip giant files (>50 kB)\n\nResult\n------\nA JSON dict   { \"relative/path/to/file\": \"UTF-8 text …\", ... }\n\"\"\"\n\nfrom pathlib import Path\nimport argparse\nimport json\nimport sys\n\nDEFAULT_EXT = (\".py\", \".md\", \".cfg\", \".toml\", \".ini\")\n\n\ndef collect(\n    roots: list[Path],\n    files: list[Path] = [],\n    *,\n    extensions: tuple[str, ...] = DEFAULT_EXT,\n    max_bytes: int | None = None,\n) -> dict[str, str]:\n    \"\"\"\n    Walk *roots* and return {relative_path: code_text}.\n    Skips __pycache__, hidden folders, and files larger than *max_bytes*.\n    \"\"\"\n    out: dict[str, str] = {}\n    for root in roots:\n        for path in root.rglob(\"*\"):\n            if (\n                path.is_file()\n                and path.suffix in extensions\n                and \"__pycache__\" not in path.parts\n                and not any(p.startswith(\".\") for p in path.parts)\n            ):\n                if max_bytes and path.stat().st_size > max_bytes:\n                    continue\n                try:\n                    text = path.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[str(path.relative_to(Path.cwd()))] = text\n    for file in files:\n        if (\n            file.is_file()\n            and file.suffix in extensions\n            and file.stat().st_size <= (max_bytes or float(\"inf\"))\n        ):\n            rel = str(file.relative_to(Path.cwd()))\n            if rel not in out:\n                try:\n                    text = file.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = file.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[rel] = text\n    return out\n\n\ndef parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Collect source files into JSON.\")\n    p.add_argument(\n        \"--root\",\n        nargs=\"+\",\n        default=[\"cadence\"],\n        help=\"Directories to scan (repeatable).\",\n    )\n    p.add_argument(\n        \"--ext\",\n        nargs=\"+\",\n        default=DEFAULT_EXT,\n        help=\"File extensions to include (repeatable).\",\n    )\n    p.add_argument(\n        \"--max-bytes\",\n        type=int,\n        default=50000,\n        help=\"Skip files larger than this size (bytes).\",\n    )\n    p.add_argument(\n        \"--out\",\n        type=str,\n        default=\"-\",\n        help=\"Output JSON file path or '-' for stdout.\",\n    )\n    p.add_argument(\n        \"--file\",\n        nargs=\"+\",\n        default=[],\n        help=\"Individual files to include (repeatable).\",\n    )\n    return p.parse_args(argv)\n\n\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = parse_args(argv)\n    payload = collect(\n        [Path(r).resolve() for r in args.root],\n        files=[Path(f).resolve() for f in args.file],\n        extensions=tuple(args.ext),\n        max_bytes=args.max_bytes,\n    )\n    if args.out == \"-\":\n        json.dump(payload, sys.stdout, indent=2, ensure_ascii=False)\n    else:\n        out_path = Path(args.out)\n        out_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False))\n        print(f\"Wrote {len(payload)} files → {out_path}\")\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    main()\n",
  "tools/plan_blueprint_tasks.py": "#!/usr/bin/env python3\n\"\"\"\nConvert blueprint tasks in dev_backlog.json into micro-tasks with validated\nChangeSets.  Uses function-calling JSON mode and full project context.\n\"\"\"\n\nfrom __future__ import annotations\nimport argparse, uuid, datetime, json, textwrap\nfrom pathlib import Path\n\nfrom cadence.dev.backlog import BacklogManager\nfrom cadence.dev.change_set import ChangeSet\nfrom cadence.llm.json_call import LLMJsonCaller\n\n# ------------------------------------------------------------------ #\n# Load project context snapshots\n# ------------------------------------------------------------------ #\nCTX_DIR = Path(\"agent_context\")\nDOCS      = json.loads((CTX_DIR / \"docs.json\").read_text())             if (CTX_DIR / \"docs.json\").exists() else {}\nMODULES   = json.loads((CTX_DIR / \"module_contexts.json\").read_text())  if (CTX_DIR / \"module_contexts.json\").exists() else {}\nCODE_SNAP = json.loads((CTX_DIR / \"code.json\").read_text())             if (CTX_DIR / \"code.json\").exists() else {}\n\n_SYSTEM_CONTEXT = textwrap.dedent(\n    f\"\"\"\n    ----------  PROJECT CONTEXT (truncated) ----------\n    ## Docs\n    {json.dumps(DOCS, separators=(\",\", \":\"), ensure_ascii=False)[:100000]}\n\n    ## Module summaries\n    {json.dumps(MODULES, separators=(\",\", \":\"), ensure_ascii=False)[:100000]}\n\n    ## Source snapshot\n    {json.dumps(CODE_SNAP, separators=(\",\", \":\"), ensure_ascii=False)[:100000]}\n    --------------------------------------------------\n    \"\"\"\n).strip()\n\nSYSTEM_PROMPT = (\n    \"You are Cadence Planner.  Given a blueprint TITLE and DESCRIPTION, \"\n    \"generate a *single* JSON object that conforms to the Cadence \"\n    \"ChangeSet schema.  Use the project context for accuracy.  \"\n    \"Do NOT return markdown, only JSON.\"\n    \"\\n\\n\" + _SYSTEM_CONTEXT\n)\n\ncaller = LLMJsonCaller()  # singleton\n\n\ndef _plan(blueprint: dict) -> ChangeSet:\n    title = blueprint[\"title\"]\n    desc = blueprint.get(\"description\", \"\")\n    user_prompt = (\n        f\"BLUEPRINT TITLE:\\n{title}\\n\\n\"\n        f\"BLUEPRINT DESCRIPTION:\\n{desc}\\n\\n\"\n        \"Return the ChangeSet JSON now.\"\n    )\n    obj = caller.ask(SYSTEM_PROMPT, user_prompt)\n    return ChangeSet.from_dict(obj)\n\n\n# ------------------------------------------------------------------ #\ndef ingest_blueprints(backlog_path: Path) -> None:\n    bm = BacklogManager(backlog_path.as_posix())\n    blueprints = [t for t in bm.list_items(\"open\") if \"change_set\" not in t]\n\n    if not blueprints:\n        print(\"No blueprint tasks pending.\")\n        return\n\n    for bp in blueprints:\n        try:\n            cs = _plan(bp)\n        except Exception as exc:  # noqa: BLE001\n            print(f\"[FAIL] {bp['title']}: {exc}\")\n            continue\n\n        micro = {\n            \"id\": str(uuid.uuid4()),\n            \"title\": bp[\"title\"],\n            \"type\": \"micro\",\n            \"status\": \"open\",\n            \"created_at\": datetime.datetime.utcnow().isoformat(),\n            \"change_set\": cs.to_dict(),\n            \"parent_id\": bp[\"id\"],\n        }\n        bm.add_item(micro)\n        bm.update_item(bp[\"id\"], {\"status\": \"archived\"})\n        print(f\"[OK] {micro['id'][:8]} — ChangeSet generated\")\n\n    print(\"\\nBacklog snapshot:\")\n    print(bm)\n\n\ndef main() -> None:\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--backlog\", default=\"dev_backlog.json\")\n    args = ap.parse_args()\n    ingest_blueprints(Path(args.backlog))\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    main()",
  "tools/gen_prompt.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\n\"\"\"\ngen_prompt.py  –  Assemble a mega-prompt that contains\n\n  • Ground-truth docs (blueprint, progress logs, etc.)\n  • Full source snapshot (or whatever roots you point at)\n  • A header that gives the LLM an explicit NEXT TASK and ENV context\n\nUsage\n-----\npython tools/gen_prompt.py \\\n       --code-root cadence \\\n       --docs-dir docs \\\n       --task \"Implement FactorRegistry API and unit tests\" \\\n       --env  \"Python 3.11, pandas 2.2, scikit-learn 1.4\" \\\n       --out  prompt.txt\n\"\"\"\n\nfrom pathlib import Path\nimport argparse\nimport sys\nimport textwrap\n\n# --------------------------------------------------------------------------- #\n#  Config\n# --------------------------------------------------------------------------- #\nDEFAULT_CODE_EXT = (\".py\", \".md\", \".toml\", \".ini\", \".cfg\")\n\n\n# --------------------------------------------------------------------------- #\n#  Helpers\n# --------------------------------------------------------------------------- #\ndef _collect_files(\n    roots: list[Path],\n    *,\n    include_ext: tuple[str, ...],\n    max_bytes: int | None = None,\n) -> list[tuple[str, str]]:\n    \"\"\"Return [(relative_path, text), …] for all files matching *include_ext*.\"\"\"\n    records: list[tuple[str, str]] = []\n    cwd = Path.cwd()\n\n    for root in roots:\n        root = Path(root).resolve()\n        if not root.exists():\n            print(f\"WARNING: directory not found → {root}\", file=sys.stderr)\n            continue\n\n        for p in root.rglob(\"*\"):\n            if (\n                p.is_file()\n                and p.suffix in include_ext\n                and \"__pycache__\" not in p.parts\n                and not any(part.startswith(\".\") for part in p.parts)\n            ):\n                if max_bytes and p.stat().st_size > max_bytes:\n                    continue\n                try:\n                    txt = p.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n                records.append((str(p.relative_to(cwd)), txt))\n\n    records.sort()\n    return records\n\n\ndef _build_prompt(\n    docs: list[tuple[str, str]],\n    code: list[tuple[str, str]],\n    *,\n    header: str,\n) -> str:\n    parts: list[str] = [header]\n\n    # -- docs ---------------------------------------------------------------\n    parts.append(\"\\n## 1. Ground-Truth Documents\")\n    if not docs:\n        parts.append(\"\\n_No Markdown / text documents found in docs directory._\")\n    for path, txt in docs:\n        parts.append(f\"\\n### {path}\\n```markdown\\n{txt}\\n```\")\n\n    # -- code ---------------------------------------------------------------\n    parts.append(\"\\n## 2. Source Code Snapshot\")\n    if not code:\n        parts.append(\"\\n_No source files found in code roots._\")\n    for path, txt in code:\n        fence = \"```python\" if path.endswith(\".py\") else \"```text\"\n        parts.append(f\"\\n### {path}\\n{fence}\\n{txt}\\n```\")\n\n    return \"\\n\".join(parts)\n\n\n# --------------------------------------------------------------------------- #\n#  CLI\n# --------------------------------------------------------------------------- #\ndef _parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Generate mega-prompt for LLM.\")\n    p.add_argument(\"--code-root\", nargs=\"+\", default=[\"cadence\"],\n                   help=\"Package directories to scan (repeatable).\")\n    p.add_argument(\"--docs-dir\", default=\"docs\",\n                   help=\"Directory holding NORTH_STAR.md, progress logs, etc.\")\n    p.add_argument(\"--ext\", nargs=\"+\", default=DEFAULT_CODE_EXT,\n                   help=\"File extensions to include from code roots.\")\n    p.add_argument(\"--max-bytes\", type=int, default=100_000,\n                   help=\"Skip individual files larger than this size (bytes).\")\n    p.add_argument(\"--skip-code\", action=\"store_true\",\n               help=\"Omit source snapshot (tasks only).\")\n    p.add_argument(\"--task\", default=\"Tell me the next highest-leverage step and write the code.\",\n                   help=\"Explicit next task instruction injected into header.\")\n    p.add_argument(\"--env\", default=\"Python 3.11, pandas 2.2, scikit-learn 1.4\",\n                   help=\"Runtime environment string added to header.\")\n    p.add_argument(\"--out\", default=\"-\",\n                   help=\"Output file path or '-' for stdout.\")\n    return p.parse_args(argv)\n\n\n# --------------------------------------------------------------------------- #\n#  Main\n# --------------------------------------------------------------------------- #\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = _parse_args(argv)\n\n    docs = _collect_files(\n        [Path(args.docs_dir).resolve()],\n        include_ext=(\".md\", \".txt\"),\n        max_bytes=args.max_bytes,\n    )\n\n    if args.skip_code:\n        code = []\n    else:\n        code = _collect_files(\n            [Path(r).resolve() for r in args.code_root],\n            include_ext=tuple(args.ext),\n            max_bytes=args.max_bytes,\n        )\n\n    header = textwrap.dedent(\n        f\"\"\"\\\n        ### CADENCE STATUS / ALIGNMENT REQUEST  (auto-generated)\n\n        **Task**: {args.task}\n        **Environment**: {args.env}\n\n        You are an expert reviewer. Read ALL content below — docs first, then full\n        code — and report:\n\n          1. Alignment gaps between implementation and blueprint  \n          2. Missing risk / compliance safeguards  \n          3. Highest-leverage next actions  \n\n        Be brutally honest. No cheerleading. Return your analysis **only**.\n\n        ---\n        \"\"\"\n    )\n\n    prompt = _build_prompt(docs, code, header=header)\n\n    if args.out == \"-\":\n        sys.stdout.write(prompt)\n    else:\n        Path(args.out).write_text(prompt, encoding=\"utf-8\")\n        print(f\"Wrote prompt to {args.out}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
}