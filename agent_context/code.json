{
  "src/cadence/__init__.py": "\n",
  "src/cadence/context/provider.py": "# src/cadence/context/provider.py\nimport subprocess, sys, json\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nclass ContextProvider(ABC):\n    @abstractmethod\n    def get_context(self, *roots: Path, exts=(\".py\", \".md\"), max_bytes=50_000) -> str: ...\nclass SnapshotContextProvider(ContextProvider):\n    def get_context(self, *roots, exts=(\".py\", \".md\"), max_bytes=50_000, out=\"-\") -> str:\n        args = [sys.executable, \"tools/collect_code.py\"]\n        for r in roots: args += [\"--root\", str(r)]\n        for e in exts:  args += [\"--ext\", e]\n        args += [\"--max-bytes\", str(max_bytes), \"--out\", out]\n        return subprocess.run(args, capture_output=True, text=True, check=True).stdout\n",
  "src/cadence/context/__init__.py": "",
  "src/cadence/dev/record.py": "\n# src/cadence/dev/record.py\n\n\"\"\"\nCadence TaskRecord\n-----------------\nSingle Responsibility: Append/persist task processtates for full audit/repro. \nWrite/read only here. \nFile format: JSON list of dicts, one per unique task_id; each has history of states/iterations.\n\"\"\"\n\nimport os\nimport json\nimport threading\nimport copy\nfrom typing import List, Dict, Optional\nfrom datetime import datetime, UTC\n\nclass TaskRecordError(Exception):\n    \"\"\"Custom error for task record issues.\"\"\"\n    pass\n\nclass TaskRecord:\n    def __init__(self, record_file: str):\n        self.record_file = record_file\n        self._lock = threading.Lock()\n        # Always keep in-memory up to date with file\n        self._records: List[Dict] = []\n        self._idmap: Dict[str, Dict] = {}  # task_id -> record dict\n        self._load()\n\n    def save(self, task: dict, state: str, extra: dict = None) -> None:\n        \"\"\"\n        Records a snapshot for given task_id and state (e.g. \"patch_proposed\", \"patch_reviewed\", etc).\n        Extra provides arbitrary info (patch, review, commit_sha, test_results, etc).\n        If task does not exist (task_id is new), creates new record.\n        \"\"\"\n        with self._lock:\n            record = self._find_or_create_record(task)\n            snapshot = {\n                \"state\": state,\n                \"timestamp\": self._now(),\n                \"task\": copy.deepcopy(task),\n                \"extra\": copy.deepcopy(extra) if extra else {},\n            }\n            record[\"history\"].append(snapshot)\n            self._sync_idmap()\n            self._persist()\n\n    def load(self) -> List[Dict]:\n        \"\"\"\n        Returns a (deep) copy of all records (full history).\n        \"\"\"\n        with self._lock:\n            return copy.deepcopy(self._records)\n\n    def append_iteration(self, task_id: str, iteration: dict) -> None:\n        \"\"\"\n        Appends a new step/edit/review (dict) to a task's record—usually finer-grained than save().\n        \"\"\"\n        with self._lock:\n            record = self._find_record(task_id)\n            if record is None:\n                raise TaskRecordError(f\"No record for task id={task_id}\")\n            iter_snapshot = {\n                \"timestamp\": self._now(),\n                **copy.deepcopy(iteration)\n            }\n            record.setdefault(\"iterations\", []).append(iter_snapshot)\n            self._persist()\n\n    # ========== Internal Below ==========\n\n    def _find_or_create_record(self, task: dict) -> Dict:\n        \"\"\"\n        Finds or creates a new record for given task.\n        \"\"\"\n        tid = self._get_task_id(task)\n        rec = self._idmap.get(tid)\n        if rec is None:\n            rec = {\n                \"task_id\": tid,\n                \"created_at\": self._now(),\n                \"history\": [],\n                \"iterations\": []\n            }\n            self._records.append(rec)\n            self._idmap[tid] = rec\n        return rec\n\n    def _find_record(self, task_id: str) -> Optional[Dict]:\n        return self._idmap.get(task_id)\n\n    def _get_task_id(self, task: dict) -> str:\n        tid = task.get(\"id\")\n        if not tid:\n            raise TaskRecordError(\"Task dict missing 'id'. Cannot save record.\")\n        return tid\n\n    def _persist(self) -> None:\n        \"\"\"\n        Writes in-memory records to disk, atomic/overwrite (JSON).\n        \"\"\"\n        tmp = self.record_file + \".tmp\"\n        with open(tmp, \"w\", encoding=\"utf8\") as f:\n            json.dump(self._records, f, indent=2)\n        os.replace(tmp, self.record_file)\n\n    def _load(self) -> None:\n        \"\"\"Loads file into memory iff exists. Otherwise, empty record.\"\"\"\n        if not os.path.exists(self.record_file):\n            self._records = []\n            self._idmap = {}\n            return\n        with open(self.record_file, \"r\", encoding=\"utf8\") as f:\n            self._records = json.load(f)\n        self._sync_idmap()\n\n    def _sync_idmap(self):\n        \"\"\"Ensures self._idmap is up to date with self._records.\"\"\"\n        self._idmap = {rec[\"task_id\"]: rec for rec in self._records}\n\n    def _now(self):\n        return datetime.now(UTC).isoformat()\n\n# Example CLI/sanity use (not for prod)\nif __name__ == \"__main__\":\n    rec = TaskRecord(\"dev_record.json\")\n    tid = \"a1b2c3\"\n    # Save new record\n    task = {\"id\": tid, \"title\": \"Do something\", \"status\": \"open\"}\n    rec.save(task, state=\"patch_proposed\", extra={\"patch\": \"--- foo\"})\n    # Append an iteration (e.g., reviewer comment)\n    rec.append_iteration(tid, {\"reviewer\": \"alice\", \"opinion\": \"looks good\"})\n    # Print record for tid\n    print(json.dumps(rec.load(), indent=2))",
  "src/cadence/dev/shell.py": "# src/cadence/dev/shell.py\n\"\"\"\nCadence ShellRunner\n-------------------\nSingle Responsibility\n    • Provide *safe*, auditable wrappers around git / pytest / shell\n      commands.  \n    • **NEW** – Support reverse-applying a patch (rollback) via the same\n      entry-point by passing `reverse=True`.\n\nNever creates code or diffs; only executes them.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Optional, Dict\n\n\nclass ShellCommandError(Exception):\n    \"\"\"Raised when a shell/git/pytest command fails.\"\"\"\n    pass\n\n\nclass ShellRunner:\n    def __init__(self, repo_dir: str = \".\"):\n        self.repo_dir = os.path.abspath(repo_dir)\n        if not os.path.isdir(self.repo_dir):\n            raise ValueError(f\"repo_dir '{self.repo_dir}' does not exist or is not a directory.\")\n\n    # ------------------------------------------------------------------ #\n    # Git patch helpers\n    # ------------------------------------------------------------------ #\n    def git_apply(self, patch: str, *, reverse: bool = False) -> bool:\n        \"\"\"\n        Apply a unified diff to the working tree.\n\n        Args:\n            patch:   Unified diff string (UTF-8).\n            reverse: If True, apply the patch in *reverse* (equivalent to\n                     `git apply -R`) – used for automatic rollback.\n\n        Returns:\n            True on success.\n\n        Raises:\n            ShellCommandError on any git failure or invalid patch input.\n        \"\"\"\n        if not patch or not isinstance(patch, str):\n            raise ShellCommandError(\"No patch supplied to apply.\")\n\n        # Write patch to a temporary file so git can consume it.\n        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".patch\", delete=False) as tf:\n            tf.write(patch)\n            tf.flush()\n            tf_path = tf.name\n\n        try:\n            cmd = [\"git\", \"apply\"]\n            if reverse:\n                cmd.append(\"-R\")\n            cmd.append(tf_path)\n\n            result = subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n            if result.returncode != 0:\n                direction = \"reverse \" if reverse else \"\"\n                raise ShellCommandError(\n                    f\"git {direction}apply failed: {result.stderr.strip() or result.stdout.strip()}\"\n                )\n            return True\n        finally:\n            os.remove(tf_path)\n\n    # ------------------------------------------------------------------ #\n    # Testing helpers\n    # ------------------------------------------------------------------ #\n    def run_pytest(self, test_path: Optional[str] = None) -> Dict:\n        \"\"\"\n        Run pytest on the given path (default: ./tests).\n\n        Returns:\n            {'success': bool, 'output': str}\n        \"\"\"\n        path = test_path or os.path.join(self.repo_dir, \"tests\")\n        if not os.path.exists(path):\n            raise ShellCommandError(f\"Tests path '{path}' does not exist.\")\n\n        try:\n            result = subprocess.run(\n                [\"pytest\", \"-q\", path],\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n            passed = result.returncode == 0\n            output = (result.stdout or \"\") + \"\\n\" + (result.stderr or \"\")\n            return {\"success\": passed, \"output\": output.strip()}\n        except FileNotFoundError as e:\n            raise ShellCommandError(\"pytest is not installed or not in PATH.\") from e\n\n    # ------------------------------------------------------------------ #\n    # Commit helper\n    # ------------------------------------------------------------------ #\n    def git_commit(self, message: str) -> str:\n        \"\"\"\n        Commit **all** staged/changed files with the given commit message.\n\n        Returns:\n            The new commit SHA string.\n\n        Raises:\n            ShellCommandError on failure (e.g., nothing to commit).\n        \"\"\"\n        # Stage all changes (MVP behaviour)\n        result = subprocess.run(\n            [\"git\", \"add\", \"-A\"],\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=False,\n        )\n        if result.returncode != 0:\n            raise ShellCommandError(f\"git add failed: {result.stderr.strip()}\")\n\n        # Commit\n        result = subprocess.run(\n            [\"git\", \"commit\", \"-m\", message],\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=False,\n        )\n        if result.returncode != 0:\n            if \"nothing to commit\" in (result.stderr + result.stdout).lower():\n                raise ShellCommandError(\"git commit: nothing to commit.\")\n            raise ShellCommandError(f\"git commit failed: {result.stderr.strip()}\")\n\n        # Retrieve last commit SHA\n        result = subprocess.run(\n            [\"git\", \"rev-parse\", \"HEAD\"],\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=True,\n        )\n        return result.stdout.strip()\n\n\n# --------------------------------------------------------------------------- #\n# Dev-only sanity CLI\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    runner = ShellRunner(\".\")\n    # Example usage:\n    # runner.git_apply(patch_string)\n    # runner.git_apply(patch_string, reverse=True)  # rollback",
  "src/cadence/dev/executor.py": "# src/cadence/dev/executor.py\n\"\"\"\nCadence TaskExecutor\n--------------------\nNow guarantees every generated patch ends with a final newline, fixing the\n`git apply` “corrupt patch” error that occurred on some modified-file\ndiffs containing trailing context lines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport difflib\nfrom typing import Dict, List\n\nclass PatchBuildError(Exception):\n    pass\n\n\nclass TaskExecutor:\n    def __init__(self, src_root: str):\n        if not os.path.isdir(src_root):\n            raise ValueError(f\"src_root '{src_root}' is not a directory.\")\n        self.src_root = os.path.abspath(src_root)\n\n    # ------------------------------------------------------------------ #\n    def build_patch(self, task: Dict) -> str:\n        try:\n            diff_info = task.get(\"diff\")\n            if not diff_info:\n                raise PatchBuildError(\"Task missing 'diff' key.\")\n\n            file_rel = diff_info.get(\"file\", \"\")\n            before   = diff_info.get(\"before\")\n            after    = diff_info.get(\"after\")\n            if not file_rel or before is None or after is None:\n                raise PatchBuildError(\"Diff dict must contain 'file', 'before', 'after'.\")\n\n            # --- normalise line endings -----------------------------------\n            if before and not before.endswith(\"\\n\"):\n                before += \"\\n\"\n            if after and not after.endswith(\"\\n\"):\n                after += \"\\n\"\n\n            before_lines: List[str] = before.splitlines(keepends=True) if before else []\n            after_lines:  List[str] = after.splitlines(keepends=True)  if after  else []\n\n            new_file    = len(before_lines) == 0 and len(after_lines) > 0\n            delete_file = len(before_lines) > 0 and len(after_lines) == 0\n\n            fromfile = \"/dev/null\" if new_file else f\"a/{file_rel}\"\n            tofile   = \"/dev/null\" if delete_file else f\"b/{file_rel}\"\n\n            diff_lines = list(\n                difflib.unified_diff(\n                    before_lines,\n                    after_lines,\n                    fromfile=fromfile,\n                    tofile=tofile,\n                    # default lineterm=\"\\n\"\n                )\n            )\n\n            patch = \"\".join(diff_lines)\n            # Ensure the patch ends with *exactly* one \\n ─ git is picky.\n            if not patch.endswith(\"\\n\"):\n                patch += \"\\n\"\n\n            if not patch.strip():\n                raise PatchBuildError(\"Generated patch is empty.\")\n\n            return patch\n\n        except Exception as e:\n            raise PatchBuildError(f\"Failed to build patch: {e}\")\n\n    # unchanged helpers …\n    def refine_patch(self, task: Dict, feedback: str) -> str:\n        raise NotImplementedError\n\n    def validate_patch(self, patch: str) -> bool:\n        return bool(patch and patch.startswith((\"---\", \"diff \", \"@@\")))\n\n\n# --------------------------------------------------------------------------- #\n# Quick manual demo\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    executor = TaskExecutor(src_root=\".\")\n    print(\n        executor.build_patch(\n            {\n                \"diff\": {\n                    \"file\": \"demo.txt\",\n                    \"before\": \"\",\n                    \"after\": \"hello\\nworld\\n\",\n                }\n            }\n        )\n    )",
  "src/cadence/dev/command_center.py": "\n# src/cadence/dev/command_center.py\n\nimport streamlit as st\n\n# You may need to adjust the import path according to your setup\nfrom src.cadence.dev.orchestrator import DevOrchestrator\n\n# ---- Basic Config (map to your dev environment) ----\nCONFIG = dict(\n    backlog_path=\"dev_backlog.json\",\n    template_file=\"dev_templates.json\",\n    src_root=\"cadence\",\n    ruleset_file=None,\n    repo_dir=\".\",\n    record_file=\"dev_record.json\"\n)\norch = DevOrchestrator(CONFIG)\n\n# ---- Session State Initialization ----\nif \"selected_task_id\" not in st.session_state:\n    st.session_state[\"selected_task_id\"] = None\nif \"phase\" not in st.session_state:\n    st.session_state[\"phase\"] = \"Backlog\"\n\n# ---- Sidebar: Phase Navigation ----\nst.sidebar.title(\"Cadence Dev Center\")\nphase = st.sidebar.radio(\n    \"Workflow phase\",\n    [\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"],\n    index=[\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"].index(st.session_state[\"phase\"])\n)\nst.session_state[\"phase\"] = phase\n\n# ---- Main: Backlog View ----\nif phase == \"Backlog\":\n    st.title(\"Task Backlog\")\n    open_tasks = orch.backlog.list_items(status=\"open\")\n    if not open_tasks:\n        st.info(\"No open tasks! Add tasks via CLI/Notebook.\")\n    else:\n        import pandas as pd\n        df = pd.DataFrame(open_tasks)\n        st.dataframe(df[[\"id\", \"title\", \"type\", \"status\", \"created_at\"]])\n        selected = st.selectbox(\n            \"Select a task to work on\",\n            options=[t[\"id\"] for t in open_tasks],\n            format_func=lambda tid: f'{tid[:8]}: {next(t[\"title\"] for t in open_tasks if t[\"id\"] == tid)}'\n        )\n        if st.button(\"Continue to task detail\"):\n            st.session_state[\"selected_task_id\"] = selected\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Task Detail View ----\nelif phase == \"Task Detail\":\n    st.title(\"Task Details\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    st.markdown(f\"**Title:** {task['title']}\\n\\n**Type:** {task['type']}\\n\\n**Status:** {task['status']}\\n\\n**Created:** {task['created_at']}\")\n    st.code(task.get(\"description\", \"\"), language=\"markdown\")\n    st.json(task)\n    if st.button(\"Proceed to Patch Review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()\n\n# ---- Patch Review ----\nelif phase == \"Patch Review\":\n    st.title(\"Patch Review & Approval\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    try:\n        patch = orch.executor.build_patch(task)\n        st.code(patch, language=\"diff\")\n        review = orch.reviewer.review_patch(patch, context=task)\n        st.markdown(\"### Review Comments\")\n        st.markdown(review[\"comments\"] or \"_No issues detected._\")\n        if review[\"pass\"]:\n            if st.button(\"Approve and Apply Patch\"):\n                # Apply patch, save, and proceed\n                orch.shell.git_apply(patch)\n                orch._record(task, \"patch_applied\")\n                st.success(\"Patch applied.\")\n                st.session_state[\"phase\"] = \"Run Test\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Patch failed review; please revise before continuing.\")\n            if st.button(\"Back to task detail\"):\n                st.session_state[\"phase\"] = \"Task Detail\"\n                st.experimental_rerun()\n    except Exception as ex:\n        st.error(f\"Patch build/review failed: {ex}\")\n        if st.button(\"Back to task detail\"):\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Run Test ----\nelif phase == \"Run Test\":\n    st.title(\"Run Pytest\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    st.markdown(\"Apply code patch complete. Run tests to confirm correctness.\")\n    if st.button(\"Run tests now\"):\n        test_result = orch.shell.run_pytest()\n        st.text_area(\"Test Output\", test_result[\"output\"], height=200)\n        if test_result[\"success\"]:\n            st.success(\"Tests passed!\")\n            if st.button(\"Proceed to Archive/Done\"):\n                # Commit and archive task\n                task = orch.backlog.get_item(task_id)\n                sha = orch.shell.git_commit(f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\")\n                orch.backlog.update_item(task_id, {\"status\": \"done\"})\n                orch.backlog.archive_completed()\n                # commit snapshot (task is still 'done' here)\n                orch.record.save(task, state=\"committed\", extra={\"commit_sha\": sha})\n                # refresh snapshot so we accurately log 'archived'\n                updated_task = orch.backlog.get_item(task_id)\n                orch.record.save(updated_task, state=\"archived\", extra={})\n                st.session_state[\"phase\"] = \"Archive\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Tests failed, fix required before progressing.\")\n    if st.button(\"Back to patch review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n\n# ---- Archive / Task Complete ----\nelif phase == \"Archive\":\n    st.title(\"Task Archived\")\n    st.success(\"Task flow completed. You may return to the backlog.\")\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"selected_task_id\"] = None\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()",
  "src/cadence/dev/orchestrator.py": "# src/cadence/dev/orchestrator.py\n\"\"\"\nCadence DevOrchestrator\n-----------------------\nThe *single* source of truth for phase-ordering in the development loop.\n\nNEW FUNCTIONALITY (2025-06-21)\n    • Atomic rollback: if any failure occurs **after** a patch is applied\n      but **before** commit succeeds, we automatically revert the working\n      tree to its pristine state using `git apply -R`.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom .backlog import BacklogManager\nfrom .generator import TaskGenerator\nfrom .executor import TaskExecutor, PatchBuildError\nfrom .reviewer import TaskReviewer\nfrom .shell import ShellRunner, ShellCommandError\nfrom .record import TaskRecord, TaskRecordError\n\nimport sys\nfrom typing import Any, Dict\n\n\nclass DevOrchestrator:\n    def __init__(self, config: dict):\n        self.backlog = BacklogManager(config[\"backlog_path\"])\n        self.generator = TaskGenerator(config.get(\"template_file\"))\n        self.executor = TaskExecutor(config[\"src_root\"])\n        self.reviewer = TaskReviewer(config.get(\"ruleset_file\"))\n        self.shell = ShellRunner(config[\"repo_dir\"])\n        self.record = TaskRecord(config[\"record_file\"])\n\n    # ------------------------------------------------------------------ #\n    # Internal helper – ALWAYS log, never raise\n    # ------------------------------------------------------------------ #\n    def _record(self, task: dict, state: str, extra: Dict[str, Any] | None = None) -> None:\n        try:\n            self.record.save(task, state=state, extra=extra or {})\n        except TaskRecordError as e:\n            print(f\"[Record-Error] {e}\", file=sys.stderr)\n\n    # ------------------------------------------------------------------ #\n    # Pretty-printing helpers\n    # ------------------------------------------------------------------ #\n    def show(self, status: str = \"open\", printout: bool = True):\n        items = self.backlog.list_items(status)\n        if printout:\n            print(self._format_backlog(items))\n        return items\n\n    def _format_backlog(self, items):\n        if not items:\n            return \"(Backlog empty)\"\n        from tabulate import tabulate\n\n        rows = [\n            (\n                t[\"id\"][:8],\n                t.get(\"title\", \"\")[:48],\n                t.get(\"type\", \"\"),\n                t.get(\"status\", \"\"),\n                t.get(\"created_at\", \"\")[:19],\n            )\n            for t in items\n            if t.get(\"status\") != \"archived\"\n        ]\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n        return tabulate(rows, headers, tablefmt=\"github\")\n\n    # ------------------------------------------------------------------ #\n    # Main workflow\n    # ------------------------------------------------------------------ #\n    def run_task_cycle(self, select_id: str | None = None, *, interactive: bool = True):\n        \"\"\"\n        End-to-end flow for ONE micro-task.\n\n        NEW BEHAVIOUR\n            • Generates a reverse diff immediately after patch apply.\n            • Any failure **before** commit triggers automatic rollback.\n        \"\"\"\n        rollback_patch: str | None = None   # Will hold the *forward* patch\n        task: dict | None = None            # current task for logging\n\n        try:\n            # 1. Select Task --------------------------------------------------\n            open_tasks = self.backlog.list_items(status=\"open\")\n            if not open_tasks:\n                raise RuntimeError(\"No open tasks in backlog.\")\n\n            if select_id:\n                task = next((t for t in open_tasks if t[\"id\"] == select_id), None)\n                if not task:\n                    raise RuntimeError(f\"Task id '{select_id}' not found in open backlog.\")\n            elif interactive:\n                print(self._format_backlog(open_tasks))\n                print(\"---\")\n                idx = self._prompt_pick(len(open_tasks))\n                task = open_tasks[idx]\n            else:\n                task = open_tasks[0]  # default: pick first open\n\n            print(f\"\\n[Selected task: {task['id'][:8]}] {task.get('title')}\\n\")\n\n            # 2. Build patch --------------------------------------------------\n            self._record(task, \"build_patch\")\n            try:\n                patch = self.executor.build_patch(task)\n                rollback_patch = patch  # Save for potential rollback\n                self._record(task, \"patch_built\", {\"patch\": patch})\n                print(\"--- Patch built ---\\n\", patch)\n            except PatchBuildError as ex:\n                self._record(task, \"failed_build_patch\", {\"error\": str(ex)})\n                print(f\"[X] Patch build failed: {ex}\")\n                return {\"success\": False, \"stage\": \"build_patch\", \"error\": str(ex)}\n\n            # 3. Review -------------------------------------------------------\n            review1 = self.reviewer.review_patch(patch, context=task)\n            self._record(task, \"patch_reviewed\", {\"review\": review1})\n            print(\"--- Review 1 ---\")\n            print(review1[\"comments\"] or \"(no comments)\")\n            if not review1[\"pass\"]:\n                self._record(task, \"failed_patch_review\", {\"review\": review1})\n                print(\"[X] Patch failed review, aborting.\")\n                return {\"success\": False, \"stage\": \"patch_review\", \"review\": review1}\n\n            # 4. Apply patch --------------------------------------------------\n            try:\n                self.shell.git_apply(patch)\n                self._record(task, \"patch_applied\")\n                print(\"[✔] Patch applied.\")\n            except ShellCommandError as ex:\n                self._record(task, \"failed_patch_apply\", {\"error\": str(ex)})\n                print(f\"[X] git apply failed: {ex}\")\n                return {\"success\": False, \"stage\": \"patch_apply\", \"error\": str(ex)}\n\n            # ------------------------------- #\n            # --- CRITICAL SECTION BEGIN --- #\n            # Any failure after this point MUST rollback before returning.\n            # ------------------------------- #\n\n            # 5. Run tests ----------------------------------------------------\n            test_result = self.shell.run_pytest()\n            self._record(task, \"pytest_run\", {\"pytest\": test_result})\n            print(\"--- Pytest ---\")\n            print(test_result[\"output\"])\n\n            if not test_result[\"success\"]:\n                print(\"[X] Tests FAILED. Initiating rollback.\")\n                self._record(task, \"failed_test\", {\"pytest\": test_result})\n                self._attempt_rollback(task, rollback_patch, src_stage=\"test\")\n                return {\"success\": False, \"stage\": \"test\", \"test_result\": test_result}\n\n            # 6. (Optional) extra review could go here ------------------------\n\n            # 7. Commit -------------------------------------------------------\n            commit_msg = f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\"\n            try:\n                sha = self.shell.git_commit(commit_msg)\n                self._record(task, \"committed\", {\"commit_sha\": sha})\n                print(f\"[✔] Committed as {sha}\")\n            except ShellCommandError as ex:\n                self._record(task, \"failed_commit\", {\"error\": str(ex)})\n                print(f\"[X] git commit failed: {ex}\")\n                # Commit failure ⇒ rollback\n                self._attempt_rollback(task, rollback_patch, src_stage=\"commit\")\n                return {\"success\": False, \"stage\": \"commit\", \"error\": str(ex)}\n\n            # 8. Mark task done + archive ------------------------------------\n            self.backlog.update_item(task[\"id\"], {\"status\": \"done\"})\n            task = self.backlog.get_item(task[\"id\"])\n            self._record(task, \"status_done\")\n\n            self.backlog.archive_completed()\n            task = self.backlog.get_item(task[\"id\"])\n            self._record(task, \"archived\")\n            print(\"[✔] Task marked done and archived.\")\n\n            return {\"success\": True, \"commit\": sha, \"task_id\": task[\"id\"]}\n\n        except Exception as ex:\n            # Catch-all safety net: attempt rollback if patch was applied\n            if task and rollback_patch:\n                self._attempt_rollback(task, rollback_patch, src_stage=\"unexpected\", quiet=True)\n            print(f\"[X] Cycle failed: {ex}\")\n            return {\"success\": False, \"error\": str(ex)}\n\n    # ------------------------------------------------------------------ #\n    # Rollback helper\n    # ------------------------------------------------------------------ #\n    def _attempt_rollback(self, task: dict, patch: str | None, *, src_stage: str, quiet: bool = False):\n        \"\"\"\n        Try to undo an applied patch.  Records outcome to TaskRecord.\n\n        Args:\n            task:       The current task dict (for logging).\n            patch:      The *forward* patch previously applied.\n            src_stage:  Where rollback was triggered (e.g., \"test\", \"commit\").\n            quiet:      If True, suppress stdout noise (used in unexpected fail).\n        \"\"\"\n        if not patch:\n            # Defensive: nothing to rollback\n            self._record(task, \"rollback_skip_no_patch\")\n            return\n\n        try:\n            self.shell.git_apply(patch, reverse=True)\n            self._record(task, f\"failed_{src_stage}_and_rollback\")\n            if not quiet:\n                print(\"[↩] Rollback successful – working tree restored.\")\n        except ShellCommandError as rb_ex:\n            # CRITICAL: rollback failed – manual intervention required\n            self._record(\n                task,\n                \"critical_rollback_failure\",\n                {\"trigger\": src_stage, \"rollback_error\": str(rb_ex)},\n            )\n            print(f\"[!!] Rollback FAILED – manual fix required: {rb_ex}\")\n\n    # ------------------------------------------------------------------ #\n    # CLI dispatch helpers\n    # ------------------------------------------------------------------ #\n    def cli_entry(self, command: str, **kwargs):\n        try:\n            if command in (\"backlog\", \"show\"):\n                return self.show(status=kwargs.get(\"status\", \"open\"))\n            if command == \"start\":\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            if command == \"evaluate\":\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            if command == \"done\":\n                if \"id\" not in kwargs:\n                    print(\"You must supply a task id for 'done'.\")\n                    return\n                self.backlog.update_item(kwargs[\"id\"], {\"status\": \"done\"})\n                self.backlog.archive_completed()\n                print(f\"Task {kwargs['id']} marked as done and archived.\")\n                return\n            print(f\"Unknown command: {command}\")\n        except Exception as ex:\n            print(f\"[X] CLI command '{command}' failed: {ex}\")\n\n    # ------------------------------------------------------------------ #\n    # Notebook / interactive helper\n    # ------------------------------------------------------------------ #\n    def _prompt_pick(self, n):\n        while True:\n            ans = input(f\"Select task [0-{n-1}]: \")\n            try:\n                ix = int(ans)\n                if 0 <= ix < n:\n                    return ix\n            except Exception:\n                pass\n            print(\"Invalid. Try again.\")\n\n\n# --------------------------------------------------------------------------- #\n# Stand-alone execution (developer convenience)\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    CONFIG = dict(\n        backlog_path=\"dev_backlog.json\",\n        template_file=\"dev_templates.json\",\n        src_root=\"cadence\",\n        ruleset_file=None,\n        repo_dir=\".\",\n        record_file=\"dev_record.json\",\n    )\n    orch = DevOrchestrator(CONFIG)\n\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command\", nargs=\"?\", help=\"show|start|evaluate|done\")\n    parser.add_argument(\"--id\", default=None, help=\"Task id to use\")\n    args = parser.parse_args()\n\n    orch.cli_entry(args.command or \"show\", id=args.id)",
  "src/cadence/dev/reviewer.py": "\n# src/cadence/dev/reviewer.py\n\n\"\"\"\nCadence TaskReviewer\n-------------------\nSingle Responsibility: Adjudicates patch/diff quality via rules/LLM/manual. Never applies code or diffs.\nFuture extensible: can host local ruleset, shell out to LLM agent, or use human-in-the-loop.\n\"\"\"\n\nimport os\nimport json\nfrom typing import Optional, Dict\n\nclass PatchReviewError(Exception):\n    \"\"\"Raised if review input is malformed or review fails outright (e.g. ruleset not found/valid).\"\"\"\n    pass\n\nclass TaskReviewer:\n    def __init__(self, ruleset_file: str = None):\n        \"\"\"\n        Optionally specify path to ruleset file (JSON list of rules),\n        or leave blank to use default built-in rules.\n        \"\"\"\n        self.ruleset_file = ruleset_file\n        self.rules = self._load_ruleset(ruleset_file) if ruleset_file else self._default_ruleset()\n\n    def review_patch(self, patch: str, context: Optional[dict] = None) -> Dict:\n        \"\"\"\n        Review a diff/patch string (unapplied) and optional context (task, commit message, etc).\n        Returns dict {'pass': bool, 'comments': str}\n        This uses static (offline) heuristics but can be swapped for agent/LLM in future.\n        \"\"\"\n        # Guard: Patch required\n        if not patch or not isinstance(patch, str):\n            return {'pass': False, 'comments': 'Patch missing or not a string.'}\n\n        # Apply rules in order. If any hard-fail, review fails.\n        comments = []\n        passed = True\n\n        for rule in self.rules:\n            ok, msg = rule(patch, context)\n            if not ok:\n                passed = False\n            if msg:\n                comments.append(msg)\n            if not ok:\n                # For now, fail-hard (but comment all)\n                break\n\n        return {'pass': passed, 'comments': \"\\n\".join(comments).strip()}\n\n    def _default_ruleset(self):\n        \"\"\"\n        Returns a list of static rule functions: (patch, context) → (bool, str)\n        \"\"\"\n        def not_empty_rule(patch, _):\n            if not patch.strip():\n                return False, \"Patch is empty.\"\n            return True, \"\"\n        def startswith_rule(patch, _):\n            if not patch.startswith((\"---\", \"diff \", \"@@ \")):\n                return False, \"Patch does not appear to be a valid unified diff.\"\n            return True, \"\"\n        def contains_todo_rule(patch, _):\n            if \"TODO\" in patch:\n                return False, \"Patch contains 'TODO'—code review must not introduce placeholders.\"\n            return True, \"\"\n\n        # Optionally check for too-huge diffs, or forbidden patterns, via rules below.\n        def size_limit_rule(patch, _):\n            line_count = patch.count(\"\\n\")\n            if line_count > 5000:  # Arbitrary large patch guard\n                return False, f\"Patch too large for standard review ({line_count} lines).\"\n            return True, \"\"\n        return [\n            not_empty_rule, \n            startswith_rule,\n            contains_todo_rule,\n            size_limit_rule,\n        ]\n\n    def _load_ruleset(self, path: str):\n        \"\"\"\n        Loads a simple external ruleset (for human/agent extension), e.g. as list of forbidden strings.\n        For extensibility only; advanced policies/LLMs should be subclassed onto this interface.\n        \"\"\"\n        if not os.path.exists(path):\n            raise PatchReviewError(f\"Ruleset file '{path}' not found.\")\n        with open(path, \"r\", encoding=\"utf8\") as f:\n            obj = json.load(f)\n        # Expect a list of {'type':..., 'pattern':..., ...} dicts for pattern rules\n        rules = []\n        def make_rule(ruleobj):\n            typ = ruleobj.get('type')\n            pattern = ruleobj.get('pattern')\n            msg = ruleobj.get('message', f\"Patch contains forbidden pattern: {pattern}\")\n            if typ == 'forbid':\n                def _inner(patch, _):\n                    if pattern in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            elif typ == 'require':\n                def _inner(patch, _):\n                    if pattern not in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            else:\n                # Ignore unknown rule types\n                def _inner(patch, _):\n                    return True, \"\"\n                return _inner\n        for ruleobj in obj:\n            rules.append(make_rule(ruleobj))\n        # Default rules always included\n        return self._default_ruleset() + rules\n\n# Standalone/example/test run\nif __name__ == \"__main__\":\n    reviewer = TaskReviewer()\n    # Good patch\n    patch = \"\"\"--- sample.py\n+++ sample.py\n@@ -1 +1,2 @@\n-print('hello')\n+print('hello world')\n\"\"\"\n    result = reviewer.review_patch(patch)\n    print(\"Result (should pass):\", result)\n\n    bad_patch = \"TODO: refactor\\n\"\n    result = reviewer.review_patch(bad_patch)\n    print(\"Result (should fail):\", result)",
  "src/cadence/dev/__init__.py": "\n",
  "src/cadence/dev/generator.py": "\n# src/cadence/dev/generator.py\n\n\"\"\"\nCadence TaskGenerator\n-------------------\nSingle Responsibility: Propose/generate well-formed tasks, optionally from template or rules/LLM seed.\nNever applies code or diffs. Future extensible to LLM/human agent.\n\"\"\"\n\nimport os, json, uuid, datetime, warnings\nfrom typing import List, Dict, Optional\n\nclass TaskTemplateError(Exception):\n    \"\"\"Raised if template file is not valid or incomplete.\"\"\"\n    pass\n\nREQUIRED_FIELDS = (\"title\", \"type\", \"status\", \"created_at\")\n\n\nclass TaskGenerator:\n    def __init__(self, template_file: str | None = None, *, strict: bool = False):\n        \"\"\"\n        Optionally supply a JSON / MD template file.  \n        If `strict` is False (default) and the file does **not** exist, we\n        continue with an empty template dictionary and merely warn.\n        \"\"\"\n        self.template_file = template_file\n        self._template_cache: Dict = {}\n        if template_file:\n            if os.path.exists(template_file):\n                self._template_cache = self._load_template(template_file)\n            elif strict:\n                # Original behaviour – hard-fail\n                raise TaskTemplateError(f\"Template file not found: {template_file}\")\n            else:\n                warnings.warn(\n                    f\"Template file '{template_file}' not found; \"\n                    \"proceeding with minimal fallback templates.\",\n                    RuntimeWarning,\n                )\n    \n    def generate_tasks(self, mode: str = \"micro\", count: int = 1, human_prompt: Optional[str]=None) -> List[Dict]:\n        \"\"\"\n        Return a list of well-formed tasks. \n        - mode: \"micro\", \"story\", \"epic\", etc.\n        - count: number of tasks to generate\n        - human_prompt: if provided, use as summary/title for each (e.g., \"Add new test\", for human CLI prompt workflow)\n        If template_file is used, will fill in mode-related templates.\n        \"\"\"\n        tasks = []\n        base_tpl = self._get_template_for_mode(mode)\n        now = datetime.datetime.utcnow().isoformat()\n        for i in range(count):\n            task = dict(base_tpl)\n            # Minimal fields: id, title, type, status, created_at\n            task[\"id\"] = str(uuid.uuid4())\n            task[\"type\"] = mode\n            task.setdefault(\"status\", \"open\")\n            task.setdefault(\"created_at\", now)\n            if human_prompt:\n                # Provide a default/barebones title/desc from human input\n                task[\"title\"] = human_prompt if count == 1 else f\"{human_prompt} [{i+1}]\"\n                task.setdefault(\"description\", human_prompt)\n            else:\n                # Fallback: title must be present; if not, use template/title from mode or 'Untitled'\n                task[\"title\"] = task.get(\"title\", f\"{mode.capitalize()} Task {i+1}\")\n                task.setdefault(\"description\", \"\")\n            self._validate_task(task)\n            tasks.append(task)\n        return tasks\n\n    def overwrite_tasks(self, new_tasks: List[Dict], output_path: Optional[str]=None) -> None:\n        \"\"\"\n        Replace all backlog tasks with given well-formed list (writes to output_path, else self.template_file).\n        \"\"\"\n        path = output_path or self.template_file\n        if not path:\n            raise TaskTemplateError(\"No output path specified to write tasks.\")\n        with open(path, \"w\", encoding=\"utf8\") as f:\n            json.dump([self._validate_task(t) for t in new_tasks], f, indent=2)\n\n    def _get_template_for_mode(self, mode: str) -> Dict:\n        \"\"\"\n        Get template for the given mode; falls back to default/minimal template.\n        \"\"\"\n        if self._template_cache and mode in self._template_cache:\n            return dict(self._template_cache[mode])  # deep copy\n        # Fallback: minimal template\n        return {\n            \"title\": \"\",\n            \"type\": mode,\n            \"status\": \"open\",\n            \"created_at\": \"\",\n            \"description\": \"\",\n        }\n\n    def _load_template(self, path: str) -> Dict:\n        \"\"\"\n        Loads a JSON template file mapping mode→template-dict.\n        If Markdown file with front-matter, parse the JSON front-matter.\n        \"\"\"\n        if not os.path.exists(path):\n            raise TaskTemplateError(f\"Template file not found: {path}\")\n        if path.endswith(\".md\"):\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                lines = f.readlines()\n            start, end = None, None\n            for i, line in enumerate(lines):\n                if line.strip() == \"```json\":\n                    start = i + 1\n                elif line.strip().startswith(\"```\") and start is not None and end is None:\n                    end = i\n                    break\n            if start is not None and end is not None:\n                json_str = \"\".join(lines[start:end])\n                tpl = json.loads(json_str)\n            else:\n                raise TaskTemplateError(\"Markdown template missing ```json ... ``` block.\")\n        else:\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                tpl = json.load(f)\n        if not isinstance(tpl, dict):\n            raise TaskTemplateError(\"Task template must be a dict mapping mode->template.\")\n        return tpl\n\n    def _validate_task(self, task: Dict) -> Dict:\n        \"\"\"\n        Ensures task has all required fields and correct types/formats.\n        Throws TaskTemplateError if not.\n        \"\"\"\n        for field in REQUIRED_FIELDS:\n            if field not in task or (field == \"title\" and not task[\"title\"].strip()):\n                raise TaskTemplateError(f\"Task missing required field: '{field}'\")\n        if not isinstance(task[\"type\"], str):\n            raise TaskTemplateError(\"Task type must be str.\")\n        if \"id\" in task and not isinstance(task[\"id\"], str):\n            task[\"id\"] = str(task[\"id\"])\n        # Optionally: check status value, etc.\n        return task\n\n    # For future agentic/LLM/human input: accept strings, call LLM API, etc.\n    # Extend here with agent hooks.\n\n# Standalone/test CLI example (not for production)\nif __name__ == \"__main__\":\n    # Example: generate 2 microtasks from default, print as JSON:\n    g = TaskGenerator()\n    tasks = g.generate_tasks(mode=\"micro\", count=2, human_prompt=\"Example user-initiated task\")\n    print(json.dumps(tasks, indent=2))",
  "src/cadence/dev/backlog.py": "\n# src/cadence/dev/backlog.py\n\n\"\"\"\nCadence BacklogManager\n---------------------\nSingle Responsibility: CRUD on task backlog (no code/diffs). All file IO explicit, JSON-logged, and future agent-ready.\n\"\"\"\n\nimport os\nimport json\nimport uuid\nfrom typing import List, Dict, Optional\n\nclass BacklogEmptyError(Exception):\n    \"\"\"Raised if attempting to pop or select from an empty backlog.\"\"\"\n    pass\n\nclass TaskStructureError(Exception):\n    \"\"\"Raised if a task dict doesn't conform to required structure.\"\"\"\n    pass\n\nclass TaskNotFoundError(Exception):\n    \"\"\"Raised if a requested task_id is not in the backlog.\"\"\"\n    pass\n\nREQUIRED_FIELDS = (\"id\", \"title\", \"type\", \"status\", \"created_at\")\n\nclass BacklogManager:\n    \"\"\"\n    Manages Cadence backlog: microtasks, stories, and epics.\n    - All tasks are plain dicts with mandatory fields.\n    - Underlying store is a JSON file [{...}, ...].\n    \"\"\"\n\n    def __init__(self, backlog_path: str):\n        self.path = backlog_path\n        self._items: List[Dict] = []\n        self.load()\n\n    def list_items(self, status: str = \"open\") -> List[Dict]:\n        \"\"\"\n        Return a list of tasks filtered by status.\n        status: \"open\", \"in_progress\", \"done\", or \"all\"\n        \"\"\"\n        if status == \"all\":\n            return list(self._items)\n        return [item for item in self._items if item.get(\"status\", \"open\") == status]\n\n    def add_item(self, task: Dict) -> None:\n        \"\"\"\n        Add a new task to backlog. Enforce structure and unique id.\n        \"\"\"\n        task = self._normalize_task(task)\n        if any(t[\"id\"] == task[\"id\"] for t in self._items):\n            raise TaskStructureError(f\"Duplicate task id: {task['id']}\")\n        self._items.append(task)\n        self.save()\n\n    def remove_item(self, task_id: str) -> None:\n        \"\"\"\n        Mark a task as archived (status = 'archived').\n        \"\"\"\n        idx = self._task_index(task_id)\n        self._items[idx][\"status\"] = \"archived\"\n        self.save()\n\n    def archive_completed(self) -> None:\n        \"\"\"\n        Mark all tasks with status 'done' as 'archived'.\n        \"\"\"\n        n = 0\n        for item in self._items:\n            if item.get(\"status\") == \"done\":\n                item[\"status\"] = \"archived\"\n                n += 1\n        if n:\n            self.save()\n\n    def save(self) -> None:\n        \"\"\"Persist backlog state to self.path as JSON (UTF-8, indent).\"\"\"\n        tmp_path = self.path + \".tmp\"\n        with open(tmp_path, \"w\", encoding=\"utf8\") as f:\n            json.dump(self._items, f, indent=2)\n        os.replace(tmp_path, self.path)\n\n    def load(self) -> None:\n        \"\"\"\n        Reload backlog state from file. If the file does not exist, starts empty.\n        \"\"\"\n        if not os.path.exists(self.path):\n            self._items = []\n            return\n        with open(self.path, \"r\", encoding=\"utf8\") as f:\n            data = json.load(f)\n            if not isinstance(data, list):\n                raise ValueError(\"Backlog JSON must be a list of tasks\")\n            self._items = [self._normalize_task(t) for t in data]\n\n    def _normalize_task(self, task: Dict) -> Dict:\n        \"\"\"\n        Ensure the dict has all required fields, fill missing, return new dict.\n        \"\"\"\n        t = dict(task)  # copy\n        for field in REQUIRED_FIELDS:\n            if field not in t:\n                if field == \"id\":\n                    t[\"id\"] = str(uuid.uuid4())\n                elif field == \"created_at\":\n                    import datetime\n                    t[\"created_at\"] = datetime.datetime.utcnow().isoformat()\n                elif field == \"status\":\n                    t[\"status\"] = \"open\"\n                elif field == \"type\":\n                    t[\"type\"] = \"micro\"\n                else:\n                    raise TaskStructureError(f\"Missing required field: {field}\")\n        # Sanity check: no harmful keys\n        if not isinstance(t[\"id\"], str):\n            t[\"id\"] = str(t[\"id\"])\n        return t\n\n    def _task_index(self, task_id: str) -> int:\n        \"\"\"\n        Internal: find list index of task by id or raise.\n        \"\"\"\n        for ix, t in enumerate(self._items):\n            if t[\"id\"] == task_id:\n                return ix\n        raise TaskNotFoundError(f\"No task found with id={task_id}\")\n\n    def get_item(self, task_id: str) -> Dict:\n        \"\"\"Retrieve a task by id.\"\"\"\n        idx = self._task_index(task_id)\n        return self._items[idx]\n\n    def update_item(self, task_id: str, updates: Dict) -> None:\n        \"\"\"\n        Update arbitrary fields of a task (e.g. assign, progress, edit).\n        \"\"\"\n        idx = self._task_index(task_id)\n        self._items[idx].update(updates)\n        self.save()\n\n    def export(self) -> List[Dict]:\n        \"\"\"\n        Return a (deep) copy of all backlog items.\n        \"\"\"\n        import copy\n        return copy.deepcopy(self._items)\n\n    # Optional: friendly CLI/str output\n    def __str__(self) -> str:\n        from tabulate import tabulate\n        if not self._items:\n            return \"(Backlog empty)\"\n        rows = [\n            (t[\"id\"][:8], t[\"title\"], t[\"type\"], t.get(\"status\", \"open\"), t.get(\"created_at\", \"\"))\n            for t in self._items if t.get(\"status\") != \"archived\"\n        ]\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n        return tabulate(rows, headers, tablefmt=\"github\")\n\n# For direct module test/dev, NOT in prod code.\nif __name__ == \"__main__\":\n    # Example usage\n    mgr = BacklogManager(\"dev_backlog.json\")\n    print(mgr)\n    # To add: mgr.add_item({\"title\": \"Test microtask\", \"type\": \"micro\"})",
  "src/cadence/agents/sidekick.py": "# src/cadence/agents/sidekick.py\n\"\"\"\nPersona agent that *delegates* to a ReasoningAgent but presents a\nhuman-centric mentor/advisor interface.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nfrom .profile import AgentProfile, REASONING_PROFILE\nfrom .reasoning import ReasoningAgent\n\n\n_SIDEKICK_PROMPT = \"\"\"\nYou are an AI-enhanced co-developer and mentor. Your primary goal is to\nextract the most creative, high-leverage ideas from the human user and\ntransform them into actionable improvements for the Cadence platform.\nAvoid tactical implementation details unless asked; focus on vision,\narchitecture, and pragmatic next steps.\n\"\"\"\n\n\nclass Sidekick:\n    \"\"\"\n    Thin wrapper: exposes `run_interaction` but delegates work to an\n    internal ReasoningAgent instance configured with a custom prompt.\n    \"\"\"\n\n    def __init__(self):\n        profile = AgentProfile(\n            name=\"sidekick\",\n            role=\"advisor\",\n            model=REASONING_PROFILE.model,\n            context_limit=REASONING_PROFILE.context_limit,\n            review_policy=REASONING_PROFILE.review_policy,\n            default_system_prompt=REASONING_PROFILE.default_system_prompt,\n            extra=REASONING_PROFILE.extra.copy() if REASONING_PROFILE.extra else {},\n        )\n        self._agent = ReasoningAgent(profile=profile, system_prompt=_SIDEKICK_PROMPT)\n        self._inject_seed_context()\n\n    # ------------------------------------------------------------------ #\n    # Public façade\n    # ------------------------------------------------------------------ #\n    def run_interaction(self, user_input: str, **kwargs) -> str:\n        return self._agent.run_interaction(user_input, **kwargs)\n\n    async def async_run_interaction(self, user_input: str, **kwargs) -> str:\n        return await self._agent.async_run_interaction(user_input, **kwargs)\n\n    # ------------------------------------------------------------------ #\n    # Private helpers\n    # ------------------------------------------------------------------ #\n    def _inject_seed_context(self):\n        docs = self._agent.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n\n        modules_path = Path(\"agent_context/module_contexts.json\")\n        modules = {}\n        if modules_path.exists():\n            modules = json.loads(modules_path.read_text())\n\n        self._agent.append_message(\n            \"user\",\n            f\"DOCS:\\n{docs}\\n---\\nMODULE_CONTEXTS:\\n{json.dumps(modules)[:10_000]}\",\n        )",
  "src/cadence/agents/base.py": "# src/cadence/agents/base.py\nfrom __future__ import annotations\n\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\n\nfrom src.cadence.llm.client import LLMClient, get_default_client\nfrom src.cadence.context.provider import ContextProvider, SnapshotContextProvider\nfrom .profile import AgentProfile\n\n\nclass BaseAgent:\n    \"\"\"\n    The one true superclass for *all* Cadence agents.\n\n    An agent = (profile) + (conversation state) + (LLM client) [+ optional helpers]\n\n    Subclasses SHOULD NOT hard-code models; they inherit that from the supplied\n    `AgentProfile`.  Core agents (Reasoning / Execution / Efficiency) simply\n    pass the canonical profile; personas may inject a custom one.\n    \"\"\"\n\n    def __init__(\n        self,\n        profile: AgentProfile,\n        *,\n        llm_client: Optional[LLMClient] = None,\n        system_prompt: Optional[str] = None,\n        context_provider: Optional[ContextProvider] = None,\n    ):\n        self.profile = profile\n        self.llm_client = llm_client or get_default_client()\n        self.system_prompt = system_prompt or profile.default_system_prompt\n        self.context_provider = context_provider or SnapshotContextProvider()\n        self.messages: List[Dict[str, Any]] = []\n        self.reset_context()\n\n    # --------------------------------------------------------------------- #\n    # Conversation helpers\n    # --------------------------------------------------------------------- #\n    def reset_context(self, system_prompt: Optional[str] = None):\n        \"\"\"Clear history and (re)set the system prompt.\"\"\"\n        self.messages = []\n        sys_prompt = system_prompt or self.system_prompt\n        if sys_prompt:\n            self.append_message(\"system\", sys_prompt)\n\n    def append_message(self, role: str, content: str):\n        self.messages.append({\"role\": role, \"content\": content})\n\n    # --------------------------------------------------------------------- #\n    # LLM calls\n    # --------------------------------------------------------------------- #\n    def run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        response = self.llm_client.call(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,  # already injected\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        return response\n\n    async def async_run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        response = await self.llm_client.acall(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        return response\n\n    # --------------------------------------------------------------------- #\n    # Persistence\n    # --------------------------------------------------------------------- #\n    def save_history(self, path: str):\n        import json\n        Path(path).write_text(json.dumps(self.messages, indent=2, ensure_ascii=False))\n\n    def load_history(self, path: str):\n        import json\n        self.messages = json.loads(Path(path).read_text())\n\n    # --------------------------------------------------------------------- #\n    # Context helpers\n    # --------------------------------------------------------------------- #\n    def gather_codebase_context(\n        self,\n        root: Tuple[str, ...] = (\"cadence\", \"docs\"),\n        ext: Tuple[str, ...] = (\".py\", \".md\", \".json\", \".mermaid\"),\n        **kwargs,\n    ) -> str:\n        \"\"\"Return repo/docs snapshot via the injected ContextProvider.\"\"\"\n        return self.context_provider.get_context(*(Path(r) for r in root), exts=ext, **kwargs)\n",
  "src/cadence/agents/efficiency.py": "# src/cadence/agents/efficiency.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EFFICIENCY_PROFILE, AgentProfile\n\n\nclass EfficiencyAgent(BaseAgent):\n    \"\"\"\n    Final class: fast, low-cost linting & summarisation.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EFFICIENCY_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/agents/reasoning.py": "# src/cadence/agents/reasoning.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import REASONING_PROFILE, AgentProfile\n\n\nclass ReasoningAgent(BaseAgent):\n    \"\"\"\n    Final class: provides deep, chain-of-thought reasoning and architectural review.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = REASONING_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)\n\n    # Automatically inject a fresh code snapshot on each reset\n    def reset_context(self, system_prompt: str | None = None):\n        super().reset_context(system_prompt)\n        docs = self.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n        self.append_message(\"user\", f\"REFERENCE_DOCUMENTS:\\n{docs}\\n---\\nYou are cleared for deep reasoning.\")",
  "src/cadence/agents/profile.py": "# src/cadence/agents/profile.py\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any\n\n\n@dataclass(frozen=True, slots=True)\nclass AgentProfile:\n    \"\"\"\n    Immutable definition of an agent’s operational contract.\n\n    Nothing here executes code; it is pure data that can be validated,\n    serialised, or inspected by the Meta-agent and CI tooling.\n    \"\"\"\n    name: str\n    role: str\n    model: str\n    context_limit: int\n    review_policy: str = \"\"\n    default_system_prompt: str = \"\"\n    extra: Dict[str, Any] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n# Canonical profiles – these are the ONLY ones that Core Agents will default to\n# --------------------------------------------------------------------------- #\nREASONING_PROFILE = AgentProfile(\n    name=\"reasoning\",\n    role=\"plan-review\",\n    model=\"o3-2025-04-16\",\n    context_limit=200_000,\n    review_policy=\"Cannot commit code; must review Execution diff\",\n)\n\nEXECUTION_PROFILE = AgentProfile(\n    name=\"execution\",\n    role=\"implement\",\n    model=\"gpt-4.1\",\n    context_limit=1_000_000,\n    review_policy=\"Needs review by Reasoning or Efficiency\",\n)\n\nEFFICIENCY_PROFILE = AgentProfile(\n    name=\"efficiency\",\n    role=\"lint-summarise\",\n    model=\"o4-mini\",\n    context_limit=200_000,\n    review_policy=\"Reviews Execution unless diff is non-code\",\n)\n\n# Convenience lookup\nBUILTIN_PROFILES = {\n    \"reasoning\": REASONING_PROFILE,\n    \"execution\": EXECUTION_PROFILE,\n    \"efficiency\": EFFICIENCY_PROFILE,\n}",
  "src/cadence/agents/__init__.py": "\n",
  "src/cadence/agents/registry.py": "# src/cadence/agents/registry.py\n\"\"\"\nSingle place to obtain a Core Agent or Profile.\n\nAvoids hard-coding classes throughout the codebase.\n\"\"\"\n\nfrom typing import Type\n\nfrom .reasoning import ReasoningAgent\nfrom .execution import ExecutionAgent\nfrom .efficiency import EfficiencyAgent\nfrom .profile import BUILTIN_PROFILES, AgentProfile\n\n_CORE_AGENTS: dict[str, Type] = {\n    \"reasoning\": ReasoningAgent,\n    \"execution\": ExecutionAgent,\n    \"efficiency\": EfficiencyAgent,\n}\n\n\ndef get_agent(agent_type: str, **kwargs):\n    \"\"\"\n    Instantiate a Core Agent by `agent_type`.\n\n    Example:\n        agent = get_agent(\"execution\")\n    \"\"\"\n    if agent_type not in _CORE_AGENTS:\n        raise ValueError(f\"Unknown agent_type '{agent_type}'. Valid: {list(_CORE_AGENTS)}\")\n    return _CORE_AGENTS[agent_type](**kwargs)\n\n\ndef get_profile(profile_name: str) -> AgentProfile:\n    if profile_name not in BUILTIN_PROFILES:\n        raise ValueError(f\"Unknown profile '{profile_name}'. Valid: {list(BUILTIN_PROFILES)}\")\n    return BUILTIN_PROFILES[profile_name]",
  "src/cadence/agents/execution.py": "# src/cadence/agents/execution.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EXECUTION_PROFILE, AgentProfile\n\n\nclass ExecutionAgent(BaseAgent):\n    \"\"\"\n    Final class: generates or refactors significant portions of the codebase.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EXECUTION_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/llm/client.py": "# src/cadence/llm/client.py\nimport os\nimport logging\nimport asyncio\nfrom typing import List, Dict, Any, Optional, cast\nfrom openai import AsyncOpenAI, OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom dotenv import load_dotenv\n\n# One-time load\nload_dotenv()\n\n# Set up logger\nlogger = logging.getLogger(\"cadence.llm.client\")\nif not logger.handlers:\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\"))\n    logger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# Global default model configs\n_DEFAULT_MODELS = {\n    \"reasoning\": \"o3-2025-04-16\",\n    \"execution\": \"gpt-4.1\",\n    \"efficiency\": \"o4-mini\"\n}\n\ndef get_env(key: str, required=True, default=None):\n    val = os.getenv(key)\n    if not val and required:\n        raise RuntimeError(f\"Environment variable {key} not set.\")\n    return val or default\n\n# Centralized sync/async LLM client\nclass LLMClient:\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        default_model: Optional[str] = None,\n    ):\n        self.api_key = api_key or get_env('OPENAI_API_KEY')\n        self.api_base = api_base or os.getenv('OPENAI_API_BASE', None)\n        self.api_version = api_version or os.getenv('OPENAI_API_VERSION', None)\n        self.default_model = default_model or _DEFAULT_MODELS[\"execution\"]\n\n        # Sync and Async clients\n        self._async_client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base)\n        self._sync_client = OpenAI(api_key=self.api_key, base_url=self.api_base)\n\n    def _resolve_model(self, model: Optional[str], agent_type: Optional[str]):\n        if model:\n            return model\n        if agent_type and agent_type in _DEFAULT_MODELS:\n            return _DEFAULT_MODELS[agent_type]\n        return self.default_model\n\n    def call(\n        self,\n        messages: List[Dict[str, Any]],\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        logger.info(f\"LLM sync call: model={used_model}, msgs_len={len(msgs)}\")\n        response = self._sync_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            # max_tokens=max_tokens,\n            **kwargs\n        )\n        content = (response.choices[0].message.content or \"\").strip()\n        logger.debug(f\"LLM response: {content[:120]}...\")\n        return content\n\n    async def acall(\n        self,\n        messages: List[Dict[str, Any]],\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        logger.info(f\"LLM async call: model={used_model}, msgs_len={len(msgs)}\")\n        response = await self._async_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            max_tokens=max_tokens,\n            **kwargs\n        )\n        content = (response.choices[0].message.content or \"\").strip()\n        logger.debug(f\"LLM response: {content[:120]}...\")\n        return content\n\n# Provide a default client getter for agents\ndef get_default_client() -> LLMClient:\n    return _DEFAULT_CLIENT\n\n_DEFAULT_CLIENT = LLMClient()\n",
  "src/cadence/llm/__init__.py": "\n",
  "src/cadence/utils/add.py": "def add(x: int, y: int) -> int:\n    \"\"\"Intentionally wrong implementation for MVP red→green demo.\"\"\"\n    return x + y\n",
  "src/cadence/utils/mvp_loop.py": "# src/cadence/utils/mvp_loop.py\n\nimport pytest\nfrom src.cadence.dev.executor import TaskExecutor\nfrom src.cadence.dev.shell import ShellRunner\n\ndef manual_test():\n    result = pytest.main([\"tests\"])\n    if result != 0:\n        print(\"Tests failed.\")\n        # Read before\n        before = open(\"cadence/utils/add.py\").read()\n        print(\"Paste fixed implementation for utils/add.py below. End input with EOF (Ctrl+D):\")\n        after = []\n        try:\n            while True:\n                after.append(input())\n        except EOFError:\n            pass\n        after = \"\\n\".join(after)\n        # build diff\n        task = {\"diff\": {\"file\": \"cadence/utils/add.py\", \"before\": before, \"after\": after}}\n        patch = TaskExecutor(\"cadence/utils\").build_patch(task)\n        print(\"---Proposed Diff---\")\n        print(patch)\n\ndef OOP_test():\n    executor = TaskExecutor(src_root=\".\")\n    shell = ShellRunner(repo_dir=\".\")\n\n    # Dynamically read and patch the file\n    with open(\"cadence/utils/add.py\") as f:\n        before = f.read()\n    if \"return x + y\" not in before:\n        after = before.replace(\"return x - 1 + y\", \"return x + y\")\n    else:\n        print(\"Already correct: no patch needed.\")\n        return\n\n    task = {\n        \"diff\": {\n            \"file\": \"cadence/utils/add.py\",\n            \"before\": before,\n            \"after\": after\n        }\n    }\n\n    patch = executor.build_patch(task)\n    try:\n        shell.git_apply(patch)\n        # Run tests via ShellRunner\n        result = shell.run_pytest()\n        if result[\"success\"]:\n            sha = shell.git_commit(\"Fix add(): correct return expression\")\n            print(f\"Patch applied and tests passed. Commit SHA: {sha}\")\n        else:\n            print(\"Tests failed after patch:\\n\", result[\"output\"])\n    except Exception as e:\n        print(\"Patch failed:\", e)\n\n\n\nif __name__ == \"__main__\":\n    OOP_test()",
  "tests/test_add.py": "from cadence.utils.add import add\n\ndef test_add():\n    assert add(2, 3) == 5",
  "tests/test_failed_rollback.py": "\"\"\"\nRegression-test — Atomic rollback on downstream failure\n=======================================================\n\nPurpose\n-------\nVerify that *any* failure **after** a patch is applied but **before**\ncommit triggers an automatic rollback that restores a pristine working\ntree **and** writes the correct snapshots to TaskRecord.\n\nStrategy\n--------\n1.  Start with a clean repo where utils.add() is *correct* and all tests\n    pass.\n\n2.  Backlog contains a task whose patch **adds a brand-new failing test\n    file** – this guarantees pytest will fail *if* the patch is applied,\n    regardless of implementation details.\n\n3.  Run a full `DevOrchestrator` cycle (non-interactive).\n\n4.  Assert:\n        ─ orchestrator reports failure at the *test* stage;\n        ─ TaskRecord contains both `\"failed_test\"` **and**\n          `\"failed_test_and_rollback\"` snapshots;\n        ─ the failing test file is gone (working tree restored);\n        ─ original tests pass again and git status is clean.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Global stubs – applied automatically\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _stub_external(monkeypatch):\n    \"\"\"Stub out optional / external deps so the test is hermetic.\"\"\"\n    # Fake OpenAI client (LLM not used by this path)\n    fake_openai = sys.modules[\"openai\"] = type(sys)(\"openai\")\n    fake_openai.AsyncOpenAI = lambda *a, **k: None  # type: ignore\n    fake_openai.OpenAI = lambda *a, **k: None       # type: ignore\n\n    # Fake tabulate (pretty-printer)\n    fake_tabulate = sys.modules[\"tabulate\"] = type(sys)(\"tabulate\")\n    fake_tabulate.tabulate = lambda *a, **k: \"\"\n\n    # Satisfy LLMClient env check\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"dummy\")\n\n    # Ensure repository *parent* (containing “src/”) is importable\n    PROJ_ROOT = Path(__file__).resolve().parents[1]\n    if (PROJ_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJ_ROOT))\n\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# Repo bootstrap helpers\n# --------------------------------------------------------------------------- #\nGOOD_IMPL = \"def add(x, y):\\n    return x + y\\n\"\nFAILING_TEST = (\n    \"def test_intentional_failure():\\n\"\n    \"    assert False, 'This test is added by the patch and must fail'\\n\"\n)\n\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal Cadence project inside a temporary git repo.\"\"\"\n    repo = tmp_path\n\n    # --- source package ----------------------------------------------------\n    pkg_root = repo / \"src\" / \"cadence\" / \"utils\"\n    pkg_root.mkdir(parents=True, exist_ok=True)\n    (repo / \"src\" / \"__init__.py\").write_text(\"\")\n    (repo / \"src\" / \"cadence\" / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"add.py\").write_text(GOOD_IMPL)\n\n    # --- baseline passing test --------------------------------------------\n    tests_dir = repo / \"tests\"\n    tests_dir.mkdir()\n    (tests_dir / \"test_add.py\").write_text(\n        \"import sys, pathlib, os\\n\"\n        \"sys.path.insert(0, os.fspath((pathlib.Path(__file__).resolve().parents[2] / 'src')))\\n\"\n        \"from cadence.utils.add import add\\n\"\n        \"\\n\"\n        \"def test_add():\\n\"\n        \"    assert add(2, 3) == 5\\n\"\n    )\n\n    # --- git init ----------------------------------------------------------\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run(\n        [\"git\", \"commit\", \"-m\", \"initial good implementation\"],\n        cwd=repo,\n        check=True,\n        stdout=subprocess.DEVNULL,\n    )\n    return repo\n\n\ndef _make_backlog(repo: Path, record_file: Path) -> Path:\n    \"\"\"Write backlog.json with one task that *adds* a failing test.\"\"\"\n    task = {\n        \"id\": \"task-add-failing-test\",\n        \"title\": \"Add failing test to trigger rollback\",\n        \"type\": \"micro\",\n        \"status\": \"open\",\n        \"created_at\": \"2025-06-21T00:00:00Z\",\n        \"diff\": {\n            # New file relative to repo root\n            \"file\": \"tests/test_break.py\",\n            \"before\": \"\",                 # new file → empty 'before'\n            \"after\":  FAILING_TEST,\n        },\n    }\n    backlog_path = repo / \"backlog.json\"\n    backlog_path.write_text(json.dumps([task], indent=2))\n    record_file.write_text(\"[]\")  # fresh record\n    return backlog_path\n\n\ndef _orch_cfg(repo: Path, backlog: Path, record: Path) -> dict:\n    \"\"\"Return minimal DevOrchestrator config.\"\"\"\n    return {\n        \"backlog_path\": str(backlog),\n        \"template_file\": None,\n        \"src_root\": str(repo),\n        \"ruleset_file\": None,\n        \"repo_dir\": str(repo),\n        \"record_file\": str(record),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# The actual test\n# --------------------------------------------------------------------------- #\ndef test_atomic_rollback_on_failed_tests(tmp_path: Path):\n    \"\"\"\n    Full DevOrchestrator run — must:\n        • fail at test phase,\n        • rollback applied patch,\n        • leave working tree clean.\n    \"\"\"\n    repo = _init_repo(tmp_path)\n    record_file = repo / \"dev_record.json\"\n    backlog_file = _make_backlog(repo, record_file)\n\n    # Import *after* stubs are in place\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator(_orch_cfg(repo, backlog_file, record_file))\n    result = orch.run_task_cycle(select_id=\"task-add-failing-test\", interactive=False)\n\n    # ---- orchestrator result ---------------------------------------------\n    assert result[\"success\"] is False\n    assert result[\"stage\"] == \"test\"\n\n    # ---- TaskRecord snapshots --------------------------------------------\n    history: List[dict] = json.loads(record_file.read_text())[0][\"history\"]\n    states = [snap[\"state\"] for snap in history]\n    assert \"failed_test\" in states, \"failure snapshot missing\"\n    assert \"failed_test_and_rollback\" in states, \"rollback snapshot missing\"\n\n    # ---- Working tree validation -----------------------------------------\n    # 1. The intentionally failing test must be *gone*\n    assert not (repo / \"tests\" / \"test_break.py\").exists(), \"rollback did not remove new file\"\n\n    # 2. Original add() implementation still correct\n    sys.path.insert(0, str(repo / \"src\"))\n    from cadence.utils.add import add  # noqa: E402  (delayed import)\n\n    assert add(2, 3) == 5\n\n    # 3. Git working tree clean (no tracked-file changes)\n    status = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=repo,\n        stdout=subprocess.PIPE,\n        encoding=\"utf-8\",\n        check=True,\n    ).stdout.strip()\n\n    # Ignore purely *untracked* lines (begin with '??')\n    tracked_changes = [line for line in status.splitlines() if not line.startswith(\"??\")]\n    assert tracked_changes == [], (\n        \"tracked files modified after rollback:\\n\" + \"\\n\".join(tracked_changes)\n    )",
  "tests/test_state_recording.py": "# tests/test_state_recording.py\n\"\"\"\nIntegration test for TaskRecord integrity.\n\nRuns DevOrchestrator.run_task_cycle twice:\n\n1.  A green run where the patch fixes the bug and pytest passes.\n2.  A red run where the patch is a no-op so pytest fails.\n\nFor each run we assert that:\n    • mandatory state snapshots appear *in order* (allowing extra entries);\n    • `task.status` matches the state for *done* → *archived*;\n    • failure snapshots carry useful diagnostics.\n\nThe test is dependency-free: it stubs `openai` and `tabulate` before any\nCadence import so no network or extra wheels are required.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nfrom datetime import datetime, UTC\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\n# --------------------------------------------------------------------------- #\n# Global stubs – applied automatically by the autouse fixture\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _stub_external(monkeypatch):\n    \"\"\"Stub out optional / external deps so the test runs anywhere.\"\"\"\n    # Fake OpenAI client (LLM not used by DevOrchestrator path)\n    fake_openai = sys.modules[\"openai\"] = type(sys)(\"openai\")\n    fake_openai.AsyncOpenAI = lambda *a, **k: None  # type: ignore\n    fake_openai.OpenAI = lambda *a, **k: None       # type: ignore\n\n    # Fake tabulate to avoid CLI pretty-printer dependency\n    fake_tabulate = sys.modules[\"tabulate\"] = type(sys)(\"tabulate\")\n    fake_tabulate.tabulate = lambda *a, **k: \"\"\n\n    # Env var so LLMClient constructor is happy\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"dummy-key\")\n\n    # --- critical fix: ensure real Cadence code is importable ---------------\n    # We need the directory that CONTAINS the top-level “src/” package.\n    if (PROJECT_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJECT_ROOT))\n    # ----------------------------------------------------------------------- #\n\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# Repo bootstrap helpers\n# --------------------------------------------------------------------------- #\nBAD_IMPL = \"def add(x, y):\\n    return x - 1 + y\\n\"\nGOOD_IMPL = BAD_IMPL.replace(\"- 1 +\", \"+\")\n\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal Cadence project inside a temporary git repo.\"\"\"\n    repo = tmp_path\n\n    # Source package\n    pkg_root = repo / \"src\" / \"cadence\" / \"utils\"\n    pkg_root.mkdir(parents=True, exist_ok=True)\n    # PEP-420 implicit namespace would work, but an explicit file removes\n    # any ambiguity on Py<3.10 or odd tooling.\n    (repo / \"src\" / \"__init__.py\").write_text(\"\")\n    (repo / \"src\" / \"cadence\" / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"add.py\").write_text(BAD_IMPL)\n\n    # Unit test that will pass only if GOOD_IMPL is in place\n    tests_dir = repo / \"tests\"\n    tests_dir.mkdir()\n    (tests_dir / \"test_add.py\").write_text(\n        # Ensure repo/src is on import path _inside_ the pytest subprocess\n        \"import sys, pathlib, os\\n\"\n        \"sys.path.insert(0, os.fspath((pathlib.Path(__file__).resolve().parents[2] / 'src')))\\n\"\n        \"from cadence.utils.add import add\\n\"\n        \"\\n\"\n        \"def test_add():\\n\"\n        \"    assert add(2, 3) == 5\\n\"\n    )\n\n    # Initial git commit so `git apply` has a base tree\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n\n    return repo\n\n\ndef _make_backlog(repo: Path, record_file: Path, *, fix_bug: bool) -> Path:\n    \"\"\"Write backlog.json containing exactly one task and return the path.\"\"\"\n    # For the “red” path we still need a *non-empty* diff so the run\n    # proceeds through patch-apply and into pytest (where it will fail).\n    # - Green run: after_code fixes the defect.\n    # - Red  run: after_code == before_code  → diff is empty → PatchBuildError.\n    after_code = GOOD_IMPL if fix_bug else BAD_IMPL\n    task = {\n        \"id\": \"task-fix-add\",\n        \"title\": \"Fix utils.add bug\",\n        \"type\": \"micro\",\n        \"status\": \"open\",\n        \"created_at\": datetime.now(UTC).isoformat(),\n        \"diff\": {\n            \"file\": \"src/cadence/utils/add.py\",\n            \"before\": BAD_IMPL,\n            \"after\":  after_code,\n        },\n    }\n    backlog = repo / \"backlog.json\"\n    backlog.write_text(json.dumps([task], indent=2))\n    record_file.write_text(\"[]\")   # empty initial record\n    return backlog\n\n\ndef _orch_cfg(repo: Path, backlog: Path, record: Path) -> dict:\n    \"\"\"Return the minimal DevOrchestrator config dict.\"\"\"\n    return {\n        \"backlog_path\": str(backlog),\n        \"template_file\": None,\n        \"src_root\": str(repo),\n        \"ruleset_file\": None,\n        \"repo_dir\": str(repo),\n        \"record_file\": str(record),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# Parametrised integration test\n# --------------------------------------------------------------------------- #\n@pytest.mark.parametrize(\"fix_bug\", [True, False])\ndef test_task_record_snapshots(tmp_path: Path, fix_bug: bool):\n    \"\"\"\n    Ensure TaskRecord snapshots are written after every mutator or failure.\n    \"\"\"\n    repo = _init_repo(tmp_path)\n    record_file = repo / \"dev_record.json\"\n    backlog_file = _make_backlog(repo, record_file, fix_bug=fix_bug)\n\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator(_orch_cfg(repo, backlog_file, record_file))\n    result = orch.run_task_cycle(select_id=\"task-fix-add\", interactive=False)\n\n    # ----------------- Inspect TaskRecord ----------------- #\n    record: List[dict] = json.loads(record_file.read_text())\n    assert len(record) == 1, \"exactly one task record expected\"\n    history = record[0][\"history\"]\n    states = [snap[\"state\"] for snap in history]\n\n    common = [\n        \"build_patch\",\n        \"patch_built\",\n        \"patch_reviewed\",\n        \"patch_applied\",\n        \"pytest_run\",\n    ]\n    if fix_bug:\n        expected_seq = common + [\"committed\", \"status_done\", \"archived\"]\n\n        # Confirm green-path sequence\n        it = iter(states)\n        for label in expected_seq:\n            assert label in it, f\"missing or out-of-order state '{label}'\"\n    else:\n        # Red path: must terminate with some `failed_…` snapshot\n        assert not result[\"success\"], \"red run unexpectedly succeeded\"\n        assert states[-1].startswith(\"failed_\"), \"last snapshot must be a failure state\"\n        # And we still expect the initial 'build_patch' snapshot\n        assert states[0] == \"build_patch\"\n\n    # Semantic checks on snapshot contents\n    if fix_bug:\n        done_ix, arch_ix = states.index(\"status_done\"), states.index(\"archived\")\n        assert history[done_ix][\"task\"][\"status\"] == \"done\"\n        assert history[arch_ix][\"task\"][\"status\"] == \"archived\"\n    else:\n        extra = history[-1][\"extra\"]\n        assert extra, \"failure snapshot must include diagnostics\"\n        assert \"error\" in extra or \"pytest\" in extra"
}