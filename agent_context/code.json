{
  "src/cadence/__init__.py": "\n",
  "src/cadence/llm/client.py": "# src/cadence/llm/client.py\nimport os\nimport logging\nimport asyncio\nfrom typing import List, Dict, Any, Optional, cast\nfrom openai import AsyncOpenAI, OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom dotenv import load_dotenv\nimport tiktoken\nimport time\n\n# One-time load\nload_dotenv()\n\n# Set up logger\nlogger = logging.getLogger(\"cadence.llm.client\")\nif not logger.handlers:\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\"))\n    logger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# Global default model configs\n_DEFAULT_MODELS = {\n    \"reasoning\": \"o3-2025-04-16\",\n    \"execution\": \"gpt-4.1\",\n    \"efficiency\": \"o4-mini\"\n}\n\ndef get_env(key: str, required=True, default=None):\n    val = os.getenv(key)\n    if not val and required:\n        raise RuntimeError(f\"Environment variable {key} not set.\")\n    return val or default\n\ndef _count_tokens(model: str, messages: List[Dict[str, str]]) -> int:\n    enc = tiktoken.get_encoding(\"o200k_base\")\n    num = 0\n    for m in messages:\n        num += len(enc.encode(m[\"role\"])) + len(enc.encode(m[\"content\"]))\n    return num\n\n# Centralized sync/async LLM client\nclass LLMClient:\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        api_base: Optional[str] = None,\n        api_version: Optional[str] = None,\n        default_model: Optional[str] = None,\n    ):\n        self.api_key = api_key or get_env('OPENAI_API_KEY')\n        self.api_base = api_base or os.getenv('OPENAI_API_BASE', None)\n        self.api_version = api_version or os.getenv('OPENAI_API_VERSION', None)\n        self.default_model = default_model or _DEFAULT_MODELS[\"execution\"]\n\n        # Sync and Async clients\n        self._async_client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base)\n        self._sync_client = OpenAI(api_key=self.api_key, base_url=self.api_base)\n\n    def _resolve_model(self, model: Optional[str], agent_type: Optional[str]):\n        if model:\n            return model\n        if agent_type and agent_type in _DEFAULT_MODELS:\n            return _DEFAULT_MODELS[agent_type]\n        return self.default_model\n\n    def call(\n        self,\n        messages: List[Dict[str, Any]],\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n\n        prompt_tokens = _count_tokens(used_model, msgs)\n        t0 = time.perf_counter()\n\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        logger.info(\n            \"LLM sync call: model=%s  msgs=%d  prompt_toks≈%d\",\n            used_model, len(msgs), prompt_tokens\n        )\n        response = self._sync_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            # max_tokens=max_tokens,\n            **kwargs\n        )\n        content = (response.choices[0].message.content or \"\").strip()\n        dt = time.perf_counter() - t0\n        logger.info(\"LLM sync done:  %.2f s  completion≈%d toks\", dt, len(content) // 4)\n        logger.debug(f\"LLM response: {content[:120]}...\")\n        return content\n\n    async def acall(\n        self,\n        messages: List[Dict[str, Any]],\n        model: Optional[str] = None,\n        agent_type: Optional[str] = None,\n        system_prompt: Optional[str] = None,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        used_model = self._resolve_model(model, agent_type)\n        msgs = messages.copy()\n        prompt_tokens = _count_tokens(used_model, msgs)\n        t0 = time.perf_counter()\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n        logger.info(\n            \"LLM async call: model=%s  msgs=%d  prompt_toks≈%d\",\n            used_model, len(msgs), prompt_tokens\n        )\n        response = await self._async_client.chat.completions.create(  # type: ignore[arg-type]\n            model=used_model,\n            messages=cast(List[ChatCompletionMessageParam], msgs),\n            max_tokens=max_tokens,\n            **kwargs\n        )\n        content = (response.choices[0].message.content or \"\").strip()\n        dt = time.perf_counter() - t0\n        logger.info(\"LLM async done: %.2f s  completion≈%d toks\", dt, len(content) // 4)\n        logger.debug(f\"LLM response: {content[:120]}...\")\n        return content\n\n# Provide a default client getter for agents\ndef get_default_client() -> LLMClient:\n    return _DEFAULT_CLIENT\n\n_DEFAULT_CLIENT = LLMClient()\n",
  "src/cadence/llm/__init__.py": "\n",
  "src/cadence/context/provider.py": "# src/cadence/context/provider.py\nimport subprocess, sys, json\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nclass ContextProvider(ABC):\n    @abstractmethod\n    def get_context(self, *roots: Path, exts=(\".py\", \".md\"), max_bytes=50_000) -> str: ...\nclass SnapshotContextProvider(ContextProvider):\n    def get_context(self, *roots, exts=(\".py\", \".md\"), max_bytes=50_000, out=\"-\") -> str:\n        args = [sys.executable, \"tools/collect_code.py\"]\n        for r in roots: args += [\"--root\", str(r)]\n        for e in exts:  args += [\"--ext\", e]\n        args += [\"--max-bytes\", str(max_bytes), \"--out\", out]\n        return subprocess.run(args, capture_output=True, text=True, check=True).stdout\n",
  "src/cadence/context/__init__.py": "",
  "src/cadence/context/select.py": "# src/cadence/context/select.py\n\ndef select_context(target_paths: list[str], max_tokens: int = 50_000) -> str:\n    \"\"\"\n    Return BFS-ranked source blobs whose cumulative size ≤ max_tokens.\n    \"\"\"\n    ...",
  "src/cadence/agents/efficiency.py": "# src/cadence/agents/efficiency.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EFFICIENCY_PROFILE, AgentProfile\n\n\nclass EfficiencyAgent(BaseAgent):\n    \"\"\"\n    Final class: fast, low-cost linting & summarisation.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EFFICIENCY_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/agents/execution.py": "# src/cadence/agents/execution.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import EXECUTION_PROFILE, AgentProfile\n\n\nclass ExecutionAgent(BaseAgent):\n    \"\"\"\n    Final class: generates or refactors significant portions of the codebase.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = EXECUTION_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)",
  "src/cadence/agents/registry.py": "# src/cadence/agents/registry.py\n\"\"\"\nSingle place to obtain a Core Agent or Profile.\n\nAvoids hard-coding classes throughout the codebase.\n\"\"\"\n\nfrom typing import Type\n\nfrom .reasoning import ReasoningAgent\nfrom .execution import ExecutionAgent\nfrom .efficiency import EfficiencyAgent\nfrom .profile import BUILTIN_PROFILES, AgentProfile\n\n_CORE_AGENTS: dict[str, Type] = {\n    \"reasoning\": ReasoningAgent,\n    \"execution\": ExecutionAgent,\n    \"efficiency\": EfficiencyAgent,\n}\n\n\ndef get_agent(agent_type: str, **kwargs):\n    \"\"\"\n    Instantiate a Core Agent by `agent_type`.\n\n    Example:\n        agent = get_agent(\"execution\")\n    \"\"\"\n    if agent_type not in _CORE_AGENTS:\n        raise ValueError(f\"Unknown agent_type '{agent_type}'. Valid: {list(_CORE_AGENTS)}\")\n    return _CORE_AGENTS[agent_type](**kwargs)\n\n\ndef get_profile(profile_name: str) -> AgentProfile:\n    if profile_name not in BUILTIN_PROFILES:\n        raise ValueError(f\"Unknown profile '{profile_name}'. Valid: {list(BUILTIN_PROFILES)}\")\n    return BUILTIN_PROFILES[profile_name]",
  "src/cadence/agents/profile.py": "# src/cadence/agents/profile.py\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any\n\n\n@dataclass(frozen=True, slots=True)\nclass AgentProfile:\n    \"\"\"\n    Immutable definition of an agent’s operational contract.\n\n    Nothing here executes code; it is pure data that can be validated,\n    serialised, or inspected by the Meta-agent and CI tooling.\n    \"\"\"\n    name: str\n    role: str\n    model: str\n    context_limit: int\n    review_policy: str = \"\"\n    default_system_prompt: str = \"\"\n    extra: Dict[str, Any] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n# Canonical profiles – these are the ONLY ones that Core Agents will default to\n# --------------------------------------------------------------------------- #\nREASONING_PROFILE = AgentProfile(\n    name=\"reasoning\",\n    role=\"plan-review\",\n    model=\"o3-2025-04-16\",\n    context_limit=200_000,\n    review_policy=\"Cannot commit code; must review Execution diff\",\n)\n\nEXECUTION_PROFILE = AgentProfile(\n    name=\"execution\",\n    role=\"implement\",\n    model=\"gpt-4.1\",\n    context_limit=1_000_000,\n    review_policy=\"Needs review by Reasoning or Efficiency\",\n)\n\nEFFICIENCY_PROFILE = AgentProfile(\n    name=\"efficiency\",\n    role=\"lint-summarise\",\n    model=\"o4-mini\",\n    context_limit=200_000,\n    review_policy=\"Reviews Execution unless diff is non-code\",\n)\n\n# Convenience lookup\nBUILTIN_PROFILES = {\n    \"reasoning\": REASONING_PROFILE,\n    \"execution\": EXECUTION_PROFILE,\n    \"efficiency\": EFFICIENCY_PROFILE,\n}",
  "src/cadence/agents/__init__.py": "\n",
  "src/cadence/agents/sidekick.py": "# src/cadence/agents/sidekick.py\n\"\"\"\nPersona agent that *delegates* to a ReasoningAgent but presents a\nhuman-centric mentor/advisor interface.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\n\nfrom .profile import AgentProfile, REASONING_PROFILE\nfrom .reasoning import ReasoningAgent\n\n\n_SIDEKICK_PROMPT = \"\"\"\nYou are an AI-enhanced co-developer and mentor. Your primary goal is to\nextract the most creative, high-leverage ideas from the human user and\ntransform them into actionable improvements for the Cadence platform.\nAvoid tactical implementation details unless asked; focus on vision,\narchitecture, and pragmatic next steps.\n\"\"\"\n\n\nclass Sidekick:\n    \"\"\"\n    Thin wrapper: exposes `run_interaction` but delegates work to an\n    internal ReasoningAgent instance configured with a custom prompt.\n    \"\"\"\n\n    def __init__(self):\n        profile = AgentProfile(\n            name=\"sidekick\",\n            role=\"advisor\",\n            model=REASONING_PROFILE.model,\n            context_limit=REASONING_PROFILE.context_limit,\n            review_policy=REASONING_PROFILE.review_policy,\n            default_system_prompt=REASONING_PROFILE.default_system_prompt,\n            extra=REASONING_PROFILE.extra.copy() if REASONING_PROFILE.extra else {},\n        )\n        self._agent = ReasoningAgent(profile=profile, system_prompt=_SIDEKICK_PROMPT)\n        self._inject_seed_context()\n\n    # ------------------------------------------------------------------ #\n    # Public façade\n    # ------------------------------------------------------------------ #\n    def run_interaction(self, user_input: str, **kwargs) -> str:\n        return self._agent.run_interaction(user_input, **kwargs)\n\n    async def async_run_interaction(self, user_input: str, **kwargs) -> str:\n        return await self._agent.async_run_interaction(user_input, **kwargs)\n\n    # ------------------------------------------------------------------ #\n    # Private helpers\n    # ------------------------------------------------------------------ #\n    def _inject_seed_context(self):\n        docs = self._agent.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n\n        modules_path = Path(\"agent_context/module_contexts.json\")\n        modules = {}\n        if modules_path.exists():\n            modules = json.loads(modules_path.read_text())\n\n        self._agent.append_message(\n            \"user\",\n            f\"DOCS:\\n{docs}\\n---\\nMODULE_CONTEXTS:\\n{json.dumps(modules)[:10_000]}\",\n        )",
  "src/cadence/agents/reasoning.py": "# src/cadence/agents/reasoning.py\nfrom __future__ import annotations\n\nfrom .base import BaseAgent\nfrom .profile import REASONING_PROFILE, AgentProfile\n\n\nclass ReasoningAgent(BaseAgent):\n    \"\"\"\n    Final class: provides deep, chain-of-thought reasoning and architectural review.\n    \"\"\"\n\n    def __init__(self, profile: AgentProfile = REASONING_PROFILE, **kwargs):\n        super().__init__(profile, **kwargs)\n\n    # Automatically inject a fresh code snapshot on each reset\n    def reset_context(self, system_prompt: str | None = None):\n        super().reset_context(system_prompt)\n        docs = self.gather_codebase_context(\n            root=(\"docs\",),\n            ext=(\".md\", \".mermaid\", \".json\"),\n        )\n        self.append_message(\"user\", f\"REFERENCE_DOCUMENTS:\\n{docs}\\n---\\nYou are cleared for deep reasoning.\")",
  "src/cadence/agents/base.py": "# src/cadence/agents/base.py\nfrom __future__ import annotations\n\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\n\nfrom src.cadence.llm.client import LLMClient, get_default_client\nfrom src.cadence.context.provider import ContextProvider, SnapshotContextProvider\nfrom .profile import AgentProfile\n\n\nclass BaseAgent:\n    \"\"\"\n    The one true superclass for *all* Cadence agents.\n\n    An agent = (profile) + (conversation state) + (LLM client) [+ optional helpers]\n\n    Subclasses SHOULD NOT hard-code models; they inherit that from the supplied\n    `AgentProfile`.  Core agents (Reasoning / Execution / Efficiency) simply\n    pass the canonical profile; personas may inject a custom one.\n    \"\"\"\n\n    def __init__(\n        self,\n        profile: AgentProfile,\n        *,\n        llm_client: Optional[LLMClient] = None,\n        system_prompt: Optional[str] = None,\n        context_provider: Optional[ContextProvider] = None,\n    ):\n        self.profile = profile\n        self.llm_client = llm_client or get_default_client()\n        self.system_prompt = system_prompt or profile.default_system_prompt\n        self.context_provider = context_provider or SnapshotContextProvider()\n        self.messages: List[Dict[str, Any]] = []\n        self.reset_context()\n\n    # --------------------------------------------------------------------- #\n    # Conversation helpers\n    # --------------------------------------------------------------------- #\n    def reset_context(self, system_prompt: Optional[str] = None):\n        \"\"\"Clear history and (re)set the system prompt.\"\"\"\n        self.messages = []\n        sys_prompt = system_prompt or self.system_prompt\n        if sys_prompt:\n            self.append_message(\"system\", sys_prompt)\n\n    def append_message(self, role: str, content: str):\n        self.messages.append({\"role\": role, \"content\": content})\n\n    # --------------------------------------------------------------------- #\n    # LLM calls\n    # --------------------------------------------------------------------- #\n    def run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        response = self.llm_client.call(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,  # already injected\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        return response\n\n    async def async_run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        self.append_message(\"user\", user_input)\n        response = await self.llm_client.acall(\n            self.messages,\n            model=self.profile.model,\n            system_prompt=None,\n            **llm_kwargs,\n        )\n        self.append_message(\"assistant\", response)\n        return response\n\n    # --------------------------------------------------------------------- #\n    # Persistence\n    # --------------------------------------------------------------------- #\n    def save_history(self, path: str):\n        import json\n        Path(path).write_text(json.dumps(self.messages, indent=2, ensure_ascii=False))\n\n    def load_history(self, path: str):\n        import json\n        self.messages = json.loads(Path(path).read_text())\n\n    # --------------------------------------------------------------------- #\n    # Context helpers\n    # --------------------------------------------------------------------- #\n    def gather_codebase_context(\n        self,\n        root: Tuple[str, ...] = (\"cadence\", \"docs\"),\n        ext: Tuple[str, ...] = (\".py\", \".md\", \".json\", \".mermaid\"),\n        **kwargs,\n    ) -> str:\n        \"\"\"Return repo/docs snapshot via the injected ContextProvider.\"\"\"\n        return self.context_provider.get_context(*(Path(r) for r in root), exts=ext, **kwargs)\n",
  "src/cadence/utils/add.py": "def add(x: int, y: int) -> int:\n    \"\"\"Intentionally wrong implementation for MVP red→green demo.\"\"\"\n    return x + y\n",
  "src/cadence/utils/mvp_loop.py": "# src/cadence/utils/mvp_loop.py\n\nimport pytest\nfrom src.cadence.dev.executor import TaskExecutor\nfrom src.cadence.dev.shell import ShellRunner\n\ndef manual_test():\n    result = pytest.main([\"tests\"])\n    if result != 0:\n        print(\"Tests failed.\")\n        # Read before\n        before = open(\"cadence/utils/add.py\").read()\n        print(\"Paste fixed implementation for utils/add.py below. End input with EOF (Ctrl+D):\")\n        after = []\n        try:\n            while True:\n                after.append(input())\n        except EOFError:\n            pass\n        after = \"\\n\".join(after)\n        # build diff\n        task = {\"diff\": {\"file\": \"cadence/utils/add.py\", \"before\": before, \"after\": after}}\n        patch = TaskExecutor(\"cadence/utils\").build_patch(task)\n        print(\"---Proposed Diff---\")\n        print(patch)\n\ndef OOP_test():\n    executor = TaskExecutor(src_root=\".\")\n    shell = ShellRunner(repo_dir=\".\")\n\n    # Dynamically read and patch the file\n    with open(\"cadence/utils/add.py\") as f:\n        before = f.read()\n    if \"return x + y\" not in before:\n        after = before.replace(\"return x - 1 + y\", \"return x + y\")\n    else:\n        print(\"Already correct: no patch needed.\")\n        return\n\n    task = {\n        \"diff\": {\n            \"file\": \"cadence/utils/add.py\",\n            \"before\": before,\n            \"after\": after\n        }\n    }\n\n    patch = executor.build_patch(task)\n    try:\n        shell.git_apply(patch)\n        # Run tests via ShellRunner\n        result = shell.run_pytest()\n        if result[\"success\"]:\n            sha = shell.git_commit(\"Fix add(): correct return expression\")\n            print(f\"Patch applied and tests passed. Commit SHA: {sha}\")\n        else:\n            print(\"Tests failed after patch:\\n\", result[\"output\"])\n    except Exception as e:\n        print(\"Patch failed:\", e)\n\n\n\nif __name__ == \"__main__\":\n    OOP_test()",
  "src/cadence/dev/command_center.py": "\n# src/cadence/dev/command_center.py\n\nimport streamlit as st\n\n# You may need to adjust the import path according to your setup\nfrom src.cadence.dev.orchestrator import DevOrchestrator\n\n# ---- Basic Config (map to your dev environment) ----\nCONFIG = dict(\n    backlog_path=\"dev_backlog.json\",\n    template_file=\"dev_templates.json\",\n    src_root=\"cadence\",\n    ruleset_file=None,\n    repo_dir=\".\",\n    record_file=\"dev_record.json\"\n)\norch = DevOrchestrator(CONFIG)\n\n# ---- Session State Initialization ----\nif \"selected_task_id\" not in st.session_state:\n    st.session_state[\"selected_task_id\"] = None\nif \"phase\" not in st.session_state:\n    st.session_state[\"phase\"] = \"Backlog\"\n\n# ---- Sidebar: Phase Navigation ----\nst.sidebar.title(\"Cadence Dev Center\")\nphase = st.sidebar.radio(\n    \"Workflow phase\",\n    [\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"],\n    index=[\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"].index(st.session_state[\"phase\"])\n)\nst.session_state[\"phase\"] = phase\n\n# ---- Main: Backlog View ----\nif phase == \"Backlog\":\n    st.title(\"Task Backlog\")\n    open_tasks = orch.backlog.list_items(status=\"open\")\n    if not open_tasks:\n        st.info(\"No open tasks! Add tasks via CLI/Notebook.\")\n    else:\n        import pandas as pd\n        df = pd.DataFrame(open_tasks)\n        st.dataframe(df[[\"id\", \"title\", \"type\", \"status\", \"created_at\"]])\n        selected = st.selectbox(\n            \"Select a task to work on\",\n            options=[t[\"id\"] for t in open_tasks],\n            format_func=lambda tid: f'{tid[:8]}: {next(t[\"title\"] for t in open_tasks if t[\"id\"] == tid)}'\n        )\n        if st.button(\"Continue to task detail\"):\n            st.session_state[\"selected_task_id\"] = selected\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Task Detail View ----\nelif phase == \"Task Detail\":\n    st.title(\"Task Details\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    st.markdown(f\"**Title:** {task['title']}\\n\\n**Type:** {task['type']}\\n\\n**Status:** {task['status']}\\n\\n**Created:** {task['created_at']}\")\n    st.code(task.get(\"description\", \"\"), language=\"markdown\")\n    st.json(task)\n    if st.button(\"Proceed to Patch Review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()\n\n# ---- Patch Review ----\nelif phase == \"Patch Review\":\n    st.title(\"Patch Review & Approval\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    try:\n        patch = orch.executor.build_patch(task)\n        st.code(patch, language=\"diff\")\n        review = orch.reviewer.review_patch(patch, context=task)\n        st.markdown(\"### Review Comments\")\n        st.markdown(review[\"comments\"] or \"_No issues detected._\")\n        if review[\"pass\"]:\n            if st.button(\"Approve and Apply Patch\"):\n                # Apply patch, save, and proceed\n                orch.shell.git_apply(patch)\n                orch._record(task, \"patch_applied\")\n                st.success(\"Patch applied.\")\n                st.session_state[\"phase\"] = \"Run Test\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Patch failed review; please revise before continuing.\")\n            if st.button(\"Back to task detail\"):\n                st.session_state[\"phase\"] = \"Task Detail\"\n                st.experimental_rerun()\n    except Exception as ex:\n        st.error(f\"Patch build/review failed: {ex}\")\n        if st.button(\"Back to task detail\"):\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Run Test ----\nelif phase == \"Run Test\":\n    st.title(\"Run Pytest\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    st.markdown(\"Apply code patch complete. Run tests to confirm correctness.\")\n    if st.button(\"Run tests now\"):\n        test_result = orch.shell.run_pytest()\n        st.text_area(\"Test Output\", test_result[\"output\"], height=200)\n        if test_result[\"success\"]:\n            st.success(\"Tests passed!\")\n            if st.button(\"Proceed to Archive/Done\"):\n                # Commit and archive task\n                task = orch.backlog.get_item(task_id)\n                sha = orch.shell.git_commit(f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\")\n                orch.backlog.update_item(task_id, {\"status\": \"done\"})\n                orch.backlog.archive_completed()\n                # commit snapshot (task is still 'done' here)\n                orch.record.save(task, state=\"committed\", extra={\"commit_sha\": sha})\n                # refresh snapshot so we accurately log 'archived'\n                updated_task = orch.backlog.get_item(task_id)\n                orch.record.save(updated_task, state=\"archived\", extra={})\n                st.session_state[\"phase\"] = \"Archive\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Tests failed, fix required before progressing.\")\n    if st.button(\"Back to patch review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n\n# ---- Archive / Task Complete ----\nelif phase == \"Archive\":\n    st.title(\"Task Archived\")\n    st.success(\"Task flow completed. You may return to the backlog.\")\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"selected_task_id\"] = None\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()",
  "src/cadence/dev/shell.py": "# src/cadence/dev/shell.py\n\"\"\"\nCadence ShellRunner\n-------------------\n\nAdditions in this revision\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n1. **Phase-order enforcement**\n   • `git_apply`, `run_pytest`, and `git_commit` now cooperate with a\n     lightweight tracker that guarantees commits cannot occur unless a\n     patch has been applied *and* the test suite has passed.\n2. **Patch pre-check**\n   • `git_apply` performs `git apply --check` before mutating the\n     working tree, aborting early if the diff’s *before* image does not\n     match the current file contents.\n\nEnforced invariants\n-------------------\n• patch_applied   – set automatically after a successful `git_apply`\n• tests_passed    – set automatically after a green `run_pytest`\n• committed       – set after `git_commit`\n\nCommit is refused (ShellCommandError) unless **both**\n`patch_applied` *and* `tests_passed` are present for the task.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Optional, Dict, List, Set\n\nfrom .record import TaskRecord\nfrom .phase_guard import enforce_phase, PhaseOrderError\n\n\nclass ShellCommandError(Exception):\n    \"\"\"Raised when a shell/git/pytest command fails.\"\"\"\n\n\nclass ShellRunner:\n    \"\"\"\n    Wrapper around common git / pytest commands **with automatic failure\n    persistence** *and* runtime phase-order guarantees.\n    \"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction / context helpers\n    # ------------------------------------------------------------------ #\n    def __init__(self, repo_dir: str = \".\", *, task_record: TaskRecord | None = None):\n        self.repo_dir = os.path.abspath(repo_dir)\n        if not os.path.isdir(self.repo_dir):\n            raise ValueError(\n                f\"repo_dir '{self.repo_dir}' does not exist or is not a directory.\"\n            )\n\n        # Recording context (may be None for stand-alone usage)\n        self._record: TaskRecord | None = task_record\n        self._current_task: dict | None = None\n\n        # Phase-tracking:  task_id → {phase labels}\n        self._phase_flags: Dict[str, Set[str]] = {}\n\n    # ---- phase-tracking helpers ---------------------------------------\n    def _init_phase_tracking(self, task_id: str) -> None:\n        self._phase_flags.setdefault(task_id, set())\n\n    def _mark_phase(self, task_id: str, phase: str) -> None:\n        self._phase_flags.setdefault(task_id, set()).add(phase)\n\n    def _has_phase(self, task_id: str, phase: str) -> bool:\n        return phase in self._phase_flags.get(task_id, set())\n\n    # ------------------------------------------------------------------ #\n    def attach_task(self, task: dict | None):\n        \"\"\"\n        Attach the *current* task dict so that failures inside any shell\n        call can be persisted and phase order can be enforced.\n        \"\"\"\n        self._current_task = task\n        if task:\n            self._init_phase_tracking(task[\"id\"])\n\n    # ------------------------------------------------------------------ #\n    # Internal helper – persist failure snapshot (best-effort)\n    # ------------------------------------------------------------------ #\n    def _record_failure(\n        self,\n        *,\n        state: str,\n        error: Exception | str,\n        output: str = \"\",\n        cmd: List[str] | None = None,\n    ):\n        if not (self._record and self._current_task):\n            return  # runner used outside orchestrated flow\n        extra = {\"error\": str(error)}\n        if output:\n            extra[\"output\"] = output.strip()\n        if cmd:\n            extra[\"cmd\"] = \" \".join(cmd)\n        try:\n            self._record.save(self._current_task, state=state, extra=extra)\n        except Exception:  # noqa: BLE001 – failure recording must not raise\n            pass\n\n    # ------------------------------------------------------------------ #\n    # Git patch helpers\n    # ------------------------------------------------------------------ #\n    @enforce_phase(mark=\"patch_applied\")\n    def git_apply(self, patch: str, *, reverse: bool = False) -> bool:\n        \"\"\"\n        Apply a unified diff to the working tree *after* ensuring the\n        patch cleanly applies via `git apply --check`.\n        \"\"\"\n        stage = \"git_apply_reverse\" if reverse else \"git_apply\"\n\n        if not patch or not isinstance(patch, str):\n            err = ShellCommandError(\"No patch supplied to apply.\")\n            self._record_failure(state=f\"failed_{stage}\", error=err)\n            raise err\n\n        # Write patch to temporary file\n        with tempfile.NamedTemporaryFile(\n            mode=\"w+\", suffix=\".patch\", delete=False\n        ) as tf:\n            tf.write(patch)\n            tf.flush()\n            tf_path = tf.name\n\n        # --- pre-check --------------------------------------------------\n        check_cmd: List[str] = [\"git\", \"apply\", \"--check\"]\n        if reverse:\n            check_cmd.append(\"-R\")\n        check_cmd.append(tf_path)\n        result = subprocess.run(\n            check_cmd,\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=False,\n        )\n        if result.returncode != 0:\n            err = ShellCommandError(\n                f\"Patch pre-check failed: {result.stderr.strip() or result.stdout.strip()}\"\n            )\n            self._record_failure(\n                state=f\"failed_{stage}\",\n                error=err,\n                output=(result.stderr or result.stdout),\n                cmd=check_cmd,\n            )\n            os.remove(tf_path)\n            raise err\n\n        # --- actual apply ----------------------------------------------\n        cmd: List[str] = [\"git\", \"apply\"]\n        if reverse:\n            cmd.append(\"-R\")\n        cmd.append(tf_path)\n\n        try:\n            result = subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n\n            if result.returncode != 0:\n                raise ShellCommandError(\n                    f\"git apply failed: {result.stderr.strip() or result.stdout.strip()}\"\n                )\n            return True\n\n        except Exception as ex:  # noqa: BLE001 – blanket to ensure capture\n            output = \"\"\n            if \"result\" in locals():\n                output = (result.stdout or \"\") + \"\\n\" + (result.stderr or \"\")\n            self._record_failure(\n                state=f\"failed_{stage}\",\n                error=ex,\n                output=output,\n                cmd=cmd,\n            )\n            raise\n        finally:\n            os.remove(tf_path)\n\n    # ------------------------------------------------------------------ #\n    # Testing helpers\n    # ------------------------------------------------------------------ #\n    def run_pytest(self, test_path: Optional[str] = None) -> Dict:\n        \"\"\"\n        Run pytest on the given path (default: ./tests).\n\n        Success automatically marks the *tests_passed* phase.\n        Returns {'success': bool, 'output': str}\n        \"\"\"\n        stage = \"pytest\"\n        path = test_path or os.path.join(self.repo_dir, \"tests\")\n        if not os.path.exists(path):\n            err = ShellCommandError(f\"Tests path '{path}' does not exist.\")\n            self._record_failure(state=f\"failed_{stage}\", error=err)\n            raise err\n\n        cmd = [\"pytest\", \"-q\", path]\n        try:\n            result = subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n            passed = result.returncode == 0\n            output = (result.stdout or \"\") + \"\\n\" + (result.stderr or \"\")\n\n            if passed and self._current_task:\n                self._mark_phase(self._current_task[\"id\"], \"tests_passed\")\n\n            if not passed:\n                # Persist *test* failure even though we don't raise here\n                self._record_failure(\n                    state=\"failed_pytest\", error=\"pytest failed\", output=output, cmd=cmd\n                )\n            return {\"success\": passed, \"output\": output.strip()}\n\n        except Exception as ex:\n            self._record_failure(state=f\"failed_{stage}\", error=ex)\n            raise\n\n    # ------------------------------------------------------------------ #\n    # Commit helper\n    # ------------------------------------------------------------------ #\n    def git_commit(self, message: str) -> str:\n        \"\"\"\n        Commit **all** staged/changed files with the given commit message.\n\n        Phase-guard: refuses to commit unless *patch_applied* **and**\n        *tests_passed* are recorded for the current task.\n        Returns the new commit SHA string.\n        \"\"\"\n        stage = \"git_commit\"\n\n        # ---- phase-order enforcement -----------------------------------\n        if self._current_task:\n            tid = self._current_task[\"id\"]\n            missing: List[str] = []\n            if not self._has_phase(tid, \"patch_applied\"):\n                missing.append(\"patch_applied\")\n            if not self._has_phase(tid, \"tests_passed\"):\n                missing.append(\"tests_passed\")\n            if missing:\n                err = ShellCommandError(\n                    f\"Cannot commit – missing prerequisite phase(s): {', '.join(missing)}\"\n                )\n                self._record_failure(state=f\"failed_{stage}\", error=err)\n                raise err\n\n        def _run(cmd: List[str]):\n            return subprocess.run(\n                cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False,\n            )\n\n        try:\n            # Stage all changes\n            add_cmd = [\"git\", \"add\", \"-A\"]\n            result = _run(add_cmd)\n            if result.returncode != 0:\n                raise ShellCommandError(f\"git add failed: {result.stderr.strip()}\")\n\n            # Commit\n            commit_cmd = [\"git\", \"commit\", \"-m\", message]\n            result = _run(commit_cmd)\n            if result.returncode != 0:\n                if \"nothing to commit\" in (result.stderr + result.stdout).lower():\n                    raise ShellCommandError(\"git commit: nothing to commit.\")\n                raise ShellCommandError(f\"git commit failed: {result.stderr.strip()}\")\n\n            # Retrieve last commit SHA\n            sha_cmd = [\"git\", \"rev-parse\", \"HEAD\"]\n            result = subprocess.run(\n                sha_cmd,\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=True,\n            )\n\n            # Mark phase completed\n            if self._current_task:\n                self._mark_phase(self._current_task[\"id\"], \"committed\")\n\n            return result.stdout.strip()\n\n        except Exception as ex:\n            self._record_failure(\n                state=f\"failed_{stage}\",\n                error=ex,\n                output=(result.stderr if \"result\" in locals() else \"\"),\n            )\n            raise\n\n\n# --------------------------------------------------------------------------- #\n# Dev-only sanity CLI\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    runner = ShellRunner(\".\", task_record=None)  # no persistence\n    print(\"ShellRunner loaded. No CLI demo.\")",
  "src/cadence/dev/__init__.py": "\n",
  "src/cadence/dev/phase_guard.py": "# src/cadence/dev/phase_guard.py\n\"\"\"cadence.dev.phase_guard\n\nRuntime enforcement of Cadence workflow-phase ordering.\n\nA lightweight decorator (enforce_phase) raises PhaseOrderError\nwhenever a caller tries to execute a phase whose required predecessors\nhave not yet been completed for the current task.  The decorator is\ngeneric: any object that exposes\n\n· self._current_task   – dict with an “id” key\n· self._has_phase(id, phase) -> bool\n· self._mark_phase(id, phase)\ncan use it.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import Any, Callable, Tuple\n\nclass PhaseOrderError(RuntimeError):\n    \"\"\"Raised when workflow phases are executed out of order.\"\"\"\n\ndef enforce_phase(\n    *required_phases: str,\n    mark: str | None = None,\n) -> Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"\n    Decorate a method representing a phase transition.\n\n    Parameters\n    ----------\n    *required_phases :\n        Zero or more phase labels that **must already be complete**\n        for the current task before the wrapped method may run.\n\n    mark :\n        Optional phase label to record as *completed* automatically\n        **after** the wrapped method returns without raising.\n\n    Notes\n    -----\n    If the decorated object is used outside an agentic task context\n    (`self._current_task is None`) the decorator becomes a no-op.\n    \"\"\"\n\n    req: Tuple[str, ...] = tuple(required_phases)\n\n    def _decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n        @functools.wraps(func)\n        def _wrapper(self, *args, **kwargs):\n            task = getattr(self, \"_current_task\", None)\n            if task and req:\n                tid = task.get(\"id\")\n                missing = [p for p in req if not self._has_phase(tid, p)]\n                if missing:\n                    raise PhaseOrderError(\n                        f\"{func.__name__} cannot run – unmet phase(s): \"\n                        f\"{', '.join(missing)}\"\n                    )\n            # --- execute wrapped method -----------------------------------\n            result = func(self, *args, **kwargs)\n\n            # --- auto-mark completion ------------------------------------\n            if task and mark:\n                self._mark_phase(task[\"id\"], mark)\n            return result\n\n        return _wrapper\n\n    return _decorator",
  "src/cadence/dev/backlog.py": "# src/cadence/dev/backlog.py\n\n\"\"\"\nCadence BacklogManager\n---------------------\nThread-safe CRUD on the task backlog.\n\nKey changes (2025-06-21)\n• Introduced a process-local re-entrant lock (`threading.RLock`) named\n  `_lock`.  ALL public mutators and any internal helpers that touch shared\n  state or disk are now executed under `with self._lock: …`.\n• Read helpers (`list_items`, `get_item`, `export`, `__str__`) also acquire\n  the lock to guarantee a coherent snapshot even while writers operate.\n• Nested calls (e.g. `archive_completed()` → `save()`) are safe because\n  RLock is re-entrant.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport json\nimport uuid\nimport threading\nimport copy\nfrom typing import List, Dict, Optional\n\n# --------------------------------------------------------------------------- #\n# Exceptions\n# --------------------------------------------------------------------------- #\nclass BacklogEmptyError(Exception):\n    \"\"\"Raised if attempting to pop or select from an empty backlog.\"\"\"\n\n\nclass TaskStructureError(Exception):\n    \"\"\"Raised if a task dict doesn't conform to required structure.\"\"\"\n\n\nclass TaskNotFoundError(Exception):\n    \"\"\"Raised if a requested task_id is not in the backlog.\"\"\"\n\n\n# --------------------------------------------------------------------------- #\n# Constants / helpers\n# --------------------------------------------------------------------------- #\nREQUIRED_FIELDS = (\"id\", \"title\", \"type\", \"status\", \"created_at\")\n\n\n# --------------------------------------------------------------------------- #\n# BacklogManager\n# --------------------------------------------------------------------------- #\nclass BacklogManager:\n    \"\"\"\n    Manages Cadence backlog: micro-tasks, stories, and epics.\n    State is persisted to JSON.  All mutating operations are guarded\n    by an *instance-local* RLock to avoid intra-process race conditions.\n    \"\"\"\n\n    # ------------------------------- #\n    # Construction / loading\n    # ------------------------------- #\n    def __init__(self, backlog_path: str):\n        self.path = backlog_path\n        self._lock = threading.RLock()\n        self._items: List[Dict] = []\n        # load() already acquires the lock – safe to call here\n        self.load()\n\n    # ------------------------------- #\n    # Public API – READ\n    # ------------------------------- #\n    def list_items(self, status: str = \"open\") -> List[Dict]:\n        \"\"\"\n        Return a list of tasks filtered by status.\n        status: \"open\", \"in_progress\", \"done\", \"archived\" or \"all\"\n        \"\"\"\n        with self._lock:\n            data = (\n                list(self._items)\n                if status == \"all\"\n                else [item for item in self._items if item.get(\"status\", \"open\") == status]\n            )\n            # Shallow-copy so caller cannot mutate our internal state.\n            return [dict(item) for item in data]\n\n    def get_item(self, task_id: str) -> Dict:\n        \"\"\"Retrieve a single task by id (defensive copy).\"\"\"\n        with self._lock:\n            idx = self._task_index(task_id)\n            return dict(self._items[idx])\n\n    def export(self) -> List[Dict]:\n        \"\"\"Return a deep copy of *all* backlog items.\"\"\"\n        with self._lock:\n            return copy.deepcopy(self._items)\n\n    # ------------------------------- #\n    # Public API – WRITE / MUTATE\n    # ------------------------------- #\n    def add_item(self, task: Dict) -> None:\n        \"\"\"Add a new task to backlog (enforces structure & unique id).\"\"\"\n        with self._lock:\n            task = self._normalize_task(task)\n            if any(t[\"id\"] == task[\"id\"] for t in self._items):\n                raise TaskStructureError(f\"Duplicate task id: {task['id']}\")\n            self._items.append(task)\n            self.save()\n\n    def remove_item(self, task_id: str) -> None:\n        \"\"\"Soft-delete: mark a task as archived.\"\"\"\n        with self._lock:\n            idx = self._task_index(task_id)\n            self._items[idx][\"status\"] = \"archived\"\n            self.save()\n\n    def update_item(self, task_id: str, updates: Dict) -> None:\n        \"\"\"Update arbitrary fields of a task (e.g. assign, progress).\"\"\"\n        with self._lock:\n            idx = self._task_index(task_id)\n            self._items[idx].update(updates)\n            self.save()\n\n    def archive_completed(self) -> None:\n        \"\"\"Mark all tasks with status 'done' as 'archived'.\"\"\"\n        with self._lock:\n            changed = False\n            for item in self._items:\n                if item.get(\"status\") == \"done\":\n                    item[\"status\"] = \"archived\"\n                    changed = True\n            if changed:\n                self.save()\n\n    # ------------------------------- #\n    # Disk persistence (internal)\n    # ------------------------------- #\n    def save(self) -> None:\n        \"\"\"Persist backlog state atomically (under lock).\"\"\"\n        with self._lock:\n            tmp_path = self.path + \".tmp\"\n            with open(tmp_path, \"w\", encoding=\"utf8\") as f:\n                json.dump(self._items, f, indent=2)\n            os.replace(tmp_path, self.path)\n\n    def load(self) -> None:\n        \"\"\"Load backlog state from disk (gracefully handles missing file).\"\"\"\n        with self._lock:\n            if not os.path.exists(self.path):\n                self._items = []\n                return\n            with open(self.path, \"r\", encoding=\"utf8\") as f:\n                data = json.load(f)\n            if not isinstance(data, list):\n                raise ValueError(\"Backlog JSON must be a list of tasks\")\n            self._items = [self._normalize_task(t) for t in data]\n\n    # ------------------------------- #\n    # Internal helpers\n    # ------------------------------- #\n    def _task_index(self, task_id: str) -> int:\n        for ix, t in enumerate(self._items):\n            if t[\"id\"] == task_id:\n                return ix\n        raise TaskNotFoundError(f\"No task found with id={task_id}\")\n\n    @staticmethod\n    def _normalize_task(task: Dict) -> Dict:\n        \"\"\"Ensure mandatory fields are present; fill sensible defaults.\"\"\"\n        t = dict(task)  # shallow copy\n        for field in REQUIRED_FIELDS:\n            if field not in t:\n                if field == \"id\":\n                    t[\"id\"] = str(uuid.uuid4())\n                elif field == \"created_at\":\n                    import datetime\n\n                    t[\"created_at\"] = datetime.datetime.utcnow().isoformat()\n                elif field == \"status\":\n                    t[\"status\"] = \"open\"\n                elif field == \"type\":\n                    t[\"type\"] = \"micro\"\n                else:\n                    raise TaskStructureError(f\"Missing required field: {field}\")\n        if not isinstance(t[\"id\"], str):\n            t[\"id\"] = str(t[\"id\"])\n        return t\n\n    # ------------------------------- #\n    # Convenience string representation\n    # ------------------------------- #\n    def __str__(self) -> str:\n        from tabulate import tabulate\n\n        with self._lock:\n            if not self._items:\n                return \"(Backlog empty)\"\n            rows = [\n                (t[\"id\"][:8], t[\"title\"], t[\"type\"], t.get(\"status\", \"open\"), t.get(\"created_at\", \"\"))\n                for t in self._items\n                if t.get(\"status\") != \"archived\"\n            ]\n            headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n            return tabulate(rows, headers, tablefmt=\"github\")\n\n\n# --------------------------------------------------------------------------- #\n# Development-only smoke-test\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":  # pragma: no cover\n    mgr = BacklogManager(\"dev_backlog.json\")\n    print(mgr)\n",
  "src/cadence/dev/generator.py": "\n# src/cadence/dev/generator.py\n\n\"\"\"\nCadence TaskGenerator\n-------------------\nSingle Responsibility: Propose/generate well-formed tasks, optionally from template or rules/LLM seed.\nNever applies code or diffs. Future extensible to LLM/human agent.\n\"\"\"\n\nimport os, json, uuid, datetime, warnings\nfrom typing import List, Dict, Optional\n\nclass TaskTemplateError(Exception):\n    \"\"\"Raised if template file is not valid or incomplete.\"\"\"\n    pass\n\nREQUIRED_FIELDS = (\"title\", \"type\", \"status\", \"created_at\")\n\n\nclass TaskGenerator:\n    def __init__(self, template_file: str | None = None, *, strict: bool = False):\n        \"\"\"\n        Optionally supply a JSON / MD template file.  \n        If `strict` is False (default) and the file does **not** exist, we\n        continue with an empty template dictionary and merely warn.\n        \"\"\"\n        self.template_file = template_file\n        self._template_cache: Dict = {}\n        if template_file:\n            if os.path.exists(template_file):\n                self._template_cache = self._load_template(template_file)\n            elif strict:\n                # Original behaviour – hard-fail\n                raise TaskTemplateError(f\"Template file not found: {template_file}\")\n            else:\n                warnings.warn(\n                    f\"Template file '{template_file}' not found; \"\n                    \"proceeding with minimal fallback templates.\",\n                    RuntimeWarning,\n                )\n    \n    def generate_tasks(self, mode: str = \"micro\", count: int = 1, human_prompt: Optional[str]=None) -> List[Dict]:\n        \"\"\"\n        Return a list of well-formed tasks. \n        - mode: \"micro\", \"story\", \"epic\", etc.\n        - count: number of tasks to generate\n        - human_prompt: if provided, use as summary/title for each (e.g., \"Add new test\", for human CLI prompt workflow)\n        If template_file is used, will fill in mode-related templates.\n        \"\"\"\n        tasks = []\n        base_tpl = self._get_template_for_mode(mode)\n        now = datetime.datetime.utcnow().isoformat()\n        for i in range(count):\n            task = dict(base_tpl)\n            # Minimal fields: id, title, type, status, created_at\n            task[\"id\"] = str(uuid.uuid4())\n            task[\"type\"] = mode\n            task.setdefault(\"status\", \"open\")\n            task.setdefault(\"created_at\", now)\n            if human_prompt:\n                # Provide a default/barebones title/desc from human input\n                task[\"title\"] = human_prompt if count == 1 else f\"{human_prompt} [{i+1}]\"\n                task.setdefault(\"description\", human_prompt)\n            else:\n                # Fallback: title must be present; if not, use template/title from mode or 'Untitled'\n                task[\"title\"] = task.get(\"title\", f\"{mode.capitalize()} Task {i+1}\")\n                task.setdefault(\"description\", \"\")\n            self._validate_task(task)\n            tasks.append(task)\n        return tasks\n\n    def overwrite_tasks(self, new_tasks: List[Dict], output_path: Optional[str]=None) -> None:\n        \"\"\"\n        Replace all backlog tasks with given well-formed list (writes to output_path, else self.template_file).\n        \"\"\"\n        path = output_path or self.template_file\n        if not path:\n            raise TaskTemplateError(\"No output path specified to write tasks.\")\n        with open(path, \"w\", encoding=\"utf8\") as f:\n            json.dump([self._validate_task(t) for t in new_tasks], f, indent=2)\n\n    def _get_template_for_mode(self, mode: str) -> Dict:\n        \"\"\"\n        Get template for the given mode; falls back to default/minimal template.\n        \"\"\"\n        if self._template_cache and mode in self._template_cache:\n            return dict(self._template_cache[mode])  # deep copy\n        # Fallback: minimal template\n        return {\n            \"title\": \"\",\n            \"type\": mode,\n            \"status\": \"open\",\n            \"created_at\": \"\",\n            \"description\": \"\",\n        }\n\n    def _load_template(self, path: str) -> Dict:\n        \"\"\"\n        Loads a JSON template file mapping mode→template-dict.\n        If Markdown file with front-matter, parse the JSON front-matter.\n        \"\"\"\n        if not os.path.exists(path):\n            raise TaskTemplateError(f\"Template file not found: {path}\")\n        if path.endswith(\".md\"):\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                lines = f.readlines()\n            start, end = None, None\n            for i, line in enumerate(lines):\n                if line.strip() == \"```json\":\n                    start = i + 1\n                elif line.strip().startswith(\"```\") and start is not None and end is None:\n                    end = i\n                    break\n            if start is not None and end is not None:\n                json_str = \"\".join(lines[start:end])\n                tpl = json.loads(json_str)\n            else:\n                raise TaskTemplateError(\"Markdown template missing ```json ... ``` block.\")\n        else:\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                tpl = json.load(f)\n        if not isinstance(tpl, dict):\n            raise TaskTemplateError(\"Task template must be a dict mapping mode->template.\")\n        return tpl\n\n    def _validate_task(self, task: Dict) -> Dict:\n        \"\"\"\n        Ensures task has all required fields and correct types/formats.\n        Throws TaskTemplateError if not.\n        \"\"\"\n        for field in REQUIRED_FIELDS:\n            if field not in task or (field == \"title\" and not task[\"title\"].strip()):\n                raise TaskTemplateError(f\"Task missing required field: '{field}'\")\n        if not isinstance(task[\"type\"], str):\n            raise TaskTemplateError(\"Task type must be str.\")\n        if \"id\" in task and not isinstance(task[\"id\"], str):\n            task[\"id\"] = str(task[\"id\"])\n        # Optionally: check status value, etc.\n        return task\n\n    # For future agentic/LLM/human input: accept strings, call LLM API, etc.\n    # Extend here with agent hooks.\n\n# Standalone/test CLI example (not for production)\nif __name__ == \"__main__\":\n    # Example: generate 2 microtasks from default, print as JSON:\n    g = TaskGenerator()\n    tasks = g.generate_tasks(mode=\"micro\", count=2, human_prompt=\"Example user-initiated task\")\n    print(json.dumps(tasks, indent=2))",
  "src/cadence/dev/reviewer.py": "\n# src/cadence/dev/reviewer.py\n\n\"\"\"\nCadence TaskReviewer\n-------------------\nSingle Responsibility: Adjudicates patch/diff quality via rules/LLM/manual. Never applies code or diffs.\nFuture extensible: can host local ruleset, shell out to LLM agent, or use human-in-the-loop.\n\"\"\"\n\nimport os\nimport json\nfrom typing import Optional, Dict\n\nclass PatchReviewError(Exception):\n    \"\"\"Raised if review input is malformed or review fails outright (e.g. ruleset not found/valid).\"\"\"\n    pass\n\nclass TaskReviewer:\n    def __init__(self, ruleset_file: str = None):\n        \"\"\"\n        Optionally specify path to ruleset file (JSON list of rules),\n        or leave blank to use default built-in rules.\n        \"\"\"\n        self.ruleset_file = ruleset_file\n        self.rules = self._load_ruleset(ruleset_file) if ruleset_file else self._default_ruleset()\n\n    def review_patch(self, patch: str, context: Optional[dict] = None) -> Dict:\n        \"\"\"\n        Review a diff/patch string (unapplied) and optional context (task, commit message, etc).\n        Returns dict {'pass': bool, 'comments': str}\n        This uses static (offline) heuristics but can be swapped for agent/LLM in future.\n        \"\"\"\n        # Guard: Patch required\n        if not patch or not isinstance(patch, str):\n            return {'pass': False, 'comments': 'Patch missing or not a string.'}\n\n        # Apply rules in order. If any hard-fail, review fails.\n        comments = []\n        passed = True\n\n        for rule in self.rules:\n            ok, msg = rule(patch, context)\n            if not ok:\n                passed = False\n            if msg:\n                comments.append(msg)\n            if not ok:\n                # For now, fail-hard (but comment all)\n                break\n\n        return {'pass': passed, 'comments': \"\\n\".join(comments).strip()}\n\n    def _default_ruleset(self):\n        \"\"\"\n        Returns a list of static rule functions: (patch, context) → (bool, str)\n        \"\"\"\n        def not_empty_rule(patch, _):\n            if not patch.strip():\n                return False, \"Patch is empty.\"\n            return True, \"\"\n        def startswith_rule(patch, _):\n            if not patch.startswith((\"---\", \"diff \", \"@@ \")):\n                return False, \"Patch does not appear to be a valid unified diff.\"\n            return True, \"\"\n        def contains_todo_rule(patch, _):\n            if \"TODO\" in patch:\n                return False, \"Patch contains 'TODO'—code review must not introduce placeholders.\"\n            return True, \"\"\n\n        # Optionally check for too-huge diffs, or forbidden patterns, via rules below.\n        def size_limit_rule(patch, _):\n            line_count = patch.count(\"\\n\")\n            if line_count > 5000:  # Arbitrary large patch guard\n                return False, f\"Patch too large for standard review ({line_count} lines).\"\n            return True, \"\"\n        return [\n            not_empty_rule, \n            startswith_rule,\n            contains_todo_rule,\n            size_limit_rule,\n        ]\n\n    def _load_ruleset(self, path: str):\n        \"\"\"\n        Loads a simple external ruleset (for human/agent extension), e.g. as list of forbidden strings.\n        For extensibility only; advanced policies/LLMs should be subclassed onto this interface.\n        \"\"\"\n        if not os.path.exists(path):\n            raise PatchReviewError(f\"Ruleset file '{path}' not found.\")\n        with open(path, \"r\", encoding=\"utf8\") as f:\n            obj = json.load(f)\n        # Expect a list of {'type':..., 'pattern':..., ...} dicts for pattern rules\n        rules = []\n        def make_rule(ruleobj):\n            typ = ruleobj.get('type')\n            pattern = ruleobj.get('pattern')\n            msg = ruleobj.get('message', f\"Patch contains forbidden pattern: {pattern}\")\n            if typ == 'forbid':\n                def _inner(patch, _):\n                    if pattern in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            elif typ == 'require':\n                def _inner(patch, _):\n                    if pattern not in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            else:\n                # Ignore unknown rule types\n                def _inner(patch, _):\n                    return True, \"\"\n                return _inner\n        for ruleobj in obj:\n            rules.append(make_rule(ruleobj))\n        # Default rules always included\n        return self._default_ruleset() + rules\n\n# Standalone/example/test run\nif __name__ == \"__main__\":\n    reviewer = TaskReviewer()\n    # Good patch\n    patch = \"\"\"--- sample.py\n+++ sample.py\n@@ -1 +1,2 @@\n-print('hello')\n+print('hello world')\n\"\"\"\n    result = reviewer.review_patch(patch)\n    print(\"Result (should pass):\", result)\n\n    bad_patch = \"TODO: refactor\\n\"\n    result = reviewer.review_patch(bad_patch)\n    print(\"Result (should fail):\", result)",
  "src/cadence/dev/orchestrator.py": "# src/cadence/dev/orchestrator.py\n\"\"\"\nCadence DevOrchestrator\n-----------------------\nNow wires ShellRunner with TaskRecord and attaches the *current* task\nbefore any shell operation so that ShellRunner can persist failures.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom .backlog import BacklogManager\nfrom .generator import TaskGenerator\nfrom .executor import TaskExecutor, PatchBuildError\nfrom .reviewer import TaskReviewer\nfrom .shell import ShellRunner, ShellCommandError\nfrom .record import TaskRecord, TaskRecordError\n\nimport sys\nfrom typing import Any, Dict\n\n\nclass DevOrchestrator:\n    def __init__(self, config: dict):\n        self.backlog = BacklogManager(config[\"backlog_path\"])\n        self.generator = TaskGenerator(config.get(\"template_file\"))\n        self.record = TaskRecord(config[\"record_file\"])\n        # ShellRunner now receives TaskRecord so it can self-record failures\n        self.shell = ShellRunner(config[\"repo_dir\"], task_record=self.record)\n        self.executor = TaskExecutor(config[\"src_root\"])\n        self.reviewer = TaskReviewer(config.get(\"ruleset_file\"))\n\n    # ------------------------------------------------------------------ #\n    # Internal helper – ALWAYS log, never raise\n    # ------------------------------------------------------------------ #\n    def _record(self, task: dict, state: str, extra: Dict[str, Any] | None = None) -> None:\n        try:\n            self.record.save(task, state=state, extra=extra or {})\n        except TaskRecordError as e:\n            print(f\"[Record-Error] {e}\", file=sys.stderr)\n\n    # ------------------------------------------------------------------ #\n    # Pretty-printing helpers  (unchanged)\n    # ------------------------------------------------------------------ #\n    def show(self, status: str = \"open\", printout: bool = True):\n        items = self.backlog.list_items(status)\n        if printout:\n            print(self._format_backlog(items))\n        return items\n\n    def _format_backlog(self, items):\n        if not items:\n            return \"(Backlog empty)\"\n        from tabulate import tabulate\n\n        rows = [\n            (\n                t[\"id\"][:8],\n                t.get(\"title\", \"\")[:48],\n                t.get(\"type\", \"\"),\n                t.get(\"status\", \"\"),\n                t.get(\"created_at\", \"\")[:19],\n            )\n            for t in items\n            if t.get(\"status\") != \"archived\"\n        ]\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n        return tabulate(rows, headers, tablefmt=\"github\")\n\n    # ------------------------------------------------------------------ #\n    # Main workflow\n    # ------------------------------------------------------------------ #\n    def run_task_cycle(self, select_id: str | None = None, *, interactive: bool = True):\n        \"\"\"\n        End-to-end flow for ONE micro-task with auto-rollback on failure.\n        \"\"\"\n        rollback_patch: str | None = None\n        task: dict | None = None\n\n        try:\n            # 1. Select Task --------------------------------------------------\n            open_tasks = self.backlog.list_items(status=\"open\")\n            if not open_tasks:\n                raise RuntimeError(\"No open tasks in backlog.\")\n\n            if select_id:\n                task = next((t for t in open_tasks if t[\"id\"] == select_id), None)\n                if not task:\n                    raise RuntimeError(f\"Task id '{select_id}' not found in open backlog.\")\n            elif interactive:\n                print(self._format_backlog(open_tasks))\n                print(\"---\")\n                idx = self._prompt_pick(len(open_tasks))\n                task = open_tasks[idx]\n            else:\n                task = open_tasks[0]\n\n            print(f\"\\n[Selected task: {task['id'][:8]}] {task.get('title')}\\n\")\n\n            # Attach task so ShellRunner can self-record failures\n            self.shell.attach_task(task)\n\n            # 2. Build patch --------------------------------------------------\n            self._record(task, \"build_patch\")\n            try:\n                patch = self.executor.build_patch(task)\n                rollback_patch = patch\n                self._record(task, \"patch_built\", {\"patch\": patch})\n                print(\"--- Patch built ---\\n\", patch)\n            except PatchBuildError as ex:\n                self._record(task, \"failed_build_patch\", {\"error\": str(ex)})\n                print(f\"[X] Patch build failed: {ex}\")\n                return {\"success\": False, \"stage\": \"build_patch\", \"error\": str(ex)}\n\n            # 3. Review -------------------------------------------------------\n            review1 = self.reviewer.review_patch(patch, context=task)\n            self._record(task, \"patch_reviewed\", {\"review\": review1})\n            print(\"--- Review 1 ---\")\n            print(review1[\"comments\"] or \"(no comments)\")\n            if not review1[\"pass\"]:\n                self._record(task, \"failed_patch_review\", {\"review\": review1})\n                print(\"[X] Patch failed review, aborting.\")\n                return {\"success\": False, \"stage\": \"patch_review\", \"review\": review1}\n\n            # 4. Apply patch --------------------------------------------------\n            try:\n                self.shell.git_apply(patch)\n                self._record(task, \"patch_applied\")\n                print(\"[✔] Patch applied.\")\n            except ShellCommandError as ex:\n                self._record(task, \"failed_patch_apply\", {\"error\": str(ex)})\n                print(f\"[X] git apply failed: {ex}\")\n                return {\"success\": False, \"stage\": \"patch_apply\", \"error\": str(ex)}\n\n            # ------------------------------- #\n            # --- CRITICAL SECTION BEGIN --- #\n            # ------------------------------- #\n\n            # 5. Run tests ----------------------------------------------------\n            test_result = self.shell.run_pytest()\n            self._record(task, \"pytest_run\", {\"pytest\": test_result})\n            print(\"--- Pytest ---\")\n            print(test_result[\"output\"])\n\n            if not test_result[\"success\"]:\n                print(\"[X] Tests FAILED. Initiating rollback.\")\n                self._record(task, \"failed_test\", {\"pytest\": test_result})\n                self._attempt_rollback(task, rollback_patch, src_stage=\"test\")\n                return {\"success\": False, \"stage\": \"test\", \"test_result\": test_result}\n\n            # 6. Commit -------------------------------------------------------\n            commit_msg = f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\"\n            try:\n                sha = self.shell.git_commit(commit_msg)\n                self._record(task, \"committed\", {\"commit_sha\": sha})\n                print(f\"[✔] Committed as {sha}\")\n            except ShellCommandError as ex:\n                self._record(task, \"failed_commit\", {\"error\": str(ex)})\n                print(f\"[X] git commit failed: {ex}\")\n                self._attempt_rollback(task, rollback_patch, src_stage=\"commit\")\n                return {\"success\": False, \"stage\": \"commit\", \"error\": str(ex)}\n\n            # 7. Mark task done + archive ------------------------------------\n            self.backlog.update_item(task[\"id\"], {\"status\": \"done\"})\n            task = self.backlog.get_item(task[\"id\"])\n            self._record(task, \"status_done\")\n\n            self.backlog.archive_completed()\n            task = self.backlog.get_item(task[\"id\"])\n            self._record(task, \"archived\")\n            print(\"[✔] Task marked done and archived.\")\n\n            return {\"success\": True, \"commit\": sha, \"task_id\": task[\"id\"]}\n\n        except Exception as ex:\n            if task and rollback_patch:\n                self._attempt_rollback(task, rollback_patch, src_stage=\"unexpected\", quiet=True)\n            print(f\"[X] Cycle failed: {ex}\")\n            return {\"success\": False, \"error\": str(ex)}\n\n    # ------------------------------------------------------------------ #\n    # Rollback helper (unchanged logic)\n    # ------------------------------------------------------------------ #\n    def _attempt_rollback(self, task: dict, patch: str | None, *, src_stage: str, quiet: bool = False):\n        if not patch:\n            self._record(task, \"rollback_skip_no_patch\")\n            return\n\n        try:\n            self.shell.git_apply(patch, reverse=True)\n            self._record(task, f\"failed_{src_stage}_and_rollback\")\n            if not quiet:\n                print(\"[↩] Rollback successful – working tree restored.\")\n        except ShellCommandError as rb_ex:\n            self._record(\n                task,\n                \"critical_rollback_failure\",\n                {\"trigger\": src_stage, \"rollback_error\": str(rb_ex)},\n            )\n            print(f\"[!!] Rollback FAILED – manual fix required: {rb_ex}\")\n\n    # ------------------------------------------------------------------ #\n    # CLI + interactive helpers (unchanged from previous version)\n    # ------------------------------------------------------------------ #\n    def cli_entry(self, command: str, **kwargs):\n        try:\n            if command in (\"backlog\", \"show\"):\n                return self.show(status=kwargs.get(\"status\", \"open\"))\n            if command == \"start\":\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            if command == \"evaluate\":\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            if command == \"done\":\n                if \"id\" not in kwargs:\n                    print(\"You must supply a task id for 'done'.\")\n                    return\n                self.backlog.update_item(kwargs[\"id\"], {\"status\": \"done\"})\n                self.backlog.archive_completed()\n                print(f\"Task {kwargs['id']} marked as done and archived.\")\n                return\n            print(f\"Unknown command: {command}\")\n        except Exception as ex:\n            print(f\"[X] CLI command '{command}' failed: {ex}\")\n\n    def _prompt_pick(self, n):\n        while True:\n            ans = input(f\"Select task [0-{n-1}]: \")\n            try:\n                ix = int(ans)\n                if 0 <= ix < n:\n                    return ix\n            except Exception:\n                pass\n            print(\"Invalid. Try again.\")\n\n\n# --------------------------------------------------------------------------- #\n# Stand-alone execution helper\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    CONFIG = dict(\n        backlog_path=\"dev_backlog.json\",\n        template_file=\"dev_templates.json\",\n        src_root=\"cadence\",\n        ruleset_file=None,\n        repo_dir=\".\",\n        record_file=\"dev_record.json\",\n    )\n    orch = DevOrchestrator(CONFIG)\n\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command\", nargs=\"?\", help=\"show|start|evaluate|done\")\n    parser.add_argument(\"--id\", default=None, help=\"Task id to use\")\n    args = parser.parse_args()\n\n    orch.cli_entry(args.command or \"show\", id=args.id)",
  "src/cadence/dev/record.py": "# src/cadence/dev/record.py\n\n\"\"\"\nCadence TaskRecord\n------------------\nThread-safe, append-only persistence of task life-cycle history.\n\nKey upgrades (2025-06-21)\n• Replaced `threading.Lock` with **re-entrant** `threading.RLock` so\n  nested mutator calls (e.g., save() → _persist()) never dead-lock.\n• Every public mutator (save, append_iteration) and every private helper\n  that writes to disk now acquires the lock.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport json\nimport threading\nimport copy\nfrom typing import List, Dict, Optional\nfrom datetime import datetime, UTC\n\n# --------------------------------------------------------------------------- #\n# Exceptions\n# --------------------------------------------------------------------------- #\nclass TaskRecordError(Exception):\n    \"\"\"Custom error for task record issues.\"\"\"\n\n\n# --------------------------------------------------------------------------- #\n# TaskRecord\n# --------------------------------------------------------------------------- #\nclass TaskRecord:\n    def __init__(self, record_file: str):\n        self.record_file = record_file\n        self._lock = threading.RLock()  # <-- upgraded to RLock\n        self._records: List[Dict] = []\n        self._idmap: Dict[str, Dict] = {}\n        self._load()  # safe – _load() acquires the lock internally\n\n    # ------------------------------------------------------------------ #\n    # Public API – mutators\n    # ------------------------------------------------------------------ #\n    def save(self, task: dict, state: str, extra: dict | None = None) -> None:\n        \"\"\"\n        Append a new state snapshot for the given task_id.\n        \"\"\"\n        with self._lock:\n            record = self._find_or_create_record(task)\n            snapshot = {\n                \"state\": state,\n                \"timestamp\": self._now(),\n                \"task\": copy.deepcopy(task),\n                \"extra\": copy.deepcopy(extra) if extra else {},\n            }\n            record[\"history\"].append(snapshot)\n            self._sync_idmap()\n            self._persist()\n\n    def append_iteration(self, task_id: str, iteration: dict) -> None:\n        \"\"\"\n        Append a fine-grained iteration step (e.g. reviewer notes).\n        \"\"\"\n        with self._lock:\n            record = self._find_record(task_id)\n            if record is None:\n                raise TaskRecordError(f\"No record for task id={task_id}\")\n            iter_snapshot = {\"timestamp\": self._now(), **copy.deepcopy(iteration)}\n            record.setdefault(\"iterations\", []).append(iter_snapshot)\n            self._persist()\n\n    # ------------------------------------------------------------------ #\n    # Public API – read-only\n    # ------------------------------------------------------------------ #\n    def load(self) -> List[Dict]:\n        \"\"\"Return a deep copy of all records.\"\"\"\n        with self._lock:\n            return copy.deepcopy(self._records)\n\n    # ------------------------------------------------------------------ #\n    # Internal helpers (locking handled by callers)\n    # ------------------------------------------------------------------ #\n    def _find_or_create_record(self, task: dict) -> Dict:\n        tid = self._get_task_id(task)\n        rec = self._idmap.get(tid)\n        if rec is None:\n            rec = {\n                \"task_id\": tid,\n                \"created_at\": self._now(),\n                \"history\": [],\n                \"iterations\": [],\n            }\n            self._records.append(rec)\n            self._idmap[tid] = rec\n        return rec\n\n    def _find_record(self, task_id: str) -> Optional[Dict]:\n        return self._idmap.get(task_id)\n\n    @staticmethod\n    def _get_task_id(task: dict) -> str:\n        tid = task.get(\"id\")\n        if not tid:\n            raise TaskRecordError(\"Task dict missing 'id'. Cannot save record.\")\n        return tid\n\n    # ------------------------------------------------------------------ #\n    # Disk persistence & loading (always under lock)\n    # ------------------------------------------------------------------ #\n    def _persist(self) -> None:\n        with self._lock:\n            tmp = self.record_file + \".tmp\"\n            with open(tmp, \"w\", encoding=\"utf8\") as f:\n                json.dump(self._records, f, indent=2)\n            os.replace(tmp, self.record_file)\n\n    def _load(self) -> None:\n        with self._lock:\n            if not os.path.exists(self.record_file):\n                self._records = []\n                self._idmap = {}\n                return\n            with open(self.record_file, \"r\", encoding=\"utf8\") as f:\n                self._records = json.load(f)\n            self._sync_idmap()\n\n    def _sync_idmap(self):\n        self._idmap = {rec[\"task_id\"]: rec for rec in self._records}\n\n    @staticmethod\n    def _now():\n        return datetime.now(UTC).isoformat()\n\n\n# --------------------------------------------------------------------------- #\n# Dev-only sanity CLI\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":  # pragma: no cover\n    rec = TaskRecord(\"dev_record.json\")\n    tid = \"a1b2c3\"\n    task = {\"id\": tid, \"title\": \"Do something\", \"status\": \"open\"}\n    rec.save(task, state=\"patch_proposed\", extra={\"patch\": \"--- foo\"})\n    rec.append_iteration(tid, {\"reviewer\": \"alice\", \"opinion\": \"looks good\"})\n    import pprint\n\n    pprint.pp(rec.load())\n",
  "src/cadence/dev/executor.py": "# src/cadence/dev/executor.py\n\"\"\"\nCadence TaskExecutor\n--------------------\nNow guarantees every generated patch ends with a final newline, fixing the\n`git apply` “corrupt patch” error that occurred on some modified-file\ndiffs containing trailing context lines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport difflib\nfrom typing import Dict, List\n\nclass PatchBuildError(Exception):\n    pass\n\n\nclass TaskExecutor:\n    def __init__(self, src_root: str):\n        if not os.path.isdir(src_root):\n            raise ValueError(f\"src_root '{src_root}' is not a directory.\")\n        self.src_root = os.path.abspath(src_root)\n\n    # ------------------------------------------------------------------ #\n    def build_patch(self, task: Dict) -> str:\n        # >>> NEW: accept a pre-computed raw patch <<<\n        raw = task.get(\"patch\")\n        if isinstance(raw, str) and raw.strip():\n            return raw.strip() + (\"\\n\" if not raw.endswith(\"\\n\") else \"\")\n        try:\n            diff_info = task.get(\"diff\")\n            if not diff_info:\n                raise PatchBuildError(\"Task missing 'diff' key.\")\n\n            file_rel = diff_info.get(\"file\", \"\")\n            before   = diff_info.get(\"before\")\n            after    = diff_info.get(\"after\")\n            if not file_rel or before is None or after is None:\n                raise PatchBuildError(\"Diff dict must contain 'file', 'before', 'after'.\")\n\n            # --- normalise line endings -----------------------------------\n            if before and not before.endswith(\"\\n\"):\n                before += \"\\n\"\n            if after and not after.endswith(\"\\n\"):\n                after += \"\\n\"\n\n            before_lines: List[str] = before.splitlines(keepends=True) if before else []\n            after_lines:  List[str] = after.splitlines(keepends=True)  if after  else []\n\n            new_file    = len(before_lines) == 0 and len(after_lines) > 0\n            delete_file = len(before_lines) > 0 and len(after_lines) == 0\n\n            fromfile = \"/dev/null\" if new_file else f\"a/{file_rel}\"\n            tofile   = \"/dev/null\" if delete_file else f\"b/{file_rel}\"\n\n            diff_lines = list(\n                difflib.unified_diff(\n                    before_lines,\n                    after_lines,\n                    fromfile=fromfile,\n                    tofile=tofile,\n                    # default lineterm=\"\\n\"\n                )\n            )\n\n            patch = \"\".join(diff_lines)\n            # Ensure the patch ends with *exactly* one \\n ─ git is picky.\n            if not patch.endswith(\"\\n\"):\n                patch += \"\\n\"\n\n            if not patch.strip():\n                raise PatchBuildError(\"Generated patch is empty.\")\n\n            return patch\n\n        except Exception as e:\n            raise PatchBuildError(f\"Failed to build patch: {e}\")\n\n    # unchanged helpers …\n    def refine_patch(self, task: Dict, feedback: str) -> str:\n        raise NotImplementedError\n\n    def validate_patch(self, patch: str) -> bool:\n        return bool(patch and patch.startswith((\"---\", \"diff \", \"@@\")))\n\n\n# --------------------------------------------------------------------------- #\n# Quick manual demo\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    executor = TaskExecutor(src_root=\".\")\n    print(\n        executor.build_patch(\n            {\n                \"diff\": {\n                    \"file\": \"demo.txt\",\n                    \"before\": \"\",\n                    \"after\": \"hello\\nworld\\n\",\n                }\n            }\n        )\n    )",
  "tests/test_shell_failure_persistence.py": "\"\"\"\nRegression tests — Shell failure persistence\n============================================\n\nGoal\n----\nAssert that *every* failing shell operation executed through\n`cadence.dev.shell.ShellRunner` writes an explicit `failed_<stage>`\nsnapshot to the provided `TaskRecord` **before** the error propagates.\n\nWe stub `subprocess.run` so the tests are hermetic (no real git/pytest\ninvocations) and execute in milliseconds.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport subprocess\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import List, Tuple\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Helpers / stubs\n# --------------------------------------------------------------------------- #\nclass _FakeTaskRecord:\n    \"\"\"Minimal in-memory stand-in for cadence.dev.record.TaskRecord.\"\"\"\n\n    def __init__(self) -> None:\n        self.calls: List[dict] = []\n\n    # Signature matches real .save()\n    def save(self, task, state: str, extra: dict | None = None):\n        self.calls.append({\"task\": task, \"state\": state, \"extra\": extra or {}})\n\n\n@pytest.fixture(autouse=True)\ndef _ensure_importable(monkeypatch):\n    \"\"\"\n    Make the repository root (containing ``src/``) importable **everywhere**\n    so the tests run from any working directory or CI container.\n    \"\"\"\n    proj_root = Path(__file__).resolve().parents[1]\n    if (proj_root / \"src\").exists():\n        monkeypatch.syspath_prepend(str(proj_root))\n    yield\n\n\ndef _proc(rc=1, *, stdout: str = \"\", stderr: str = \"\") -> SimpleNamespace:\n    \"\"\"Return a dummy CompletedProcess-like object.\"\"\"\n    return SimpleNamespace(returncode=rc, stdout=stdout, stderr=stderr)\n\n\ndef _patch_subprocess(monkeypatch, mapping: dict[Tuple[str, str], SimpleNamespace]):\n    \"\"\"\n    Replace ``subprocess.run`` so that:\n\n        key = tuple(cmd[:2])   # e.g. (“git”, “apply”)\n\n    If *key* is in *mapping* → return that DummyProcess.\n    Otherwise → succeed (rc=0).\n    \"\"\"\n\n    def _fake_run(cmd, **_kwargs):\n        key = tuple(cmd[:2])\n        return mapping.get(key, _proc(rc=0))\n\n    monkeypatch.setattr(subprocess, \"run\", _fake_run)\n\n\ndef _make_runner(tmp_path: Path, record: _FakeTaskRecord):\n    \"\"\"Set up a ShellRunner pointed at an empty temp repo dir.\"\"\"\n    from src.cadence.dev.shell import ShellRunner\n\n    repo_dir = tmp_path / \"repo\"\n    repo_dir.mkdir()\n    runner = ShellRunner(repo_dir=str(repo_dir), task_record=record)\n    runner.attach_task({\"id\": \"task-1\", \"title\": \"demo\", \"status\": \"open\"})\n    return runner, repo_dir\n\n\n# --------------------------------------------------------------------------- #\n# Tests\n# --------------------------------------------------------------------------- #\ndef test_git_apply_failure_persists(monkeypatch, tmp_path: Path):\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _ = _make_runner(tmp_path, record)\n\n    # Simulate `git apply` failing\n    _patch_subprocess(\n        monkeypatch,\n        {(\"git\", \"apply\"): _proc(stderr=\"boom\")},\n    )\n\n    with pytest.raises(ShellCommandError):\n        runner.git_apply(\"--- broken diff\")\n\n    assert record.calls, \"TaskRecord.save was not called on failure\"\n    snapshot = record.calls[-1]\n    assert snapshot[\"state\"] == \"failed_git_apply\"\n    assert \"boom\" in snapshot[\"extra\"].get(\"error\", \"\") or \"boom\" in snapshot[\"extra\"].get(\n        \"output\", \"\"\n    )\n\n\ndef test_pytest_failure_persists(monkeypatch, tmp_path: Path):\n    record = _FakeTaskRecord()\n    runner, repo_dir = _make_runner(tmp_path, record)\n\n    # Ensure ./tests exists so run_pytest() doesn't raise path-missing error\n    (repo_dir / \"tests\").mkdir()\n\n    _patch_subprocess(\n        monkeypatch,\n        {(\"pytest\", \"-q\"): _proc(stdout=\"F..\", stderr=\"1 failed\")},\n    )\n\n    result = runner.run_pytest()\n    assert result[\"success\"] is False, \"stubbed pytest should fail\"\n\n    snapshot = record.calls[-1]\n    assert snapshot[\"state\"] == \"failed_pytest\"\n    assert \"1 failed\" in snapshot[\"extra\"][\"output\"]\n\n\ndef test_git_commit_failure_persists(monkeypatch, tmp_path: Path):\n    \"\"\"\n    Commit may now fail **either** because prerequisites were not met\n    (*phase-guard short-circuit*) **or** because `git commit` itself\n    returns a non-zero exit code.  Both paths must record a snapshot\n    with state ``failed_git_commit``.\n    \"\"\"\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _ = _make_runner(tmp_path, record)\n\n    # `git add` succeeds, `git commit` fails with \"nothing to commit\"\n    mapping = {\n        (\"git\", \"add\"): _proc(rc=0),\n        (\"git\", \"commit\"): _proc(rc=1, stderr=\"nothing to commit\"),\n    }\n    _patch_subprocess(monkeypatch, mapping)\n\n    with pytest.raises(ShellCommandError):\n        runner.git_commit(\"empty commit\")\n\n    snapshot = record.calls[-1]\n    assert snapshot[\"state\"] == \"failed_git_commit\"\n    # Accept either the original git-level error or the new phase-guard msg\n    err_msg = snapshot[\"extra\"][\"error\"]\n    assert (\n        \"nothing to commit\" in err_msg\n        or \"missing prerequisite phase\" in err_msg\n        or \"missing prerequisite phase(s)\" in err_msg\n    )",
  "tests/test_add.py": "from cadence.utils.add import add\n\ndef test_add():\n    assert add(2, 3) == 5",
  "tests/test_concurrency_locking.py": "# tests/test_concurrency_locking.py\n\"\"\"\nConcurrency / locking integration tests\n=======================================\n\nObjective\n---------\nEnsure that the new RLock-based protection in BacklogManager and TaskRecord\nprevents race-condition corruption when many threads mutate the same\nobjects *simultaneously*.\n\nThe test intentionally shares a single instance across  multiple threads\nto stress intra-process locking.  Cross-process safety (file-level\nlocking) is out-of-scope for this change-set.\n\"\"\"\n\nfrom __future__ import annotations\nimport json\nimport sys\nimport threading\nimport uuid\nfrom pathlib import Path\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Helper – ensure the repo \"src/\" folder is importable inside the test run\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _ensure_importable(monkeypatch):\n    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n    if (PROJECT_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJECT_ROOT))\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# BacklogManager concurrency test\n# --------------------------------------------------------------------------- #\ndef test_backlog_thread_safety(tmp_path: Path):\n    from src.cadence.dev.backlog import BacklogManager\n\n    backlog_path = tmp_path / \"backlog.json\"\n    mgr = BacklogManager(str(backlog_path))\n\n    THREADS = 10\n    TASKS_PER_THREAD = 100\n\n    def _worker(tid: int):\n        for i in range(TASKS_PER_THREAD):\n            mgr.add_item(\n                {\n                    \"title\": f\"task {tid}-{i}\",\n                    \"type\": \"micro\",\n                    \"status\": \"open\",\n                    \"created_at\": \"2025-06-21T00:00:00Z\",\n                }\n            )\n\n    threads = [threading.Thread(target=_worker, args=(n,)) for n in range(THREADS)]\n    for th in threads:\n        th.start()\n    for th in threads:\n        th.join(timeout=10)\n        assert not th.is_alive(), \"thread hung – possible deadlock\"\n\n    # Validate in-memory state\n    items = mgr.list_items(status=\"all\")\n    assert len(items) == THREADS * TASKS_PER_THREAD, \"missing or duplicate tasks in memory\"\n\n    # Validate on-disk JSON integrity\n    on_disk = json.loads(backlog_path.read_text())\n    assert len(on_disk) == len(items), \"disk state differs from memory state\"\n\n\n# --------------------------------------------------------------------------- #\n# TaskRecord concurrency test\n# --------------------------------------------------------------------------- #\ndef test_taskrecord_thread_safety(tmp_path: Path):\n    from src.cadence.dev.record import TaskRecord\n\n    record_path = tmp_path / \"record.json\"\n    tr = TaskRecord(str(record_path))\n\n    THREADS = 8\n    SAVES_PER_THREAD = 75\n\n    def _worker():\n        for _ in range(SAVES_PER_THREAD):\n            task_id = str(uuid.uuid4())\n            task = {\"id\": task_id, \"title\": \"concurrency\", \"status\": \"open\"}\n            tr.save(task, state=\"init\")\n\n    threads = [threading.Thread(target=_worker) for _ in range(THREADS)]\n    for th in threads:\n        th.start()\n    for th in threads:\n        th.join(timeout=10)\n        assert not th.is_alive(), \"thread hung – possible deadlock\"\n\n    # Verify integrity: unique task_id for each record\n    data = tr.load()\n    ids = [rec[\"task_id\"] for rec in data]\n    assert len(ids) == len(set(ids)), \"duplicate task_id detected – race condition?\"\n    assert len(ids) == THREADS * SAVES_PER_THREAD, \"missing records – some saves lost\"\n",
  "tests/test_phase_ordering_and_precheck.py": "\"\"\"\nTests for ShellRunner: diff pre-check & phase-ordering\n=====================================================\n\nThese tests verify that\n\n1.  A patch whose *before* image does **not** match the working tree\n    fails during the *pre-check* stage and records the correct snapshot.\n\n2.  `git_commit` is refused unless both *patch_applied* **and**\n    *tests_passed* phases are already recorded for the current task.\n\n3.  When phases are executed in the correct order\n    (apply → tests → commit) the commit succeeds and the *committed*\n    phase flag is set.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport subprocess\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import Dict, List, Tuple\n\nimport pytest\n\n# --------------------------------------------------------------------------- #\n# Helper – fake in-memory TaskRecord\n# --------------------------------------------------------------------------- #\nclass _FakeTaskRecord:\n    def __init__(self) -> None:\n        self.calls: List[dict] = []\n\n    def save(self, task, state: str, extra: dict | None = None):\n        self.calls.append({\"task\": task, \"state\": state, \"extra\": extra or {}})\n\n\n# --------------------------------------------------------------------------- #\n# Pytest fixtures / stubs\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _ensure_importable(monkeypatch):\n    \"\"\"\n    Ensure ``src/`` is import-searchable regardless of the cwd that the\n    test runner happens to use.\n    \"\"\"\n    proj_root = Path(__file__).resolve().parents[1]\n    if (proj_root / \"src\").exists():\n        monkeypatch.syspath_prepend(str(proj_root))\n    yield\n\n\ndef _proc(rc: int = 1, *, stdout: str = \"\", stderr: str = \"\") -> SimpleNamespace:\n    \"\"\"Return a dummy CompletedProcess-like object.\"\"\"\n    return SimpleNamespace(returncode=rc, stdout=stdout, stderr=stderr)\n\n\ndef _patch_subprocess(monkeypatch, mapping: Dict[Tuple[str, str], SimpleNamespace]):\n    \"\"\"\n    Monkey-patch ``subprocess.run`` so that the first two CLI tokens form a\n    lookup key.  If the key exists in *mapping* we return that fake\n    process; otherwise return a zero-exit stub.\n    \"\"\"\n\n    def _fake_run(cmd, **_kwargs):\n        key = tuple(cmd[:2])\n        return mapping.get(key, _proc(rc=0))\n\n    monkeypatch.setattr(subprocess, \"run\", _fake_run)\n\n\ndef _make_runner(tmp_path: Path, record: _FakeTaskRecord):\n    \"\"\"Return a (runner, repo_dir, task_id) triple.\"\"\"\n    from src.cadence.dev.shell import ShellRunner\n\n    repo_dir = tmp_path / \"repo\"\n    repo_dir.mkdir()\n    task = {\"id\": \"task-xyz\", \"title\": \"demo\", \"status\": \"open\"}\n    runner = ShellRunner(repo_dir=str(repo_dir), task_record=record)\n    runner.attach_task(task)\n    return runner, repo_dir, task[\"id\"]\n\n\n# --------------------------------------------------------------------------- #\n# Test 1 – diff pre-check failure\n# --------------------------------------------------------------------------- #\ndef test_patch_precheck_failure(monkeypatch, tmp_path: Path):\n    \"\"\"\n    git apply --check returns non-zero → ShellRunner must raise and record\n    ``failed_git_apply`` without setting *patch_applied*.\n    \"\"\"\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _repo_dir, tid = _make_runner(tmp_path, record)\n\n    # Pre-check fails\n    _patch_subprocess(monkeypatch, {(\"git\", \"apply\"): _proc(stderr=\"mismatch\")})\n\n    with pytest.raises(ShellCommandError):\n        runner.git_apply(\"--- broken diff\")\n\n    # Snapshot written\n    snap = record.calls[-1]\n    assert snap[\"state\"] == \"failed_git_apply\"\n    assert \"mismatch\" in snap[\"extra\"][\"error\"] or \"mismatch\" in snap[\"extra\"].get(\n        \"output\", \"\"\n    )\n\n    # Phase flag **not** set\n    assert not runner._has_phase(tid, \"patch_applied\")  # pylint: disable=protected-access\n\n\n# --------------------------------------------------------------------------- #\n# Test 2 – commit refused when prerequisites are missing\n# --------------------------------------------------------------------------- #\ndef test_commit_refused_without_prerequisites(monkeypatch, tmp_path: Path):\n    from src.cadence.dev.shell import ShellCommandError\n\n    record = _FakeTaskRecord()\n    runner, _repo_dir, _tid = _make_runner(tmp_path, record)\n\n    # Underlying git commands would *succeed* but the phase guard should\n    # short-circuit first.\n    _patch_subprocess(\n        monkeypatch,\n        {\n            (\"git\", \"add\"): _proc(rc=0),\n            (\"git\", \"commit\"): _proc(rc=0),  # never reached\n        },\n    )\n\n    with pytest.raises(ShellCommandError) as exc:\n        runner.git_commit(\"should fail\")\n\n    assert \"missing prerequisite phase\" in str(exc.value)\n    snap = record.calls[-1]\n    assert snap[\"state\"] == \"failed_git_commit\"\n\n\n# --------------------------------------------------------------------------- #\n# Test 3 – happy-path: apply → tests → commit\n# --------------------------------------------------------------------------- #\ndef test_full_success_flow(monkeypatch, tmp_path: Path):\n    \"\"\"\n    Execute the correct phase sequence and assert that commit succeeds and\n    the internal *committed* flag is set.\n    \"\"\"\n    record = _FakeTaskRecord()\n    runner, repo_dir, tid = _make_runner(tmp_path, record)\n\n    # --- make an empty ./tests folder so ShellRunner.run_pytest() passes its\n    #     early path-existence guard.\n    (Path(repo_dir) / \"tests\").mkdir()\n\n    sha = \"abc123\"\n\n    _patch_subprocess(\n        monkeypatch,\n        {\n            # Patch pre-check OK, apply OK\n            (\"git\", \"apply\"): _proc(rc=0),\n            # Pytest green\n            (\"pytest\", \"-q\"): _proc(rc=0, stdout=\"\"),\n            # Git plumbing\n            (\"git\", \"add\"): _proc(rc=0),\n            (\"git\", \"commit\"): _proc(rc=0),\n            (\"git\", \"rev-parse\"): _proc(rc=0, stdout=f\"{sha}\\n\"),\n        },\n    )\n\n    # 1. apply\n    runner.git_apply(\"--- dummy diff\")\n\n    # 2. tests\n    py_res = runner.run_pytest()\n    assert py_res[\"success\"] is True\n\n    # 3. commit\n    out_sha = runner.git_commit(\"commit msg\")\n    assert out_sha == sha\n    assert runner._has_phase(tid, \"committed\")  # pylint: disable=protected-access",
  "tests/test_failed_rollback.py": "\"\"\"\nRegression-test — Atomic rollback on downstream failure\n=======================================================\n\nPurpose\n-------\nVerify that *any* failure **after** a patch is applied but **before**\ncommit triggers an automatic rollback that restores a pristine working\ntree **and** writes the correct snapshots to TaskRecord.\n\nStrategy\n--------\n1.  Start with a clean repo where utils.add() is *correct* and all tests\n    pass.\n\n2.  Backlog contains a task whose patch **adds a brand-new failing test\n    file** – this guarantees pytest will fail *if* the patch is applied,\n    regardless of implementation details.\n\n3.  Run a full `DevOrchestrator` cycle (non-interactive).\n\n4.  Assert:\n        ─ orchestrator reports failure at the *test* stage;\n        ─ TaskRecord contains both `\"failed_test\"` **and**\n          `\"failed_test_and_rollback\"` snapshots;\n        ─ the failing test file is gone (working tree restored);\n        ─ original tests pass again and git status is clean.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\n\n\n# --------------------------------------------------------------------------- #\n# Global stubs – applied automatically\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _stub_external(monkeypatch):\n    \"\"\"Stub out optional / external deps so the test is hermetic.\"\"\"\n    # Fake OpenAI client (LLM not used by this path)\n    fake_openai = sys.modules[\"openai\"] = type(sys)(\"openai\")\n    fake_openai.AsyncOpenAI = lambda *a, **k: None  # type: ignore\n    fake_openai.OpenAI = lambda *a, **k: None       # type: ignore\n\n    # Fake tabulate (pretty-printer)\n    fake_tabulate = sys.modules[\"tabulate\"] = type(sys)(\"tabulate\")\n    fake_tabulate.tabulate = lambda *a, **k: \"\"\n\n    # Satisfy LLMClient env check\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"dummy\")\n\n    # Ensure repository *parent* (containing “src/”) is importable\n    PROJ_ROOT = Path(__file__).resolve().parents[1]\n    if (PROJ_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJ_ROOT))\n\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# Repo bootstrap helpers\n# --------------------------------------------------------------------------- #\nGOOD_IMPL = \"def add(x, y):\\n    return x + y\\n\"\nFAILING_TEST = (\n    \"def test_intentional_failure():\\n\"\n    \"    assert False, 'This test is added by the patch and must fail'\\n\"\n)\n\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal Cadence project inside a temporary git repo.\"\"\"\n    repo = tmp_path\n\n    # --- source package ----------------------------------------------------\n    pkg_root = repo / \"src\" / \"cadence\" / \"utils\"\n    pkg_root.mkdir(parents=True, exist_ok=True)\n    (repo / \"src\" / \"__init__.py\").write_text(\"\")\n    (repo / \"src\" / \"cadence\" / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"add.py\").write_text(GOOD_IMPL)\n\n    # --- baseline passing test --------------------------------------------\n    tests_dir = repo / \"tests\"\n    tests_dir.mkdir()\n    (tests_dir / \"test_add.py\").write_text(\n        \"import sys, pathlib, os\\n\"\n        \"sys.path.insert(0, os.fspath((pathlib.Path(__file__).resolve().parents[2] / 'src')))\\n\"\n        \"from cadence.utils.add import add\\n\"\n        \"\\n\"\n        \"def test_add():\\n\"\n        \"    assert add(2, 3) == 5\\n\"\n    )\n\n    # --- git init ----------------------------------------------------------\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run(\n        [\"git\", \"commit\", \"-m\", \"initial good implementation\"],\n        cwd=repo,\n        check=True,\n        stdout=subprocess.DEVNULL,\n    )\n    return repo\n\n\ndef _make_backlog(repo: Path, record_file: Path) -> Path:\n    \"\"\"Write backlog.json with one task that *adds* a failing test.\"\"\"\n    task = {\n        \"id\": \"task-add-failing-test\",\n        \"title\": \"Add failing test to trigger rollback\",\n        \"type\": \"micro\",\n        \"status\": \"open\",\n        \"created_at\": \"2025-06-21T00:00:00Z\",\n        \"diff\": {\n            # New file relative to repo root\n            \"file\": \"tests/test_break.py\",\n            \"before\": \"\",                 # new file → empty 'before'\n            \"after\":  FAILING_TEST,\n        },\n    }\n    backlog_path = repo / \"backlog.json\"\n    backlog_path.write_text(json.dumps([task], indent=2))\n    record_file.write_text(\"[]\")  # fresh record\n    return backlog_path\n\n\ndef _orch_cfg(repo: Path, backlog: Path, record: Path) -> dict:\n    \"\"\"Return minimal DevOrchestrator config.\"\"\"\n    return {\n        \"backlog_path\": str(backlog),\n        \"template_file\": None,\n        \"src_root\": str(repo),\n        \"ruleset_file\": None,\n        \"repo_dir\": str(repo),\n        \"record_file\": str(record),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# The actual test\n# --------------------------------------------------------------------------- #\ndef test_atomic_rollback_on_failed_tests(tmp_path: Path):\n    \"\"\"\n    Full DevOrchestrator run — must:\n        • fail at test phase,\n        • rollback applied patch,\n        • leave working tree clean.\n    \"\"\"\n    repo = _init_repo(tmp_path)\n    record_file = repo / \"dev_record.json\"\n    backlog_file = _make_backlog(repo, record_file)\n\n    # Import *after* stubs are in place\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator(_orch_cfg(repo, backlog_file, record_file))\n    result = orch.run_task_cycle(select_id=\"task-add-failing-test\", interactive=False)\n\n    # ---- orchestrator result ---------------------------------------------\n    assert result[\"success\"] is False\n    assert result[\"stage\"] == \"test\"\n\n    # ---- TaskRecord snapshots --------------------------------------------\n    history: List[dict] = json.loads(record_file.read_text())[0][\"history\"]\n    states = [snap[\"state\"] for snap in history]\n    assert \"failed_test\" in states, \"failure snapshot missing\"\n    assert \"failed_test_and_rollback\" in states, \"rollback snapshot missing\"\n\n    # ---- Working tree validation -----------------------------------------\n    # 1. The intentionally failing test must be *gone*\n    assert not (repo / \"tests\" / \"test_break.py\").exists(), \"rollback did not remove new file\"\n\n    # 2. Original add() implementation still correct\n    sys.path.insert(0, str(repo / \"src\"))\n    from cadence.utils.add import add  # noqa: E402  (delayed import)\n\n    assert add(2, 3) == 5\n\n    # 3. Git working tree clean (no tracked-file changes)\n    status = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=repo,\n        stdout=subprocess.PIPE,\n        encoding=\"utf-8\",\n        check=True,\n    ).stdout.strip()\n\n    # Ignore purely *untracked* lines (begin with '??')\n    tracked_changes = [line for line in status.splitlines() if not line.startswith(\"??\")]\n    assert tracked_changes == [], (\n        \"tracked files modified after rollback:\\n\" + \"\\n\".join(tracked_changes)\n    )",
  "tests/test_state_recording.py": "# tests/test_state_recording.py\n\"\"\"\nIntegration test for TaskRecord integrity.\n\nRuns DevOrchestrator.run_task_cycle twice:\n\n1.  A green run where the patch fixes the bug and pytest passes.\n2.  A red run where the patch is a no-op so pytest fails.\n\nFor each run we assert that:\n    • mandatory state snapshots appear *in order* (allowing extra entries);\n    • `task.status` matches the state for *done* → *archived*;\n    • failure snapshots carry useful diagnostics.\n\nThe test is dependency-free: it stubs `openai` and `tabulate` before any\nCadence import so no network or extra wheels are required.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nfrom datetime import datetime, UTC\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\n\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\n\n# --------------------------------------------------------------------------- #\n# Global stubs – applied automatically by the autouse fixture\n# --------------------------------------------------------------------------- #\n@pytest.fixture(autouse=True)\ndef _stub_external(monkeypatch):\n    \"\"\"Stub out optional / external deps so the test runs anywhere.\"\"\"\n    # Fake OpenAI client (LLM not used by DevOrchestrator path)\n    fake_openai = sys.modules[\"openai\"] = type(sys)(\"openai\")\n    fake_openai.AsyncOpenAI = lambda *a, **k: None  # type: ignore\n    fake_openai.OpenAI = lambda *a, **k: None       # type: ignore\n\n    # Fake tabulate to avoid CLI pretty-printer dependency\n    fake_tabulate = sys.modules[\"tabulate\"] = type(sys)(\"tabulate\")\n    fake_tabulate.tabulate = lambda *a, **k: \"\"\n\n    # Env var so LLMClient constructor is happy\n    monkeypatch.setenv(\"OPENAI_API_KEY\", \"dummy-key\")\n\n    # --- critical fix: ensure real Cadence code is importable ---------------\n    # We need the directory that CONTAINS the top-level “src/” package.\n    if (PROJECT_ROOT / \"src\").exists():\n        monkeypatch.syspath_prepend(str(PROJECT_ROOT))\n    # ----------------------------------------------------------------------- #\n\n    yield\n\n\n# --------------------------------------------------------------------------- #\n# Repo bootstrap helpers\n# --------------------------------------------------------------------------- #\nBAD_IMPL = \"def add(x, y):\\n    return x - 1 + y\\n\"\nGOOD_IMPL = BAD_IMPL.replace(\"- 1 +\", \"+\")\n\n\ndef _init_repo(tmp_path: Path) -> Path:\n    \"\"\"Create a minimal Cadence project inside a temporary git repo.\"\"\"\n    repo = tmp_path\n\n    # Source package\n    pkg_root = repo / \"src\" / \"cadence\" / \"utils\"\n    pkg_root.mkdir(parents=True, exist_ok=True)\n    # PEP-420 implicit namespace would work, but an explicit file removes\n    # any ambiguity on Py<3.10 or odd tooling.\n    (repo / \"src\" / \"__init__.py\").write_text(\"\")\n    (repo / \"src\" / \"cadence\" / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"__init__.py\").write_text(\"\")\n    (pkg_root / \"add.py\").write_text(BAD_IMPL)\n\n    # Unit test that will pass only if GOOD_IMPL is in place\n    tests_dir = repo / \"tests\"\n    tests_dir.mkdir()\n    (tests_dir / \"test_add.py\").write_text(\n        # Ensure repo/src is on import path _inside_ the pytest subprocess\n        \"import sys, pathlib, os\\n\"\n        \"sys.path.insert(0, os.fspath((pathlib.Path(__file__).resolve().parents[2] / 'src')))\\n\"\n        \"from cadence.utils.add import add\\n\"\n        \"\\n\"\n        \"def test_add():\\n\"\n        \"    assert add(2, 3) == 5\\n\"\n    )\n\n    # Initial git commit so `git apply` has a base tree\n    subprocess.run([\"git\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n    subprocess.run([\"git\", \"config\", \"user.email\", \"ci@example.com\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"config\", \"user.name\", \"CI\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"add\", \"-A\"], cwd=repo, check=True)\n    subprocess.run([\"git\", \"commit\", \"-m\", \"init\"], cwd=repo, check=True, stdout=subprocess.DEVNULL)\n\n    return repo\n\n\ndef _make_backlog(repo: Path, record_file: Path, *, fix_bug: bool) -> Path:\n    \"\"\"Write backlog.json containing exactly one task and return the path.\"\"\"\n    # For the “red” path we still need a *non-empty* diff so the run\n    # proceeds through patch-apply and into pytest (where it will fail).\n    # - Green run: after_code fixes the defect.\n    # - Red  run: after_code == before_code  → diff is empty → PatchBuildError.\n    after_code = GOOD_IMPL if fix_bug else BAD_IMPL\n    task = {\n        \"id\": \"task-fix-add\",\n        \"title\": \"Fix utils.add bug\",\n        \"type\": \"micro\",\n        \"status\": \"open\",\n        \"created_at\": datetime.now(UTC).isoformat(),\n        \"diff\": {\n            \"file\": \"src/cadence/utils/add.py\",\n            \"before\": BAD_IMPL,\n            \"after\":  after_code,\n        },\n    }\n    backlog = repo / \"backlog.json\"\n    backlog.write_text(json.dumps([task], indent=2))\n    record_file.write_text(\"[]\")   # empty initial record\n    return backlog\n\n\ndef _orch_cfg(repo: Path, backlog: Path, record: Path) -> dict:\n    \"\"\"Return the minimal DevOrchestrator config dict.\"\"\"\n    return {\n        \"backlog_path\": str(backlog),\n        \"template_file\": None,\n        \"src_root\": str(repo),\n        \"ruleset_file\": None,\n        \"repo_dir\": str(repo),\n        \"record_file\": str(record),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# Parametrised integration test\n# --------------------------------------------------------------------------- #\n@pytest.mark.parametrize(\"fix_bug\", [True, False])\ndef test_task_record_snapshots(tmp_path: Path, fix_bug: bool):\n    \"\"\"\n    Ensure TaskRecord snapshots are written after every mutator or failure.\n    \"\"\"\n    repo = _init_repo(tmp_path)\n    record_file = repo / \"dev_record.json\"\n    backlog_file = _make_backlog(repo, record_file, fix_bug=fix_bug)\n\n    from src.cadence.dev.orchestrator import DevOrchestrator\n\n    orch = DevOrchestrator(_orch_cfg(repo, backlog_file, record_file))\n    result = orch.run_task_cycle(select_id=\"task-fix-add\", interactive=False)\n\n    # ----------------- Inspect TaskRecord ----------------- #\n    record: List[dict] = json.loads(record_file.read_text())\n    assert len(record) == 1, \"exactly one task record expected\"\n    history = record[0][\"history\"]\n    states = [snap[\"state\"] for snap in history]\n\n    common = [\n        \"build_patch\",\n        \"patch_built\",\n        \"patch_reviewed\",\n        \"patch_applied\",\n        \"pytest_run\",\n    ]\n    if fix_bug:\n        expected_seq = common + [\"committed\", \"status_done\", \"archived\"]\n\n        # Confirm green-path sequence\n        it = iter(states)\n        for label in expected_seq:\n            assert label in it, f\"missing or out-of-order state '{label}'\"\n    else:\n        # Red path: must terminate with some `failed_…` snapshot\n        assert not result[\"success\"], \"red run unexpectedly succeeded\"\n        assert states[-1].startswith(\"failed_\"), \"last snapshot must be a failure state\"\n        # And we still expect the initial 'build_patch' snapshot\n        assert states[0] == \"build_patch\"\n\n    # Semantic checks on snapshot contents\n    if fix_bug:\n        done_ix, arch_ix = states.index(\"status_done\"), states.index(\"archived\")\n        assert history[done_ix][\"task\"][\"status\"] == \"done\"\n        assert history[arch_ix][\"task\"][\"status\"] == \"archived\"\n    else:\n        extra = history[-1][\"extra\"]\n        assert extra, \"failure snapshot must include diagnostics\"\n        assert \"error\" in extra or \"pytest\" in extra",
  "scripts/auto_generate_patches.py": "#!/usr/bin/env python3\n\"\"\"\nGenerate executable before/after diffs (or raw `patch`) for every open\ntask in dev_backlog.json.\n\nRun with --force to overwrite existing diff/patch blocks.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport tempfile\nfrom pathlib import Path\n\nfrom cadence.agents.registry import get_agent\nfrom cadence.dev.backlog import BacklogManager\n\n# --------------------------------------------------------------------- #\n# Config paths\n# --------------------------------------------------------------------- #\nBACKLOG_PATH = Path(os.getenv(\"CADENCE_BACKLOG\", \"dev_backlog.json\"))\n\nPROMPT_TPL   = Path(\"agent_context/base_execution_prompt.txt\")\nDOCS_JSON    = Path(\"agent_context/docs.json\")\nCODE_JSON    = Path(\"agent_context/code.json\")\nMODULES_JSON = Path(\"agent_context/module_contexts.json\")\n\n# --------------------------------------------------------------------- #\n# Rich system-prompt (≈30 k tokens once!)\n# --------------------------------------------------------------------- #\nif PROMPT_TPL.exists():\n    base_prompt = PROMPT_TPL.read_text()\n    with open(DOCS_JSON, \"r\", encoding=\"utf8\") as f:\n        with open(CODE_JSON, \"r\", encoding=\"utf8\") as g:\n            with open(MODULES_JSON, \"r\", encoding=\"utf8\") as h:\n                base_prompt = base_prompt.format(\n                    docs=json.load(f),\n                    codebase=json.load(g),\n                    contexts=json.load(h)\n                )\nelse:\n    base_prompt = \"You are Cadence ExecutionAgent.  Produce git-apply-able unified diffs.\"\n\ndef _make_agent():\n    # Each task gets a fresh ExecutionAgent with the same big prompt\n    return get_agent(\"execution\", system_prompt=base_prompt)\n\n# --------------------------------------------------------------------- #\n# Task-specific user prompt\n# --------------------------------------------------------------------- #\ndef _build_prompt(task: dict) -> str:\n    return (\n        \"You are an ExecutionAgent inside the Cadence platform.\\n\"\n        \"Produce a UNIFIED DIFF that implements exactly this task:\\n\\n\"\n        f\"{json.dumps(task, indent=2)}\\n\\n\"\n        \"STRICT REQUIREMENTS:\\n\"\n        \"• The diff MUST contain `--- a/<path>` and `+++ b/<path>` header lines.\\n\"\n        \"• Hunks must begin with @@, context lines with ' '.\\n\"\n        \"• It must apply with `git apply -p0` at repo root without error.\\n\"\n        \"• Return *only* one fenced  ```diff  code block – no explanation.\\n\"\n    ).strip()\n\n# --------------------------------------------------------------------- #\ndef _cli() -> argparse.Namespace:\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--force\", action=\"store_true\",\n                   help=\"Regenerate even if task already has diff or patch.\")\n    return p.parse_args()\n\n# --------------------------------------------------------------------- #\ndef main() -> None:\n    args = _cli()\n    bm   = BacklogManager(str(BACKLOG_PATH))\n\n    candidates = [\n        t for t in bm.list_items(\"open\")\n        if args.force or (\"diff\" not in t and \"patch\" not in t)\n    ]\n    if not candidates:\n        print(\"No tasks need patch generation.\")\n        return\n\n    for task in candidates:\n        agent = _make_agent()\n        agent.append_message(\"user\", _build_prompt(task))\n        llm_reply = agent.run_interaction(\"\")\n\n        # ---------------- extract fenced diff ---------------------------\n        fence = re.search(r\"```diff\\s*([\\s\\S]*?)```\", llm_reply)\n        patch = (fence.group(1) if fence else llm_reply).strip()\n\n        if not patch:\n            print(f\"[WARN] Empty patch for '{task['title']}', skipping.\")\n            continue\n\n        # ---------------- try to compute after-text ---------------------\n        file_match = re.search(r\"^[+-]{3}\\s+(?:a/|b/)?(.+)$\", patch, re.M)\n        if not file_match:\n            print(f\"[WARN] No file header found in patch for '{task['title']}', \"\n                  f\"storing raw patch only.\")\n            task[\"patch\"] = patch\n            bm.update_item(task[\"id\"], task)\n            continue\n\n        file_rel = file_match.group(1).strip()\n        before   = Path(file_rel).read_text(encoding=\"utf8\") if Path(file_rel).exists() else \"\"\n\n        try:\n            with tempfile.TemporaryDirectory() as td:\n                tmp_root = Path(td)\n                tmp_file = tmp_root / file_rel\n                tmp_file.parent.mkdir(parents=True, exist_ok=True)\n                tmp_file.write_text(before, encoding=\"utf8\")\n\n                subprocess.run(                         # may raise\n                    [\"git\", \"apply\", \"-p0\", \"-\"],\n                    input=patch, text=True, cwd=tmp_root, check=True\n                )\n                after = tmp_file.read_text(encoding=\"utf8\")\n\n        except subprocess.CalledProcessError as exc:\n            print(f\"[WARN] git apply failed for '{task['title']}'. \"\n                  f\"Storing raw patch. First 20 lines:\\n\"\n                  f\"{patch.splitlines()[:20]}\\n---\")\n            task[\"patch\"] = patch\n            bm.update_item(task[\"id\"], task)\n            continue\n\n        # -------------- success → store structured diff ----------------\n        task[\"diff\"] = {\"file\": file_rel, \"before\": before, \"after\": after}\n        task.pop(\"patch\", None)\n        bm.update_item(task[\"id\"], task)\n        print(f\"[OK] Diff attached for '{task['title']}' → {file_rel}\")\n\n    print(f\"Updated {len(candidates)} task(s).\")\n\nif __name__ == \"__main__\":\n    main()",
  "scripts/run_orchestrator.py": "# scripts/run_orchestrator.py\nfrom cadence.dev.orchestrator import DevOrchestrator\n\nCONFIG = {\n    \"backlog_path\": \"dev_backlog.json\",\n    \"template_file\": None,\n    \"src_root\": \"src\",          # <--- correct path\n    \"ruleset_file\": None,\n    \"repo_dir\": \".\",\n    \"record_file\": \"dev_record.json\",\n}\n\nif __name__ == \"__main__\":\n    orch = DevOrchestrator(CONFIG)\n    while True:\n        result = orch.run_task_cycle(interactive=False)\n        if not result.get(\"success\"):\n            break",
  "scripts/seed_round2_backlog.py": "# scripts/seed_round2_backlog.py\nfrom cadence.dev.generator import TaskGenerator\nfrom cadence.dev.backlog   import BacklogManager\n\n# ----- 2.1  create plain-language task shells -------------------------\nTASKS = [\n    {\"title\": \"TASK-1 Auto-replenish backlog\",            \"description\": \"\"\"Title: Auto-replenish backlog when empty\nGoal: Keep the pipeline perpetually flowing without human babysitting.\nImplementation Steps:\n\n1. Add **`DevOrchestrator._ensure_backlog()`** • If **`self.backlog.list_items(\"open\")`** is empty, call **`TaskGenerator.generate_tasks(mode=\"micro\", count=<N>)`** (N default = 3; expose CLI flag). • Persist the newly generated tasks with **`BacklogManager.add_item`**. • Record snapshot: **`state=\"backlog_replenished\"`**, extra={\"count\": N}.\n2. Call **`_ensure_backlog()`** at the very top of **`run_task_cycle()`**.\n3. Unit test: run an orchestrator in a temp repo with an empty backlog, assert it auto-populates.\n\nAcceptance: **`run_task_cycle(interactive=False)`** no longer raises **`RuntimeError`** when no tasks exist.\n\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-2 EfficiencyAgent second review\",     \"description\": \"\"\"Title: Wire EfficiencyAgent as mandatory second review\nGoal: Conform to DEV_PROCESS phase table (“Review” → Reasoning *and* Efficiency).\nImplementation Steps:\n\n1. In **`DevOrchestrator.__init__`** create **`self.efficiency = get_agent(\"efficiency\")`**.\n2. After **first** review passes, call **`eff_review = self.efficiency.run_interaction(<prompt_with_patch>)`** or, simpler for now, reuse **`TaskReviewer`** but tag the state **`\"efficiency_reviewed\"`**.\n3. Fail the task cycle unless both reviews pass.\n4. Record both review results with distinct states: **`\"patch_reviewed_reasoning\"`** / **`\"patch_reviewed_efficiency\"`**.\n5. Extend phase flags so **`git_commit`** requires **`\"efficiency_passed\"`** as well.\n\nAcceptance: A commit cannot occur unless *both* reviews have succeeded; tests updated accordingly.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-3 MetaAgent hook\",                    \"description\": \"\"\"Title: First-class MetaAgent hook\nGoal: Provide real-time governance / drift detection per DEV_PROCESS.\nImplementation Steps:\n\n1. Add simple **`MetaAgent.analyse(run_summary: dict)`** stub that just logs or appends to TaskRecord.\n2. Call it at the end of every **`run_task_cycle()`** (success *or* failure) with the full result dict.\n3. Record state **`\"meta_analysis\"`** plus whatever telemetry the MetaAgent returns.\n4. (Future-proof) Keep invocation behind **`config[\"enable_meta\"]`** flag (default True).\n\nAcceptance: TaskRecord shows a **`meta_analysis`** snapshot for every cycle; meta failures do not crash the run.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-4 Reviewer strict rule types\",        \"description\": \"\"\"Title: Harden TaskReviewer rule parsing\nGoal: Unknown rule types must never be ignored silently.\nImplementation Steps:\n\n1. In **`TaskReviewer._load_ruleset`** raise **`PatchReviewError`** **or** emit **`logger.warning`** when **`type`** is unrecognised.\n2. Provide **`strict`** constructor flag (default True).\n3. Add regression test loading a ruleset with an invalid type → expect exception or warning.\n\nAcceptance: CI fails (or logs) on an unrecognised rule type; no silent pass.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-5 Commit guard review flags\",         \"description\": \"\"\"Title: Expand enforce_phase → include review guards\nGoal: Prevent any commit unless **`\"review_passed\"`** *and* **`\"efficiency_passed\"`** flags exist.\nImplementation Steps:\n\n1. Add new decorator usage or explicit check in **`ShellRunner.git_commit`**: required = [\"patch_applied\", \"tests_passed\", \"review_passed\", \"efficiency_passed\"]\n2. Set those flags inside DevOrchestrator right after each successful review.\n3. Update tests in test_phase_ordering_and_precheck.py to assert commit fails without both review flags.\n\nAcceptance: New tests pass; existing tests updated to set the new flags on the happy path.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-6 Cross-process file locks\",          \"description\": \"\"\"Title: Cross-process file-locking for backlog & record\nGoal: Prevent two orchestrators on the same repo from racing.\nImplementation Steps:\n\n1. Add lightweight cross-process lock via **`filelock`** (pip-light) or portalocker.\n2. Acquire the lock in **`.save()`** and **`.load()`** of BacklogManager & TaskRecord *in addition* to the existing RLock. Lock file path = **`<jsonfile>.lock`**.\n3. Time-out (e.g., 10 s) then raise custom **`FileLockTimeoutError`**; caller should retry or alert.\n4. Add smoke test: spawn two **`multiprocessing.Process`** objects that hammer **`.add_item`**; assert no JSON corruption.\n\nAcceptance: Concurrency test passes; manual ctrl-C leaves **`.lock`** cleaned up.\"\"\", \"status\": \"open\"},\n    {\"title\": \"TASK-7 LLMClient stub mode\",               \"description\": \"\"\"Title: Graceful LLMClient fallback when env is missing\nGoal: Allow offline/CI runs without exporting OPENAI_API_KEY.\nImplementation Steps:\n\n1. In **`LLMClient.__init__`**, if api_key is missing: – log a **warning**; – enter “stub-mode”: **`.call()`** and **`.acall()`** return a canned message (e.g., **`\"LLM unavailable\"`**).\n2. Add **`self.stub = True`** flag; tests can assert behaviour.\n3. Update existing CI tests to expect stub-mode (they already monkey-patch OpenAI).\n\nAcceptance: Running orchestrator without the env var no longer crashes; warning is emitted exactly once per process.\"\"\", \"status\": \"open\"},\n]\n\ntg = TaskGenerator()\nwith_backfill = [*TASKS]            # TaskGenerator will fill id/created_at\nbm = BacklogManager(\"dev_backlog.json\")\nfor t in with_backfill:\n    bm.add_item(t)\n\nprint(f\"Backlog now contains {len(bm.list_items('open'))} open tasks.\")",
  "tools/module_contexts.py": "\nimport os\nimport json\nimport ast\nimport re\n\nEXCLUDES = {'archive', 'temp', 'code_payloads', '.git', '.pytest_cache', '__pycache__'}\nROOT = os.getcwd()\nCONTEXT_JSON = \"module_contexts.json\"\n\nDEFAULT_CONTEXT = dict(\n    purpose=\"\",\n    public_api=[],\n    depends_on=[],\n    used_by=[],\n    direct_imports=[],\n    related_schemas=[],\n    context_window_expected=\"\",\n    escalation_review=\"\",\n)\n\ndef relpath(path):\n    return os.path.relpath(path, ROOT).replace(os.sep, \"/\")\n\ndef get_module_import_path(rel_path):\n    # \"cadence/dev/executor.py\" -> \"cadence.dev.executor\"\n    p = rel_path\n    if p.endswith(\".py\"):\n        p = p[:-3]\n    if p.endswith(\"/__init__\"):\n        p = p[:-9]\n    return p.replace(\"/\", \".\")\n\ndef extract_and_strip_shebang_and_futures(lines):\n    shebang = None\n    futures = []\n    body = []\n    for line in lines:\n        if shebang is None and line.startswith(\"#!\"):\n            shebang = line\n            continue\n        m = re.match(r\"\\s*from __future__ import\", line)\n        if m:\n            # Avoid duplicates, but preserve order\n            if line not in futures:\n                futures.append(line)\n            continue\n        body.append(line)\n    return shebang, futures, body\n\ndef strip_duplicate_headers_at_top(lines):\n    \"\"\"Remove all context summary header blocks at the file top (before any code).\"\"\"\n    out = []\n    i = 0\n    n = len(lines)\n    while i < n:\n        line = lines[i]\n        # Allow blank lines and comments to stay at top\n        if line.strip() == \"\" or line.strip().startswith(\"#\"):\n            out.append(line)\n            i += 1\n            continue\n        # Remove all context headers at the top\n        if \"# MODULE CONTEXT SUMMARY\" in line:\n            while i < n and \"# END MODULE CONTEXT SUMMARY\" not in lines[i]:\n                i += 1\n            if i < n:\n                i += 1  # Skip END marker\n            # Keep going in case of further headers\n            continue\n        break  # Non-header, non-blank, non-comment: stop removing\n    out.extend(lines[i:])\n    # Remove extra blank lines at the start\n    while len(out) > 1 and out[0].strip() == \"\" and out[1].strip() == \"\":\n        out = out[1:]\n    return out\n\n\ndef find_existing_context(lines):\n    start = None\n    end = None\n    for i, line in enumerate(lines):\n        if \"MODULE CONTEXT SUMMARY\" in line:\n            start = i\n        if start is not None and \"END MODULE CONTEXT SUMMARY\" in line:\n            end = i\n            break\n    return (start, end) if start is not None and end is not None else (None, None)\n\ndef render_pretty_list(lst, indent=4):\n    if not lst:\n        return \"[]\"\n    pad = \" \" * indent\n    return \"[\\n\" + \"\".join(f\"{pad}{repr(x)},\\n\" for x in lst) + \"]\"\n\ndef render_context_block(rel, context):\n    def pretty(key):\n        val = context[key]\n        if isinstance(val, list):\n            return f\"{key}: {render_pretty_list(val)}\"\n        return f'{key}: \"{val}\"' if isinstance(val, str) else f\"{key}: {val}\"\n\n    lines = [\n        '\"\"\"# MODULE CONTEXT SUMMARY',\n        f'filepath: {rel}',\n        pretty(\"purpose\"),\n        pretty(\"public_api\"),\n        pretty(\"depends_on\"),\n        pretty(\"used_by\"),\n        pretty(\"direct_imports\"),\n        pretty(\"related_schemas\"),\n        pretty(\"context_window_expected\"),\n        pretty(\"escalation_review\"),\n        '# END MODULE CONTEXT SUMMARY\"\"\"',\n        ''\n    ]\n    return \"\\n\".join(lines) + \"\\n\"\n\ndef load_all_contexts():\n    if os.path.exists(CONTEXT_JSON):\n        with open(CONTEXT_JSON, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    else:\n        return {}\n\ndef write_all_contexts(contexts):\n    with open(CONTEXT_JSON, \"w\", encoding=\"utf-8\") as f:\n        json.dump(contexts, f, indent=2, ensure_ascii=False)\n\ndef scan_python_modules():\n    for dirpath, dirnames, filenames in os.walk(ROOT):\n        dirnames[:] = [d for d in dirnames if d not in EXCLUDES]\n        for fname in filenames:\n            if fname.endswith(\".py\") and fname not in EXCLUDES:\n                abspath = os.path.join(dirpath, fname)\n                yield relpath(abspath), abspath\n\ndef scan_all_internal_modules(root_dir):\n    internal = set()\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        for fname in filenames:\n            if fname.endswith(\".py\"):\n                abs_path = os.path.join(dirpath, fname)\n                rel = os.path.relpath(abs_path, root_dir).replace(os.sep, \"/\")\n                mod_path = get_module_import_path(rel)\n                internal.add(mod_path)\n    return internal\n\ndef parse_module(path, rel_path, all_internal_modules):\n    \"\"\"Returns (public_api, depends_on, direct_imports) for a python module.\n       - public_api: list of fully qualified names for top-level defs/classes in this file\n       - depends_on: internal modules imported (as import paths)\n       - direct_imports: all directly imported packages/modules (raw names, incl. external)\n    \"\"\"\n    public_api = []\n    depends_on = set()\n    direct_imports = set()\n\n    module_import_path = get_module_import_path(rel_path)\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            node = ast.parse(f.read(), filename=path)\n    except Exception:\n        return public_api, depends_on, direct_imports\n\n    # Top-level functions/classes\n    for n in node.body:\n        if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n            public_api.append(f\"{module_import_path}.{n.name}\")\n\n    # Imports\n    for n in ast.walk(node):\n        if isinstance(n, ast.Import):\n            for alias in n.names:\n                direct_imports.add(alias.name.split(\".\")[0])\n        elif isinstance(n, ast.ImportFrom):\n            mod = n.module\n            if mod:\n                mod_path = mod.replace(\".\", \"/\") + \".py\"\n                mod_import_path = mod.replace(\"/\", \".\")\n                direct_imports.add(mod.split(\".\")[0])\n                # Internal module dependency as import path (e.g. cadence.dev.executor)\n                if mod_import_path in all_internal_modules:\n                    depends_on.add(mod_import_path)\n    return sorted(public_api), sorted(depends_on), sorted(direct_imports)\n\ndef sync_contexts():\n    all_internal_modules = scan_all_internal_modules(ROOT)\n    all_contexts = load_all_contexts()\n    updated_contexts = {}\n    modified = 0\n    for rel, abspath in scan_python_modules():\n        context = dict(DEFAULT_CONTEXT)\n        context.update(all_contexts.get(rel, {}))\n        context['filepath'] = rel\n        public_api, depends_on, direct_imports = parse_module(abspath, rel, all_internal_modules)\n        context['public_api'] = public_api\n        context['depends_on'] = depends_on\n        context['direct_imports'] = direct_imports\n        with open(abspath, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n        # Extract and remove all shebang/future imports anywhere in the file\n        shebang, futures, lines_no_shebang = extract_and_strip_shebang_and_futures(lines)\n        # Remove all context header blocks at the top\n        code_body = strip_duplicate_headers_at_top(lines_no_shebang)\n        block = render_context_block(rel, context)\n        new_lines = []\n        if shebang:\n            new_lines.append(shebang)\n        if futures:\n            new_lines.extend(futures)\n        new_lines.append(block)\n        new_lines.extend(code_body)\n        # Ensure only one blank line after header\n        i = 1\n        while i < len(new_lines) and new_lines[i].strip() == \"\":\n            i += 1\n        if i > 2:\n            new_lines = [new_lines[0], \"\\n\"] + new_lines[i:]\n        with open(abspath, \"w\", encoding=\"utf-8\") as f:\n            f.writelines(new_lines)\n        updated_contexts[rel] = context\n        modified += 1\n    write_all_contexts(updated_contexts)\n    print(f\"Updated {modified} file(s) and wrote {CONTEXT_JSON}.\")\n\n\n\ndef print_context(module):\n    contexts = load_all_contexts()\n    ctx = contexts.get(module)\n    if not ctx:\n        print(f\"No context found for {module}\")\n        return\n    for k, v in ctx.items():\n        print(f\"{k}: {v}\")\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) == 2 and sys.argv[1] == \"sync\":\n        sync_contexts()\n    elif len(sys.argv) == 3 and sys.argv[1] == \"show\":\n        print_context(sys.argv[2])\n    else:\n        print(\"Usage:\")\n        print(\"  python module_context.py sync         # Update headers and JSON for all modules\")\n        print(\"  python module_context.py show path/to/module.py   # Print context for a module\")\n",
  "tools/collect_code.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\n\"\"\"\ncollect_code.py  –  Export Cadence source files to a single JSON payload.\n\nUsage\n-----\npython tools/collect_code.py \\\n       --root cadence              # package folder(s) to scan (repeatable)\n       --out  code_payload.json   # written JSON (stdout if omitted)\n       --ext .py .md              # file extensions to keep\n       --max-bytes 50000          # skip giant files (>50 kB)\n\nResult\n------\nA JSON dict   { \"relative/path/to/file\": \"UTF-8 text …\", ... }\n\"\"\"\n\nfrom pathlib import Path\nimport argparse\nimport json\nimport sys\n\nDEFAULT_EXT = (\".py\", \".md\", \".cfg\", \".toml\", \".ini\")\n\n\ndef collect(\n    roots: list[Path],\n    files: list[Path] = [],\n    *,\n    extensions: tuple[str, ...] = DEFAULT_EXT,\n    max_bytes: int | None = None,\n) -> dict[str, str]:\n    \"\"\"\n    Walk *roots* and return {relative_path: code_text}.\n    Skips __pycache__, hidden folders, and files larger than *max_bytes*.\n    \"\"\"\n    out: dict[str, str] = {}\n    for root in roots:\n        for path in root.rglob(\"*\"):\n            if (\n                path.is_file()\n                and path.suffix in extensions\n                and \"__pycache__\" not in path.parts\n                and not any(p.startswith(\".\") for p in path.parts)\n            ):\n                if max_bytes and path.stat().st_size > max_bytes:\n                    continue\n                try:\n                    text = path.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[str(path.relative_to(Path.cwd()))] = text\n    for file in files:\n        if (\n            file.is_file()\n            and file.suffix in extensions\n            and file.stat().st_size <= (max_bytes or float(\"inf\"))\n        ):\n            rel = str(file.relative_to(Path.cwd()))\n            if rel not in out:\n                try:\n                    text = file.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = file.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[rel] = text\n    return out\n\n\ndef parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Collect source files into JSON.\")\n    p.add_argument(\n        \"--root\",\n        nargs=\"+\",\n        default=[\"cadence\"],\n        help=\"Directories to scan (repeatable).\",\n    )\n    p.add_argument(\n        \"--ext\",\n        nargs=\"+\",\n        default=DEFAULT_EXT,\n        help=\"File extensions to include (repeatable).\",\n    )\n    p.add_argument(\n        \"--max-bytes\",\n        type=int,\n        default=50000,\n        help=\"Skip files larger than this size (bytes).\",\n    )\n    p.add_argument(\n        \"--out\",\n        type=str,\n        default=\"-\",\n        help=\"Output JSON file path or '-' for stdout.\",\n    )\n    p.add_argument(\n        \"--file\",\n        nargs=\"+\",\n        default=[],\n        help=\"Individual files to include (repeatable).\",\n    )\n    return p.parse_args(argv)\n\n\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = parse_args(argv)\n    payload = collect(\n        [Path(r).resolve() for r in args.root],\n        files=[Path(f).resolve() for f in args.file],\n        extensions=tuple(args.ext),\n        max_bytes=args.max_bytes,\n    )\n    if args.out == \"-\":\n        json.dump(payload, sys.stdout, indent=2, ensure_ascii=False)\n    else:\n        out_path = Path(args.out)\n        out_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False))\n        print(f\"Wrote {len(payload)} files → {out_path}\")\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    main()\n",
  "tools/gen_prompt.py": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\n\"\"\"\ngen_prompt.py  –  Assemble a mega-prompt that contains\n\n  • Ground-truth docs (blueprint, progress logs, etc.)\n  • Full source snapshot (or whatever roots you point at)\n  • A header that gives the LLM an explicit NEXT TASK and ENV context\n\nUsage\n-----\npython tools/gen_prompt.py \\\n       --code-root cadence \\\n       --docs-dir docs \\\n       --task \"Implement FactorRegistry API and unit tests\" \\\n       --env  \"Python 3.11, pandas 2.2, scikit-learn 1.4\" \\\n       --out  prompt.txt\n\"\"\"\n\nfrom pathlib import Path\nimport argparse\nimport sys\nimport textwrap\n\n# --------------------------------------------------------------------------- #\n#  Config\n# --------------------------------------------------------------------------- #\nDEFAULT_CODE_EXT = (\".py\", \".md\", \".toml\", \".ini\", \".cfg\")\n\n\n# --------------------------------------------------------------------------- #\n#  Helpers\n# --------------------------------------------------------------------------- #\ndef _collect_files(\n    roots: list[Path],\n    *,\n    include_ext: tuple[str, ...],\n    max_bytes: int | None = None,\n) -> list[tuple[str, str]]:\n    \"\"\"Return [(relative_path, text), …] for all files matching *include_ext*.\"\"\"\n    records: list[tuple[str, str]] = []\n    cwd = Path.cwd()\n\n    for root in roots:\n        root = Path(root).resolve()\n        if not root.exists():\n            print(f\"WARNING: directory not found → {root}\", file=sys.stderr)\n            continue\n\n        for p in root.rglob(\"*\"):\n            if (\n                p.is_file()\n                and p.suffix in include_ext\n                and \"__pycache__\" not in p.parts\n                and not any(part.startswith(\".\") for part in p.parts)\n            ):\n                if max_bytes and p.stat().st_size > max_bytes:\n                    continue\n                try:\n                    txt = p.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n                records.append((str(p.relative_to(cwd)), txt))\n\n    records.sort()\n    return records\n\n\ndef _build_prompt(\n    docs: list[tuple[str, str]],\n    code: list[tuple[str, str]],\n    *,\n    header: str,\n) -> str:\n    parts: list[str] = [header]\n\n    # -- docs ---------------------------------------------------------------\n    parts.append(\"\\n## 1. Ground-Truth Documents\")\n    if not docs:\n        parts.append(\"\\n_No Markdown / text documents found in docs directory._\")\n    for path, txt in docs:\n        parts.append(f\"\\n### {path}\\n```markdown\\n{txt}\\n```\")\n\n    # -- code ---------------------------------------------------------------\n    parts.append(\"\\n## 2. Source Code Snapshot\")\n    if not code:\n        parts.append(\"\\n_No source files found in code roots._\")\n    for path, txt in code:\n        fence = \"```python\" if path.endswith(\".py\") else \"```text\"\n        parts.append(f\"\\n### {path}\\n{fence}\\n{txt}\\n```\")\n\n    return \"\\n\".join(parts)\n\n\n# --------------------------------------------------------------------------- #\n#  CLI\n# --------------------------------------------------------------------------- #\ndef _parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Generate mega-prompt for LLM.\")\n    p.add_argument(\"--code-root\", nargs=\"+\", default=[\"cadence\"],\n                   help=\"Package directories to scan (repeatable).\")\n    p.add_argument(\"--docs-dir\", default=\"docs\",\n                   help=\"Directory holding NORTH_STAR.md, progress logs, etc.\")\n    p.add_argument(\"--ext\", nargs=\"+\", default=DEFAULT_CODE_EXT,\n                   help=\"File extensions to include from code roots.\")\n    p.add_argument(\"--max-bytes\", type=int, default=100_000,\n                   help=\"Skip individual files larger than this size (bytes).\")\n    p.add_argument(\"--skip-code\", action=\"store_true\",\n               help=\"Omit source snapshot (tasks only).\")\n    p.add_argument(\"--task\", default=\"Tell me the next highest-leverage step and write the code.\",\n                   help=\"Explicit next task instruction injected into header.\")\n    p.add_argument(\"--env\", default=\"Python 3.11, pandas 2.2, scikit-learn 1.4\",\n                   help=\"Runtime environment string added to header.\")\n    p.add_argument(\"--out\", default=\"-\",\n                   help=\"Output file path or '-' for stdout.\")\n    return p.parse_args(argv)\n\n\n# --------------------------------------------------------------------------- #\n#  Main\n# --------------------------------------------------------------------------- #\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = _parse_args(argv)\n\n    docs = _collect_files(\n        [Path(args.docs_dir).resolve()],\n        include_ext=(\".md\", \".txt\"),\n        max_bytes=args.max_bytes,\n    )\n\n    if args.skip_code:\n        code = []\n    else:\n        code = _collect_files(\n            [Path(r).resolve() for r in args.code_root],\n            include_ext=tuple(args.ext),\n            max_bytes=args.max_bytes,\n        )\n\n    header = textwrap.dedent(\n        f\"\"\"\\\n        ### CADENCE STATUS / ALIGNMENT REQUEST  (auto-generated)\n\n        **Task**: {args.task}\n        **Environment**: {args.env}\n\n        You are an expert reviewer. Read ALL content below — docs first, then full\n        code — and report:\n\n          1. Alignment gaps between implementation and blueprint  \n          2. Missing risk / compliance safeguards  \n          3. Highest-leverage next actions  \n\n        Be brutally honest. No cheerleading. Return your analysis **only**.\n\n        ---\n        \"\"\"\n    )\n\n    prompt = _build_prompt(docs, code, header=header)\n\n    if args.out == \"-\":\n        sys.stdout.write(prompt)\n    else:\n        Path(args.out).write_text(prompt, encoding=\"utf-8\")\n        print(f\"Wrote prompt to {args.out}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
}