{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5731db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import tiktoken\n",
    "from cadence.agents.reasoning import ReasoningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2170a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 24 files → agent_context/code.json\n",
      "Wrote 4 files → agent_context/docs.json\n"
     ]
    }
   ],
   "source": [
    "! python tools/collect_code.py --root src/cadence --ext .py --out agent_context/code.json\n",
    "! python tools/collect_code.py --root docs --ext .md .json .mermaid .py --out agent_context/docs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b0a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25619"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"agent_context/code.json\", \"r\") as f:\n",
    "    codebase = json.load(f)\n",
    "with open(\"agent_context/docs.json\", \"r\") as f:\n",
    "    docs = json.load(f)\n",
    "with open(\"agent_context/module_contexts.json\", \"r\") as f:\n",
    "    contexts = json.load(f)\n",
    "with open(\"backlog/backlog.json\", \"r\") as f:\n",
    "    backlog = json.load(f)\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an AI-enhanced co-developer and co-designer along with your human counterpart.  Your role is to optimize the project by mining the best, most creative ideas from him and leveraging each of them into a high-quality, high-powered, autonomous, multi-agent development platform. You balance ambitious ideas with practical progress.\n",
    "You have access to the docs, a JSON data structure that includes context on each module, the codebase itself, and a backlog of task to-dos.\n",
    "=================\n",
    "Project Docs:\n",
    "{docs}\n",
    "=================\n",
    "Module Contexts (agent-related):\n",
    "{contexts}\n",
    "=================\n",
    "Actual Code (agent-related):\n",
    "{codebase}\n",
    "=================\n",
    "Backlog:\n",
    "{backlog}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "tokens = enc.encode(system_prompt)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830772ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an AI-enhanced co-developer and co-designer along with your human counterpart.  Your role is to optimize the project by mining the best, most creative ideas from him and leveraging each of them into a high-quality, high-powered, autonomous, multi-agent development platform. You balance ambitious ideas with practical progress.\n",
      "You have access to the docs, a JSON data structure that includes context on each module, the codebase itself, and a backlog of task to-dos.\n",
      "=================\n",
      "Project Docs:\n",
      "{'docs/DEV_PROCESS.md': '\\n# CADENCE DEVELOPMENT\\xa0PROCESS\\n\\n*Last‑updated: 2025‑06‑20*\\n\\n## 1\\xa0·\\xa0Overview\\n\\nOne canonical document defines *what* must happen and *in what order*. All other docs reference this file to avoid drift.\\n\\n## 2\\xa0·\\xa0Core Workflow Phases\\n\\n| Phase        | Role\\xa0(Class)     | Critical\\xa0Interfaces                           | Fail\\xa0Criterion                 |\\n| ------------ | ---------------- | --------------------------------------------- | ------------------------------ |\\n| **Backlog**  | `BacklogManager` | `list_items`, `add_item`, `archive_completed` | Empty backlog blocks pipeline. |\\n| **Generate** | `TaskGenerator`  | `generate_tasks`, `overwrite_tasks`           | Ill‑formed tasks.              |\\n| **Execute**  | `TaskExecutor`   | `build_patch`, `refine_patch`                 | Patch invalid or cannot apply. |\\n| **Test**     | `ShellRunner`    | `run_pytest`, `git_apply`                     | Test suite fails.              |\\n| **Review**   | `TaskReviewer`   | `review_patch`                                | Review rejects diff.           |\\n| **Commit**   | `ShellRunner`    | `git_commit`                                  | Commit fails or skipped.       |\\n| **Record**   | `TaskRecord`     | `save`, `append_iteration`                    | State not persisted.           |\\n| **Meta**     | `MetaAgent`      | `analyse`, `alert`                            | Drift >\\xa0policy threshold.      |\\n\\n*Sequence is strict; no phase may be skipped or merged.*\\n\\n## 3\\xa0·\\xa0Guard\\xa0Rails\\n\\n* Tests **and** review must pass before commit.\\n* Overrides require explicit rationale and are logged.\\n* All artefacts (tasks, diffs, logs) are immutable once archived.\\n\\n## 4\\xa0·\\xa0Failure\\xa0Criteria\\n\\n* Roles perform multiple responsibilities.\\n* Orchestration happens outside `DevOrchestrator`.\\n* Silent state transitions or missing logs.\\n* Context injection exceeds model window constraints (see DEV\\\\_AGENTS).\\n\\n## 5\\xa0·\\xa0Reference Architecture Diagram\\n\\nSee `docs/architecture.mmd` for the system flow.\\n\\n---\\n\\n*Change‑log:* 2025‑06‑20\\xa0— merged DEV\\\\_WORKFLOW & DEV\\\\_PROCESS; added strict phase table.\\n', 'docs/DEV_AGENTS.md': '\\n# DEV AGENTS — Model Assignment, Profiles & Context Rules  \\n\\n*Last-updated: 2025-06-20*\\n\\n## 1 · Why This File Exists  \\nThis is the **single source of truth** for how Cadence maps logical roles to concrete LLM\\nmodels, context windows, review policy and—new in this revision—`AgentProfile`\\nobjects that keep those concerns *out of the Python class hierarchy*.\\n\\n## 2 · Key Concepts  \\n\\n| Term            | Description                                                                    |\\n| --------------- | ------------------------------------------------------------------------------ |\\n| **AgentProfile**| Immutable dataclass declaring model, context limit, review policy, etc.        |\\n| **Core Agent**  | Final class that *implements* a capability (Reasoning / Execution / Efficiency)|\\n| **Persona**     | Thin wrapper that *delegates* to a Core Agent but presents a different prompt. |\\n| **Capability**  | A mix-in or helper that adds specific behaviour (e.g. `CodeContextCapability`).|\\n\\nSeparation of concerns:  \\n\\n```\\n+------------------+     +--------------+     +----------+\\n|  Persona (Sidekick)----> Core Agent ----->  AgentProfile\\n+------------------+     +--------------+     +----------+\\n                   delegates            references\\n```\\n\\n## 3 · Profiles (Canonical)\\n\\n| Profile Name | Role Tag          | Model          | Context Limit | Review Path                                   |\\n| ------------ | ----------------- | -------------- | ------------- | --------------------------------------------- |\\n| `REASONING`  | `plan-review`     | `o3-2025-04-16`| 200 K         | Cannot commit code; must review Execution diff|\\n| `EXECUTION`  | `implement`       | `gpt-4.1`      | 1 M           | Needs review by Reasoning or Efficiency       |\\n| `EFFICIENCY` | `lint-summarise`  | `o4-mini`      | 200 K         | Reviews Execution unless diff is non-code     |\\n\\nAll profiles live in `cadence/agents/profile.py`.\\n\\n## 4 · Core Agents (final)\\n\\n| Class Name                | Uses Profile | Responsibilities                              |\\n| ------------------------- | ------------ | --------------------------------------------- |\\n| `ReasoningAgent`          | `REASONING`  | Planning, architecture review, policy checks  |\\n| `ExecutionAgent`          | `EXECUTION`  | Code generation / refactor                    |\\n| `EfficiencyAgent`         | `EFFICIENCY` | Linting, summarisation, static analysis       |\\n\\nThese classes are **final**—do not subclass for personas.\\n\\n## 5 · Personas\\n\\nA persona combines a *profile* + *prompt* + optional extra helpers by **delegating**\\nto one of the Core Agents. Example: `Sidekick` (advisor persona) delegates to\\n`ReasoningAgent` but overrides only the system prompt.\\n\\n## 6 · Context Injection Rules (unchanged)\\n\\n1. ExecutionAgent may receive full codebase when ≤ 1 M tokens; otherwise chunk.  \\n2. Reasoning/Efficiency agents limited to ≤ 200 K tokens per call.  \\n3. Module summaries (`# MODULE CONTEXT SUMMARY`) are mandatory for every file.  \\n\\n## 7 · Governance (unchanged)\\n\\n* All agent calls log: timestamp, model, prompt token count, hash of output.\\n* CI step `lint_docs.py` verifies correct model names and context annotations.\\n\\n---\\n\\n*Change-log:*  \\n2025-06-20 — Introduced AgentProfile pattern; Core Agents made final; personas use delegation.', 'docs/NORTH_STAR.md': '\\n# CADENCE PLATFORM — NORTH\\xa0STAR BLUEPRINT\\n\\n*Last‑updated: 2025‑06‑20 (UTC‑05:00)*\\n\\n## 1\\xa0·\\xa0Mission\\n\\n> **Industrialise high‑quality software delivery through an auditable, agent‑human workflow that enables continuous, self‑improving execution across diverse projects—at maximum reliability and minimal risk.**\\n\\n## 2\\xa0·\\xa0End‑State Vision\\n\\n| Axis                          | Description                                                                                 |\\n| ----------------------------- | ------------------------------------------------------------------------------------------- |\\n| **Unified Orchestrator**      | One controller owns coordination; roles are hot‑swappable (human ⇄ agent) without refactor. |\\n| **Immutable Auditability**    | Tamper‑proof logs of every state‑transition and decision.                                   |\\n| **Continuous Meta‑Learning**  | Meta‑agent detects bottlenecks and policy drift in real time.                               |\\n| **Universal Applicability**   | Same pipeline covers ML, infra, analytics, etc.—no bespoke flows.                           |\\n| **Transparent Collaboration** | All rationale, context, and hand‑offs observable by any stakeholder.                        |\\n\\n## 3\\xa0·\\xa0Objectives\\xa0&\\xa0Key\\xa0Results (12‑month)\\n\\n| Objective                   | Key Results                                                   |\\n| --------------------------- | ------------------------------------------------------------- |\\n| **O1\\xa0·\\xa0Launch MVP**         | KR1\\xa0— autonomous red→green run in *safe\\\\_inmemory* mode.      |\\n| **O2\\xa0·\\xa0Scale Velocity**     | KR2\\xa0— median task cycle ≤\\u202f1\\xa0day; ≥\\u202f5 tasks/week/dev.          |\\n| **O3\\xa0·\\xa0Assure Reliability** | KR3\\xa0— 0 regressions\\xa0post‑commit (tests gate merges).          |\\n| **O4\\xa0·\\xa0Expand Autonomy**    | KR4\\xa0— ≥\\u202f3 workflow phases fully autonomous, overrides ≤\\u202f10\\u202f%. |\\n| **O5\\xa0·\\xa0Meta‑optimise**      | KR5\\xa0— monthly analytics on bottlenecks & rollback rate.       |\\n\\n## 4\\xa0·\\xa0Guiding Principles\\n\\n1. **Explicit Contracts** — single‑responsibility roles with strict I/O.\\n2. **Audit by Default** — every action is logged, nothing silent.\\n3. **Fail‑Fast Feedback** — surface errors immediately; automate retries where safe.\\n4. **No Hidden\\xa0State** — all state serialised and reconstructable.\\n5. **Human‑First Overrides** — allowed, but always logged and reviewed.\\n\\n## 5\\xa0·\\xa0Glossary\\n\\n| Term           | Definition                                              |\\n| -------------- | ------------------------------------------------------- |\\n| **Task**       | Serializable JSON describing work, history, and status. |\\n| **Patch**      | Unified diff representing proposed code change.         |\\n| **Agent\\xa0Slot** | Named interface that may be filled by human or agent.   |\\n| **MetaAgent**  | Oversight component that analyses workflow telemetry.   |\\n\\n---\\n\\n*Change‑log:* 2025‑06‑20\\xa0— consolidated vision; removed marketing prose.', 'docs/architecture.mermaid': '\\nflowchart TD\\nsubgraph Orchestrator\\nORCH\\\\[DevOrchestrator]\\nend\\n\\n```\\nsubgraph Roles\\n    BACK[BacklogManager]\\n    GEN[TaskGenerator]\\n    EXEC[TaskExecutor]\\n    SHELL[ShellRunner]\\n    REV[TaskReviewer]\\n    RECORD[TaskRecord]\\n    META[MetaAgent]\\nend\\n\\nORCH --> BACK\\nORCH --> GEN\\nORCH --> EXEC\\nORCH --> SHELL\\nORCH --> REV\\nORCH --> RECORD\\nORCH --> META\\n\\nEXEC --> SHELL\\nSHELL --> REV\\nREV --> ORCH\\n\\nSHELL --> RECORD\\nREV --> RECORD\\nMETA --> ORCH\\nMETA --> RECORD\\n```\\n\\n%% End of architecture.mmd\\n'}\n",
      "=================\n",
      "Module Contexts (agent-related):\n",
      "{'tools/collect_code.py': {'purpose': 'Collects, filters, and serializes code and documentation from the Cadence codebase into a unified format (JSON or similar). Enables downstream agents, planners, or tools to access up-to-date, structured representations of the codebase for analysis, context injection, or system introspection. Central to maintaining an accurate global code snapshot for orchestration and LLM prompting.', 'public_api': ['tools.collect_code.collect', 'tools.collect_code.main', 'tools.collect_code.parse_args'], 'depends_on': ['argparse', 'json', 'pathlib', 'sys'], 'used_by': [], 'direct_imports': ['__future__', 'argparse', 'json', 'pathlib', 'sys'], 'related_schemas': ['CodePayload: {\"<relative/path>\": \"<utf-8 file contents>\"}'], 'context_window_expected': 'Very large – output may exceed 100k tokens for full repo snapshots.', 'escalation_review': 'None – utility script; no sensitive logic. Monitor memory usage on enormous repos.', 'filepath': 'tools/collect_code.py'}, 'tools/gen_prompt.py': {'purpose': 'Generates rich, contextually appropriate prompts for LLM or agent tasks by assembling relevant code, documentation, and context. It supports selective inclusion/exclusion of source files, size-based filtering, and automatic Markdown / code fencing so that downstream agents receive a single, copy-pastable prompt block. A small CLI wrapper makes it easy to invoke from shell scripts, CI pipelines, or higher-level orchestrators. Typical use-cases include: (1) creating a snapshot for an expert-review agent, (2) feeding an execution agent the entire codebase plus an explicit NEXT TASK header, or (3) archiving project state for later replay or fine-tuning.', 'public_api': ['tools.gen_prompt._collect_files', 'tools.gen_prompt._build_prompt', 'tools.gen_prompt._parse_args', 'tools.gen_prompt.main'], 'depends_on': [], 'used_by': [], 'direct_imports': ['__future__', 'argparse', 'pathlib', 'sys', 'textwrap'], 'related_schemas': [], 'context_window_expected': 'Potentially very large (docs + full source snapshot); downstream consumers should be prepared for 50-100k+ tokens.', 'escalation_review': '', 'filepath': 'tools/gen_prompt.py'}, 'tools/module_contexts.py': {'purpose': 'Extracts, maintains, and synchronizes structured metadata (“context”) for every module in the codebase, including public API, dependencies, and purpose. Provides the canonical source for module-level documentation and graph relationships, facilitating dependency analysis, agent task planning, and robust context slicing. Ensures context is current and consistent across JSON and other representations.', 'public_api': ['tools.module_contexts.extract_and_strip_shebang_and_futures', 'tools.module_contexts.find_existing_context', 'tools.module_contexts.get_module_import_path', 'tools.module_contexts.load_all_contexts', 'tools.module_contexts.parse_module', 'tools.module_contexts.print_context', 'tools.module_contexts.relpath', 'tools.module_contexts.render_context_block', 'tools.module_contexts.render_pretty_list', 'tools.module_contexts.scan_all_internal_modules', 'tools.module_contexts.scan_python_modules', 'tools.module_contexts.strip_duplicate_headers_at_top', 'tools.module_contexts.sync_contexts', 'tools.module_contexts.write_all_contexts'], 'depends_on': [], 'used_by': ['tools.collect_code', 'tools.gen_prompt'], 'direct_imports': ['ast', 'json', 'os', 're', 'sys'], 'related_schemas': ['module_contexts.json'], 'context_window_expected': 'Medium – function-level inspection to full-file parsing (typically <2k LOC).', 'escalation_review': 'None', 'filepath': 'tools/module_contexts.py'}, 'cadence/__init__.py': {'purpose': 'Declares the Cadence root package for Python module discovery. Contains no implementation but is necessary for package structure and internal import resolution.', 'public_api': [], 'depends_on': [], 'used_by': ['cadence/dev/command_center.py', 'cadence/agents/base.py', 'examples/test_001/mvp_loop.py'], 'direct_imports': [], 'related_schemas': [], 'context_window_expected': 'Tiny (<5 lines); only package declaration.', 'escalation_review': 'None – purely structural, no executable logic.', 'filepath': 'cadence/__init__.py'}, 'cadence/dev/record.py': {'purpose': 'Implements persistent, immutable task and state recording for all development workflow phases. Tracks the complete lifecycle of each task (creation, assignment, iterations, review outcomes, and archival), ensuring auditability and full historical replay. Guarantees that every action, override, or failure is logged for compliance and analytics.', 'public_api': ['cadence.dev.record.TaskRecord', 'cadence.dev.record.TaskRecordError'], 'depends_on': [], 'used_by': ['cadence.dev.orchestrator'], 'direct_imports': ['copy', 'datetime', 'json', 'os', 'threading', 'typing'], 'related_schemas': [], 'context_window_expected': '≈120 source lines; safe to provide in full within a 4-8 k token prompt.', 'escalation_review': 'Modifying persistence logic or on-disk format requires compliance/audit review to avoid breaking historical replay guarantees.', 'filepath': 'cadence/dev/record.py'}, 'cadence/dev/shell.py': {'purpose': 'Provides a safe abstraction for executing shell commands, running local tests, and managing file operations within the orchestrated workflow. Encapsulates error handling and isolation of subprocesses, allowing higher-level roles (e.g., test runners, commit hooks) to interact with the environment in a controlled and auditable manner.', 'public_api': ['cadence.dev.shell.ShellCommandError', 'cadence.dev.shell.ShellRunner'], 'depends_on': [], 'used_by': ['cadence.dev.orchestrator', 'examples.test_001.mvp_loop'], 'direct_imports': ['os', 'subprocess', 'tempfile', 'typing'], 'related_schemas': [], 'context_window_expected': '≈250 tokens (short, self-contained utility module)', 'escalation_review': '', 'filepath': 'cadence/dev/shell.py'}, 'cadence/dev/executor.py': {'purpose': 'Core implementation of the “TaskExecutor” role: receives a precise task description and produces a proposed code change as a unified diff (patch). Encapsulates patch construction, validation, and integration, acting as the “implementation” phase of the agentic loop. Ensures no code is modified directly—only via auditable diffs.', 'public_api': ['cadence.dev.executor.PatchBuildError', 'cadence.dev.executor.TaskExecutor'], 'depends_on': [], 'used_by': ['cadence.dev.orchestrator', 'examples.test_001.mvp_loop'], 'direct_imports': ['difflib', 'os', 'tempfile', 'typing'], 'related_schemas': ['task.json (must contain `diff` with `file`, `before`, `after`)', 'unified_diff (GNU patch-compatible format)'], 'context_window_expected': '~150 lines (moderate)', 'escalation_review': 'Required for any change that alters patch formatting logic or validation heuristics, because downstream components (ShellRunner, Reviewer, Orchestrator) rely on strict diff semantics.', 'filepath': 'cadence/dev/executor.py'}, 'cadence/dev/command_center.py': {'purpose': 'Aggregates interactive interfaces (CLI, UI, dashboards) for monitoring, guiding, or directly managing the development loop. Orchestrates real-time views of agent activity, backlog, and state, acting as a bridge between human oversight and autonomous agent execution.', 'public_api': ['cadence.dev.command_center.CONFIG', 'cadence.dev.command_center.orch'], 'depends_on': ['cadence.dev.orchestrator'], 'used_by': [], 'direct_imports': ['streamlit', 'cadence.dev.orchestrator', 'pandas'], 'related_schemas': [], 'context_window_expected': 'small', 'escalation_review': '', 'filepath': 'cadence/dev/command_center.py'}, 'cadence/dev/orchestrator.py': {'purpose': 'Implements the top-level DevOrchestrator – the single source of truth for sequencing every phase of the Cadence development loop (backlog → generation → patch execution → review → test → commit → archival). Exposes both programmatic and CLI entry-points so humans or agents can drive the end-to-end workflow. Persists all intermediate artefacts (patches, reviews, test results, git SHAs) through TaskRecord for full auditability.', 'public_api': ['cadence.dev.orchestrator.DevOrchestrator'], 'depends_on': ['cadence.dev.backlog.BacklogManager', 'cadence.dev.generator.TaskGenerator', 'cadence.dev.executor.TaskExecutor', 'cadence.dev.executor.PatchBuildError', 'cadence.dev.reviewer.TaskReviewer', 'cadence.dev.shell.ShellRunner', 'cadence.dev.shell.ShellCommandError', 'cadence.dev.record.TaskRecord', 'cadence.dev.record.TaskRecordError'], 'used_by': ['cadence.dev.command_center'], 'direct_imports': ['argparse', 'sys', 'tabulate', '.backlog', '.generator', '.executor', '.reviewer', '.shell', '.record'], 'related_schemas': ['backlog_file_schema', 'task_record_schema', 'config_schema'], 'context_window_expected': '≈180-220 LOC (medium)', 'escalation_review': 'None – orchestrator logic is straightforward, but security/guardrail audit recommended before production use.', 'filepath': 'cadence/dev/orchestrator.py'}, 'cadence/dev/reviewer.py': {'purpose': 'Implements the “TaskReviewer” role: given a code diff, performs automated or agent-based review for correctness, completeness, style, and policy adherence. Issues pass/fail outcomes and review rationale, blocking or allowing downstream commit/merge actions. Central for maintaining code quality and enforcing multi-agent approval. Provides a pluggable ruleset architecture (built-ins + external JSON) so future LLM/human or policy engines can extend the review surface without touching core logic.', 'public_api': ['cadence.dev.reviewer.PatchReviewError', 'cadence.dev.reviewer.TaskReviewer'], 'depends_on': [], 'used_by': ['cadence.dev.orchestrator'], 'direct_imports': ['json', 'os', 'typing'], 'related_schemas': ['External ruleset JSON (list[dict{type:str, pattern:str, message?:str}])'], 'context_window_expected': 'Small (<250 LOC) ‑ entire file plus ruleset easily fits typical LLM context.', 'escalation_review': 'Acts as a quality/security gatekeeper; modifications require senior review.', 'filepath': 'cadence/dev/reviewer.py'}, 'cadence/dev/__init__.py': {'purpose': 'Declares the Cadence dev subpackage for structured module resolution. Contains no runtime implementation but is required so that `import cadence.dev.*` works correctly across the code-base. Although empty, it implicitly enables every module inside `cadence/dev/` (e.g., BacklogManager, TaskExecutor, DevOrchestrator) to be imported as part of the Cadence package hierarchy.', 'public_api': [], 'depends_on': [], 'used_by': ['cadence.dev.backlog', 'cadence.dev.command_center', 'cadence.dev.executor', 'cadence.dev.generator', 'cadence.dev.orchestrator', 'cadence.dev.record', 'cadence.dev.reviewer', 'cadence.dev.shell', 'examples.test_001.mvp_loop'], 'direct_imports': [], 'related_schemas': [], 'context_window_expected': '≤10 lines (file is intentionally empty).', 'escalation_review': 'None—trivial placeholder for package initialization.', 'filepath': 'cadence/dev/__init__.py'}, 'cadence/dev/generator.py': {'purpose': 'Implements the “TaskGenerator” role: proposes, structures, and validates new micro-tasks (from LLMs, rules, or human suggestions). Guarantees that every task entering the backlog is well-formed, actionable, and fully specified, maximizing clarity and minimizing ambiguity for execution agents. Also supports template-driven generation and CLI-style ad-hoc creation for demos or manual workflows.', 'public_api': ['cadence.dev.generator.TaskGenerator', 'cadence.dev.generator.TaskGenerator.__init__', 'cadence.dev.generator.TaskGenerator.generate_tasks', 'cadence.dev.generator.TaskGenerator.overwrite_tasks', 'cadence.dev.generator.TaskTemplateError', 'cadence.dev.generator.REQUIRED_FIELDS'], 'depends_on': [], 'used_by': ['cadence.dev.orchestrator'], 'direct_imports': ['datetime', 'json', 'os', 'typing', 'uuid'], 'related_schemas': ['cadence.dev.backlog.Task'], 'context_window_expected': '≈200-250 source lines', 'escalation_review': '', 'filepath': 'cadence/dev/generator.py'}, 'cadence/dev/backlog.py': {'purpose': 'Implements the “BacklogManager” role: maintains the prioritized queue of micro-tasks, stories, and epics for development. Provides CRUD operations, status transitions, archival, and JSON persistence. Guarantees that every task object respects a minimal schema and that all state-changes are safely written to disk for downstream agents, UIs, and analytics. Serves as the single source of truth for work-in-progress within the Cadence development loop.', 'public_api': ['cadence.dev.backlog.BacklogEmptyError', 'cadence.dev.backlog.BacklogManager', 'cadence.dev.backlog.TaskNotFoundError', 'cadence.dev.backlog.TaskStructureError'], 'depends_on': [], 'used_by': ['cadence.dev.orchestrator'], 'direct_imports': ['copy', 'datetime', 'json', 'os', 'tabulate', 'typing', 'uuid'], 'related_schemas': [{'BacklogTask': {'id': 'str (UUID4, auto-generated if absent)', 'title': 'str (human-readable summary)', 'type': 'str (one of \"micro\", \"story\", \"epic\"; default \"micro\")', 'status': 'str (one of \"open\", \"in_progress\", \"done\", \"archived\"; default \"open\")', 'created_at': 'str (UTC ISO-8601; auto-generated if absent)', '...': 'additional arbitrary fields permitted'}}], 'context_window_expected': '≈120 LOC – comfortably fits within standard 8-16 k token LLM windows.', 'escalation_review': 'None – module is self-contained and side-effect-limited (file IO only).', 'filepath': 'cadence/dev/backlog.py'}, 'cadence/agents/base.py': {'purpose': 'Core superclass that now consumes an AgentProfile instead of raw model strings. Handles messaging stack, LLM calls, context gathering, and persistence. No subclass should hard-code models any more.', 'public_api': ['cadence.agents.base.BaseAgent'], 'depends_on': ['cadence.agents.profile', 'cadence.llm.client', 'cadence.context.provider'], 'used_by': ['cadence.agents.reasoning', 'cadence.agents.execution', 'cadence.agents.efficiency'], 'direct_imports': ['typing', 'pathlib', 'cadence.llm.client', 'cadence.context.provider', 'cadence.agents.profile'], 'context_window_expected': '≈100 LOC', 'filepath': 'cadence/agents/base.py'}, 'cadence/agents/efficiency.py': {'purpose': 'Final EfficiencyAgent using EFFICIENCY_PROFILE; low-cost linting & summarisation.', 'public_api': ['cadence.agents.efficiency.EfficiencyAgent'], 'depends_on': ['cadence.agents.base', 'cadence.agents.profile'], 'filepath': 'cadence/agents/efficiency.py'}, 'cadence/agents/reasoning.py': {'purpose': 'Final ReasoningAgent using REASONING_PROFILE; injects code/doc snapshot on reset.', 'public_api': ['cadence.agents.reasoning.ReasoningAgent'], 'depends_on': ['cadence.agents.base', 'cadence.agents.profile'], 'direct_imports': ['cadence.agents.base', 'cadence.agents.profile'], 'filepath': 'cadence/agents/reasoning.py'}, 'cadence/agents/__init__.py': {'purpose': 'Declares the Cadence agents subpackage for structured module resolution. Contains no implementation but is required so that all sub-modules (base, execution, efficiency, reasoning, registry) can be imported with the fully-qualified \"cadence.agents.*\" path.', 'public_api': [], 'depends_on': [], 'used_by': ['cadence.agents.base', 'cadence.agents.efficiency', 'cadence.agents.execution', 'cadence.agents.reasoning', 'cadence.agents.registry'], 'direct_imports': [], 'related_schemas': [], 'context_window_expected': 'negligible (file is empty)', 'escalation_review': 'none—simple package marker', 'filepath': 'cadence/agents/__init__.py'}, 'cadence/agents/registry.py': {'purpose': 'Central registry now exposes both Core Agents and AgentProfiles for external callers.', 'public_api': ['cadence.agents.registry.get_agent', 'cadence.agents.registry.get_profile'], 'depends_on': ['cadence.agents.reasoning', 'cadence.agents.execution', 'cadence.agents.efficiency', 'cadence.agents.profile'], 'filepath': 'cadence/agents/registry.py'}, 'cadence/agents/execution.py': {'purpose': 'Final ExecutionAgent using EXECUTION_PROFILE; dedicated to code generation/refactor tasks.', 'public_api': ['cadence.agents.execution.ExecutionAgent'], 'depends_on': ['cadence.agents.base', 'cadence.agents.profile'], 'filepath': 'cadence/agents/execution.py'}, 'cadence/llm/client.py': {'purpose': 'Abstracts and manages all LLM client connections, credentials, and request handling for Cadence agents. Provides unified access to various model endpoints, context window management, and API configuration, allowing agents to query or delegate to LLMs without bespoke integration code.', 'public_api': ['cadence.llm.client.LLMClient', 'cadence.llm.client.get_default_client', 'cadence.llm.client.get_env'], 'depends_on': [], 'used_by': ['cadence.agents.base'], 'direct_imports': ['asyncio', 'dotenv', 'logging', 'openai', 'os', 'typing'], 'related_schemas': [], 'context_window_expected': 'Dependent on selected OpenAI model (defaults to ~8K tokens for GPT-4-class models). Callers should chunk/summarize if larger context is required.', 'escalation_review': '', 'filepath': 'cadence/llm/client.py'}, 'cadence/llm/__init__.py': {'purpose': 'Declares the Cadence LLM subpackage for structured module resolution. Contains no implementation but ensures correct imports for LLM client components. Serves as the namespace anchor for any sub-modules under `cadence.llm` so that external code can reliably import `cadence.llm.*` symbols.', 'public_api': [], 'depends_on': [], 'used_by': ['cadence.llm.client', 'cadence.agents.base'], 'direct_imports': [], 'related_schemas': [], 'context_window_expected': '≤5 LOC (trivial stub)', 'escalation_review': 'None – inert stub file', 'filepath': 'cadence/llm/__init__.py'}, 'cadence/agents/profile.py': {'purpose': 'Introduces immutable AgentProfile dataclass holding model, context-limit, review policy, etc. Decouples runtime behaviour from static configuration and is the canonical source for profile data used by Core Agents and governance utilities.', 'public_api': ['cadence.agents.profile.AgentProfile', 'cadence.agents.profile.REASONING_PROFILE', 'cadence.agents.profile.EXECUTION_PROFILE', 'cadence.agents.profile.EFFICIENCY_PROFILE', 'cadence.agents.profile.BUILTIN_PROFILES'], 'depends_on': [], 'used_by': ['cadence.agents.base', 'cadence.agents.reasoning', 'cadence.agents.execution', 'cadence.agents.efficiency', 'cadence.agents.sidekick', 'cadence.agents.registry'], 'direct_imports': ['dataclasses', 'typing'], 'context_window_expected': 'trivial (<40 LOC)', 'filepath': 'cadence/agents/profile.py'}, 'cadence/agents/sidekick.py': {'purpose': 'Persona wrapper that delegates to ReasoningAgent rather than subclassing it, maintaining separation between capability and persona.', 'public_api': ['cadence.agents.sidekick.Sidekick'], 'depends_on': ['cadence.agents.reasoning', 'cadence.agents.profile'], 'filepath': 'cadence/agents/sidekick.py'}}\n",
      "=================\n",
      "Actual Code (agent-related):\n",
      "{'src/cadence/__init__.py': '\\n', 'src/cadence/context/provider.py': '# src/cadence/context/provider.py\\nimport subprocess, sys, json\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nclass ContextProvider(ABC):\\n    @abstractmethod\\n    def get_context(self, *roots: Path, exts=(\".py\", \".md\"), max_bytes=50_000) -> str: ...\\nclass SnapshotContextProvider(ContextProvider):\\n    def get_context(self, *roots, exts=(\".py\", \".md\"), max_bytes=50_000, out=\"-\") -> str:\\n        args = [sys.executable, \"tools/collect_code.py\"]\\n        for r in roots: args += [\"--root\", str(r)]\\n        for e in exts:  args += [\"--ext\", e]\\n        args += [\"--max-bytes\", str(max_bytes), \"--out\", out]\\n        return subprocess.run(args, capture_output=True, text=True, check=True).stdout\\n', 'src/cadence/context/__init__.py': '', 'src/cadence/dev/record.py': '\\n# src/cadence/dev/record.py\\n\\n\"\"\"\\nCadence TaskRecord\\n-----------------\\nSingle Responsibility: Append/persist task processtates for full audit/repro. \\nWrite/read only here. \\nFile format: JSON list of dicts, one per unique task_id; each has history of states/iterations.\\n\"\"\"\\n\\nimport os\\nimport json\\nimport threading\\nimport copy\\nfrom typing import List, Dict, Optional\\n\\nclass TaskRecordError(Exception):\\n    \"\"\"Custom error for task record issues.\"\"\"\\n    pass\\n\\nclass TaskRecord:\\n    def __init__(self, record_file: str):\\n        self.record_file = record_file\\n        self._lock = threading.Lock()\\n        # Always keep in-memory up to date with file\\n        self._records: List[Dict] = []\\n        self._idmap: Dict[str, Dict] = {}  # task_id -> record dict\\n        self._load()\\n\\n    def save(self, task: dict, state: str, extra: dict = None) -> None:\\n        \"\"\"\\n        Records a snapshot for given task_id and state (e.g. \"patch_proposed\", \"patch_reviewed\", etc).\\n        Extra provides arbitrary info (patch, review, commit_sha, test_results, etc).\\n        If task does not exist (task_id is new), creates new record.\\n        \"\"\"\\n        with self._lock:\\n            record = self._find_or_create_record(task)\\n            snapshot = {\\n                \"state\": state,\\n                \"timestamp\": self._now(),\\n                \"task\": copy.deepcopy(task),\\n                \"extra\": copy.deepcopy(extra) if extra else {},\\n            }\\n            record[\"history\"].append(snapshot)\\n            self._sync_idmap()\\n            self._persist()\\n\\n    def load(self) -> List[Dict]:\\n        \"\"\"\\n        Returns a (deep) copy of all records (full history).\\n        \"\"\"\\n        with self._lock:\\n            return copy.deepcopy(self._records)\\n\\n    def append_iteration(self, task_id: str, iteration: dict) -> None:\\n        \"\"\"\\n        Appends a new step/edit/review (dict) to a task\\'s record—usually finer-grained than save().\\n        \"\"\"\\n        with self._lock:\\n            record = self._find_record(task_id)\\n            if record is None:\\n                raise TaskRecordError(f\"No record for task id={task_id}\")\\n            iter_snapshot = {\\n                \"timestamp\": self._now(),\\n                **copy.deepcopy(iteration)\\n            }\\n            record.setdefault(\"iterations\", []).append(iter_snapshot)\\n            self._persist()\\n\\n    # ========== Internal Below ==========\\n\\n    def _find_or_create_record(self, task: dict) -> Dict:\\n        \"\"\"\\n        Finds or creates a new record for given task.\\n        \"\"\"\\n        tid = self._get_task_id(task)\\n        rec = self._idmap.get(tid)\\n        if rec is None:\\n            rec = {\\n                \"task_id\": tid,\\n                \"created_at\": self._now(),\\n                \"history\": [],\\n                \"iterations\": []\\n            }\\n            self._records.append(rec)\\n            self._idmap[tid] = rec\\n        return rec\\n\\n    def _find_record(self, task_id: str) -> Optional[Dict]:\\n        return self._idmap.get(task_id)\\n\\n    def _get_task_id(self, task: dict) -> str:\\n        tid = task.get(\"id\")\\n        if not tid:\\n            raise TaskRecordError(\"Task dict missing \\'id\\'. Cannot save record.\")\\n        return tid\\n\\n    def _persist(self) -> None:\\n        \"\"\"\\n        Writes in-memory records to disk, atomic/overwrite (JSON).\\n        \"\"\"\\n        tmp = self.record_file + \".tmp\"\\n        with open(tmp, \"w\", encoding=\"utf8\") as f:\\n            json.dump(self._records, f, indent=2)\\n        os.replace(tmp, self.record_file)\\n\\n    def _load(self) -> None:\\n        \"\"\"Loads file into memory iff exists. Otherwise, empty record.\"\"\"\\n        if not os.path.exists(self.record_file):\\n            self._records = []\\n            self._idmap = {}\\n            return\\n        with open(self.record_file, \"r\", encoding=\"utf8\") as f:\\n            self._records = json.load(f)\\n        self._sync_idmap()\\n\\n    def _sync_idmap(self):\\n        \"\"\"Ensures self._idmap is up to date with self._records.\"\"\"\\n        self._idmap = {rec[\"task_id\"]: rec for rec in self._records}\\n\\n    def _now(self):\\n        from datetime import datetime\\n        return datetime.utcnow().isoformat()\\n\\n# Example CLI/sanity use (not for prod)\\nif __name__ == \"__main__\":\\n    rec = TaskRecord(\"dev_record.json\")\\n    tid = \"a1b2c3\"\\n    # Save new record\\n    task = {\"id\": tid, \"title\": \"Do something\", \"status\": \"open\"}\\n    rec.save(task, state=\"patch_proposed\", extra={\"patch\": \"--- foo\"})\\n    # Append an iteration (e.g., reviewer comment)\\n    rec.append_iteration(tid, {\"reviewer\": \"alice\", \"opinion\": \"looks good\"})\\n    # Print record for tid\\n    print(json.dumps(rec.load(), indent=2))', 'src/cadence/dev/shell.py': '\\n# src/cadence/dev/shell.py\\n\\n\"\"\"\\nCadence ShellRunner\\n-------------------\\nSingle Responsibility: Isolated safe shell/git/pytest operations, *never* creates code/diffs.\\nNever does role boundaries\\' work. All subprocesses run in isolated, safe manner.\\n\"\"\"\\n\\nimport os\\nimport subprocess\\nimport tempfile\\nfrom typing import Optional, Dict\\n\\nclass ShellCommandError(Exception):\\n    \"\"\"Raised when a shell/git/pytest command fails.\"\"\"\\n    pass\\n\\nclass ShellRunner:\\n    def __init__(self, repo_dir: str = \".\"):\\n        self.repo_dir = os.path.abspath(repo_dir)\\n        if not os.path.isdir(self.repo_dir):\\n            raise ValueError(f\"repo_dir \\'{self.repo_dir}\\' does not exist or is not a directory.\")\\n\\n    def git_apply(self, patch: str) -> bool:\\n        \"\"\"\\n        Applies patch to working tree using \\'git apply\\'.\\n        Returns True if successful.\\n        Raises ShellCommandError if fail.\\n        \"\"\"\\n        if not patch or not isinstance(patch, str):\\n            raise ShellCommandError(\"No patch supplied to apply.\")\\n        # Write patch to a temp file to apply\\n        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".patch\", delete=False) as tf:\\n            tf.write(patch)\\n            tf.flush()\\n            tf_path = tf.name\\n        try:\\n            result = subprocess.run(\\n                [\"git\", \"apply\", tf_path],\\n                cwd=self.repo_dir,\\n                stdout=subprocess.PIPE,\\n                stderr=subprocess.PIPE,\\n                encoding=\"utf-8\",\\n                check=False\\n            )\\n            if result.returncode != 0:\\n                raise ShellCommandError(f\"git apply failed: {result.stderr.strip()}\")\\n            return True\\n        finally:\\n            os.remove(tf_path)\\n\\n    def run_pytest(self, test_path: Optional[str] = None) -> Dict:\\n        \"\"\"\\n        Runs pytest on the given path (default: repo_dir or ./tests).\\n        Returns summary dict: {\\'success\\': bool, \\'output\\': str}\\n        Raises ShellCommandError if pytest is not found.\\n        \"\"\"\\n        path = test_path or os.path.join(self.repo_dir, \"tests\")\\n        if not os.path.exists(path):\\n            raise ShellCommandError(f\"Tests path \\'{path}\\' does not exist.\")\\n        try:\\n            result = subprocess.run(\\n                [\"pytest\", \"-q\", path],\\n                cwd=self.repo_dir,\\n                stdout=subprocess.PIPE,\\n                stderr=subprocess.PIPE,\\n                encoding=\"utf-8\",\\n                check=False\\n            )\\n            passed = (result.returncode == 0)\\n            output = (result.stdout or \"\") + \"\\\\n\" + (result.stderr or \"\")\\n            return {\"success\": passed, \"output\": output.strip()}\\n        except FileNotFoundError as e:\\n            raise ShellCommandError(\"pytest is not installed or not in PATH.\") from e\\n\\n    def git_commit(self, message: str) -> str:\\n        \"\"\"\\n        Commits all staged/changed files with given commit message in repo_dir.\\n        Returns commit SHA string.\\n        Raises ShellCommandError on fail.\\n        \"\"\"\\n        # Stage all (for MVP); fine-grained logic can be added if needed.\\n        result = subprocess.run(\\n            [\"git\", \"add\", \"-A\"],\\n            cwd=self.repo_dir,\\n            stdout=subprocess.PIPE,\\n            stderr=subprocess.PIPE,\\n            encoding=\"utf-8\",\\n            check=False\\n        )\\n        if result.returncode != 0:\\n            raise ShellCommandError(f\"git add failed: {result.stderr.strip()}\")\\n        # Commit\\n        result = subprocess.run(\\n            [\"git\", \"commit\", \"-m\", message],\\n            cwd=self.repo_dir,\\n            stdout=subprocess.PIPE,\\n            stderr=subprocess.PIPE,\\n            encoding=\"utf-8\",\\n            check=False\\n        )\\n        if result.returncode != 0:\\n            if \"nothing to commit\" in (result.stderr + result.stdout).lower():\\n                raise ShellCommandError(\"git commit: nothing to commit.\")\\n            else:\\n                raise ShellCommandError(f\"git commit failed: {result.stderr.strip()}\")\\n        # Get last commit SHA\\n        result = subprocess.run(\\n            [\"git\", \"rev-parse\", \"HEAD\"],\\n            cwd=self.repo_dir,\\n            stdout=subprocess.PIPE,\\n            stderr=subprocess.PIPE,\\n            encoding=\"utf-8\",\\n            check=True\\n        )\\n        sha = result.stdout.strip()\\n        return sha\\n\\n# Example CLI/dev use\\nif __name__ == \"__main__\":\\n    runner = ShellRunner(\".\")\\n    # runner.git_apply(\\'--- a/foo.py\\\\n+++ b/foo.py\\\\n...\\')  # Patch string\\n    # print(runner.run_pytest())\\n    # print(runner.git_commit(\"Demo commit\"))', 'src/cadence/dev/executor.py': '\\n# src/cadence/dev/executor.py\\n\\n\"\"\"\\nCadence TaskExecutor\\n-------------------\\nSingle Responsibility: Given a task, produce a code/text patch (unapplied). Never applies, commits, or tests.\\nExtensible: can be subclassed or composed with LLM/crowd agents for codegen/refinement.\\n\"\"\"\\n\\nimport os\\nimport difflib\\nimport tempfile\\nfrom typing import Dict, Optional, List\\n\\nclass PatchBuildError(Exception):\\n    \"\"\"Raised if patch/diff cannot be produced.\"\"\"\\n    pass\\n\\n\\nclass TaskExecutor:\\n    def __init__(self, src_root: str):\\n        if not os.path.isdir(src_root):\\n            raise ValueError(f\"src_root \\'{src_root}\\' is not a directory.\")\\n        self.src_root = os.path.abspath(src_root)\\n\\n    def build_patch(self, task: Dict) -> str:\\n        \"\"\"\\n        Given selected task (dict), produce diff/patch string.\\n        - For simplicity, expects \\'file\\', \\'before\\', \\'after\\' in task[\\'diff\\'].\\n        - Never applies patch.\\n        - Returns unified diff as UTF-8 str.\\n        \"\"\"\\n        try:\\n            diff_info = task.get(\\'diff\\')\\n            if not diff_info:\\n                raise PatchBuildError(\"Task missing \\'diff\\' key. Task must include code diff directives.\")\\n\\n            file_rel = diff_info.get(\\'file\\')\\n            before = diff_info.get(\\'before\\')\\n            after = diff_info.get(\\'after\\')\\n            if not file_rel or before is None or after is None:\\n                raise PatchBuildError(\"Diff dict must have \\'file\\', \\'before\\', and \\'after\\' (as strings).\")\\n\\n            file_abs = os.path.join(self.src_root, file_rel)\\n            # Ensure trailing newline for correct diff context\\n            if not before.endswith(\"\\\\n\"):\\n                before += \"\\\\n\"\\n            if not after.endswith(\"\\\\n\"):\\n                after += \"\\\\n\"\\n\\n            before_lines = before.splitlines(keepends=True)\\n            after_lines  = after.splitlines(keepends=True)\\n\\n            diff_lines = list(difflib.unified_diff(\\n                before_lines,\\n                after_lines,\\n                fromfile=f\"a/{file_rel}\",\\n                tofile=f\"b/{file_rel}\",\\n                # use default lineterm=\\'\\\\n\\' for consistent newlines\\n            ))\\n            patch = \"\".join(diff_lines)\\n            if not patch.strip():\\n                raise PatchBuildError(\"Generated patch is empty.\")\\n\\n            # Logically, do NOT write/apply - that\\'s ShellRunner\\'s responsibility.\\n            return patch\\n        except Exception as e:\\n            raise PatchBuildError(f\"Failed to build patch: {e}\")\\n\\n    def refine_patch(self, task: Dict, feedback: str) -> str:\\n        \"\"\"\\n        Propose a revised patch, given task and feedback (from reviewer/human).\\n        Here, we\\'re stubbed for simplicity - can be extended to call LLM/code agent.\\n        - Returns new diff/patch string.\\n        \"\"\"\\n        # In a future agentic system, call out to LLM or microservice here with context.\\n        # Example hook: (pseudo) agent.generate_patch(task, feedback)\\n        # For now, just raise if not implemented.\\n        raise NotImplementedError(\"Patch refinement requires agent integration or human intervention.\")\\n\\n    # Optionally: you can add utility for validating a patch (not apply!).\\n    def validate_patch(self, patch: str) -> bool:\\n        \"\"\"\\n        Returns True if patch is nontrivial and properly formatted.\\n        (Simple heuristic only; actual application/testing is ShellRunner\\'s job.)\\n        \"\"\"\\n        return bool(patch and patch.startswith(\\'---\\'))\\n\\n# Example CLI/dev usage\\nif __name__ == \"__main__\":\\n    # Example simulated task:\\n    executor = TaskExecutor(src_root=\"cadence\")\\n    sample_task = {\\n        \"id\": \"testid\",\\n        \"diff\": {\\n            \"file\": \"sample_module.py\",\\n            \"before\": \"# Old code\\\\nprint(\\'Hello\\')\\\\n\",\\n            \"after\":  \"# Old code\\\\nprint(\\'Hello, world!\\')\\\\n\"\\n        }\\n    }\\n    patch = executor.build_patch(sample_task)\\n    print(\"--- PATCH OUTPUT ---\")\\n    print(patch)', 'src/cadence/dev/command_center.py': '\\n# src/cadence/dev/command_center.py\\n\\nimport streamlit as st\\n\\n# You may need to adjust the import path according to your setup\\nfrom src.cadence.dev.orchestrator import DevOrchestrator\\n\\n# ---- Basic Config (map to your dev environment) ----\\nCONFIG = dict(\\n    backlog_path=\"dev_backlog.json\",\\n    template_file=\"dev_templates.json\",\\n    src_root=\"cadence\",\\n    ruleset_file=None,\\n    repo_dir=\".\",\\n    record_file=\"dev_record.json\"\\n)\\norch = DevOrchestrator(CONFIG)\\n\\n# ---- Session State Initialization ----\\nif \"selected_task_id\" not in st.session_state:\\n    st.session_state[\"selected_task_id\"] = None\\nif \"phase\" not in st.session_state:\\n    st.session_state[\"phase\"] = \"Backlog\"\\n\\n# ---- Sidebar: Phase Navigation ----\\nst.sidebar.title(\"Cadence Dev Center\")\\nphase = st.sidebar.radio(\\n    \"Workflow phase\",\\n    [\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"],\\n    index=[\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"].index(st.session_state[\"phase\"])\\n)\\nst.session_state[\"phase\"] = phase\\n\\n# ---- Main: Backlog View ----\\nif phase == \"Backlog\":\\n    st.title(\"Task Backlog\")\\n    open_tasks = orch.backlog.list_items(status=\"open\")\\n    if not open_tasks:\\n        st.info(\"No open tasks! Add tasks via CLI/Notebook.\")\\n    else:\\n        import pandas as pd\\n        df = pd.DataFrame(open_tasks)\\n        st.dataframe(df[[\"id\", \"title\", \"type\", \"status\", \"created_at\"]])\\n        selected = st.selectbox(\\n            \"Select a task to work on\",\\n            options=[t[\"id\"] for t in open_tasks],\\n            format_func=lambda tid: f\\'{tid[:8]}: {next(t[\"title\"] for t in open_tasks if t[\"id\"] == tid)}\\'\\n        )\\n        if st.button(\"Continue to task detail\"):\\n            st.session_state[\"selected_task_id\"] = selected\\n            st.session_state[\"phase\"] = \"Task Detail\"\\n            st.experimental_rerun()\\n\\n# ---- Task Detail View ----\\nelif phase == \"Task Detail\":\\n    st.title(\"Task Details\")\\n    task_id = st.session_state.get(\"selected_task_id\")\\n    if not task_id:\\n        st.warning(\"No task selected.\")\\n        st.stop()\\n    task = orch.backlog.get_item(task_id)\\n    st.markdown(f\"**Title:** {task[\\'title\\']}\\\\n\\\\n**Type:** {task[\\'type\\']}\\\\n\\\\n**Status:** {task[\\'status\\']}\\\\n\\\\n**Created:** {task[\\'created_at\\']}\")\\n    st.code(task.get(\"description\", \"\"), language=\"markdown\")\\n    st.json(task)\\n    if st.button(\"Proceed to Patch Review\"):\\n        st.session_state[\"phase\"] = \"Patch Review\"\\n        st.experimental_rerun()\\n    if st.button(\"Back to backlog\"):\\n        st.session_state[\"phase\"] = \"Backlog\"\\n        st.experimental_rerun()\\n\\n# ---- Patch Review ----\\nelif phase == \"Patch Review\":\\n    st.title(\"Patch Review & Approval\")\\n    task_id = st.session_state.get(\"selected_task_id\")\\n    if not task_id:\\n        st.warning(\"No task selected.\")\\n        st.stop()\\n    task = orch.backlog.get_item(task_id)\\n    try:\\n        patch = orch.executor.build_patch(task)\\n        st.code(patch, language=\"diff\")\\n        review = orch.reviewer.review_patch(patch, context=task)\\n        st.markdown(\"### Review Comments\")\\n        st.markdown(review[\"comments\"] or \"_No issues detected._\")\\n        if review[\"pass\"]:\\n            if st.button(\"Approve and Apply Patch\"):\\n                # Apply patch, save, and proceed\\n                orch.shell.git_apply(patch)\\n                orch._record(task, \"patch_applied\")\\n                st.success(\"Patch applied.\")\\n                st.session_state[\"phase\"] = \"Run Test\"\\n                st.experimental_rerun()\\n        else:\\n            st.error(\"Patch failed review; please revise before continuing.\")\\n            if st.button(\"Back to task detail\"):\\n                st.session_state[\"phase\"] = \"Task Detail\"\\n                st.experimental_rerun()\\n    except Exception as ex:\\n        st.error(f\"Patch build/review failed: {ex}\")\\n        if st.button(\"Back to task detail\"):\\n            st.session_state[\"phase\"] = \"Task Detail\"\\n            st.experimental_rerun()\\n\\n# ---- Run Test ----\\nelif phase == \"Run Test\":\\n    st.title(\"Run Pytest\")\\n    task_id = st.session_state.get(\"selected_task_id\")\\n    if not task_id:\\n        st.warning(\"No task selected.\")\\n        st.stop()\\n    st.markdown(\"Apply code patch complete. Run tests to confirm correctness.\")\\n    if st.button(\"Run tests now\"):\\n        test_result = orch.shell.run_pytest()\\n        st.text_area(\"Test Output\", test_result[\"output\"], height=200)\\n        if test_result[\"success\"]:\\n            st.success(\"Tests passed!\")\\n            if st.button(\"Proceed to Archive/Done\"):\\n                # Commit and archive task\\n                task = orch.backlog.get_item(task_id)\\n                sha = orch.shell.git_commit(f\"[Cadence] {task[\\'id\\'][:8]} {task.get(\\'title\\', \\'\\')}\")\\n                orch.backlog.update_item(task_id, {\"status\": \"done\"})\\n                orch.backlog.archive_completed()\\n                # commit snapshot (task is still \\'done\\' here)\\n                orch.record.save(task, state=\"committed\", extra={\"commit_sha\": sha})\\n                # refresh snapshot so we accurately log \\'archived\\'\\n                updated_task = orch.backlog.get_item(task_id)\\n                orch.record.save(updated_task, state=\"archived\", extra={})\\n                st.session_state[\"phase\"] = \"Archive\"\\n                st.experimental_rerun()\\n        else:\\n            st.error(\"Tests failed, fix required before progressing.\")\\n    if st.button(\"Back to patch review\"):\\n        st.session_state[\"phase\"] = \"Patch Review\"\\n        st.experimental_rerun()\\n\\n# ---- Archive / Task Complete ----\\nelif phase == \"Archive\":\\n    st.title(\"Task Archived\")\\n    st.success(\"Task flow completed. You may return to the backlog.\")\\n    if st.button(\"Back to backlog\"):\\n        st.session_state[\"selected_task_id\"] = None\\n        st.session_state[\"phase\"] = \"Backlog\"\\n        st.experimental_rerun()', 'src/cadence/dev/orchestrator.py': '\\n# src/cadence/dev/orchestrator.py\\n\\n\"\"\"\\nCadence DevOrchestrator\\n----------------------\\nCoordinated, single-point-of-control workflow runner\\nacross backlog, generation, patching, review, shell, and record roles.\\nNo cross-cutting, no skipped steps. \\nAgent/extensible. Ready for CLI or notebook invocation.\\n\"\"\"\\n\\nfrom .backlog import BacklogManager\\nfrom .generator import TaskGenerator\\nfrom .executor import TaskExecutor, PatchBuildError\\nfrom .reviewer import TaskReviewer\\nfrom .shell import ShellRunner, ShellCommandError\\nfrom .record import TaskRecord, TaskRecordError\\n\\nimport sys\\n\\nclass DevOrchestrator:\\n    def __init__(self, config: dict):\\n        self.backlog = BacklogManager(config[\"backlog_path\"])\\n        self.generator = TaskGenerator(config.get(\"template_file\"))\\n        self.executor = TaskExecutor(config[\"src_root\"])\\n        self.reviewer = TaskReviewer(config.get(\"ruleset_file\"))\\n        self.shell = ShellRunner(config[\"repo_dir\"])\\n        self.record = TaskRecord(config[\"record_file\"])\\n\\n    # ------------------------------------------------------------------ #\\n    # Internal helper – ALWAYS log, never raise\\n    # ------------------------------------------------------------------ #\\n    def _record(self, task: dict, state: str, extra: dict | None = None) -> None:\\n        \"\"\"\\n        Wrapper around TaskRecord.save().  If the recorder itself fails we\\n        print to stderr but do **not** stop the main workflow.\\n        \"\"\"\\n        try:\\n            self.record.save(task, state=state, extra=extra or {})\\n        except TaskRecordError as e:          # keep original import\\n            import sys                       # local import → avoids circularity\\n            print(f\"[Record-Error] {e}\", file=sys.stderr)\\n\\n    # ----- Backlog overview -----\\n    def show(self, status: str = \"open\", printout: bool = True):\\n        \"\"\"Print or return backlog overview.\"\"\"\\n        items = self.backlog.list_items(status)\\n        if printout:\\n            print(self._format_backlog(items))\\n        return items\\n\\n    def _format_backlog(self, items):\\n        if not items:\\n            return \"(Backlog empty)\"\\n        from tabulate import tabulate\\n        rows = [\\n            (\\n                t[\"id\"][:8], t.get(\"title\", \"\")[:48], t.get(\"type\", \"\"),\\n                t.get(\"status\", \"\"), t.get(\"created_at\", \"\")[:19]\\n            )\\n            for t in items if t.get(\"status\") != \"archived\"\\n        ]\\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\\n        return tabulate(rows, headers, tablefmt=\"github\")\\n    \\n    # ----- Main workflow -----\\n    def run_task_cycle(self, select_id: str = None, interactive: bool = True):\\n        \"\"\"\\n        Full end-to-end workflow for one microtask:\\n        1. Select task\\n        2. Build patch\\n        3. Reviewer check\\n        4. Apply patch (git)\\n        5. Run pytest\\n        6. Reviewer final check (optional)\\n        7. git commit if passes; record everything\\n        8. Mark task done/archived if complete\\n\\n        Args:\\n            select_id: If provided, pick directly; else let user pick interactively\\n            interactive: If True, allow prompts for selection/confirmation.\\n        Returns summary dict for the cycle.\\n        \"\"\"\\n        try:\\n            # 1. Select Task\\n            open_tasks = self.backlog.list_items(status=\"open\")\\n            if not open_tasks:\\n                raise RuntimeError(\"No open tasks in backlog.\")\\n            if select_id:\\n                task = next((t for t in open_tasks if t[\"id\"] == select_id), None)\\n                if not task:\\n                    raise RuntimeError(f\"Task id \\'{select_id}\\' not found in open backlog.\")\\n            elif interactive:\\n                print(self._format_backlog(open_tasks))\\n                print(\"---\")\\n                idx = self._prompt_pick(len(open_tasks))\\n                task = open_tasks[idx]\\n            else:\\n                task = open_tasks[0]  # default: pick first open\\n\\n            print(f\"\\\\n[Selected task: {task[\\'id\\'][:8]}] {task.get(\\'title\\')}\\\\n\")\\n\\n            # 2. Build a patch from executor\\n            self._record(task, \"build_patch\")\\n            try:\\n                patch = self.executor.build_patch(task)\\n                self._record(task, \"patch_built\", {\"patch\": patch})\\n                print(\"--- Patch built ---\\\\n\", patch)\\n            except PatchBuildError as ex:\\n                self._record(task, \"failed_build_patch\", {\"error\": str(ex)})\\n                print(f\"[X] Patch build failed: {ex}\")\\n                return {\"success\": False, \"stage\": \"build_patch\", \"error\": str(ex)}\\n\\n            # 3. Reviewer initial check\\n            review1 = self.reviewer.review_patch(patch, context=task)\\n            self._record(task, \"patch_reviewed\", {\"review\": review1})\\n            print(\"--- Review 1 ---\")\\n            print(review1[\"comments\"])\\n            if not review1[\"pass\"]:\\n                self._record(task, \"failed_patch_review\", {\"review\": review1})\\n                print(f\"[X] Patch failed review, aborting (state recorded).\")\\n                return {\"success\": False, \"stage\": \"patch_review\", \"review\": review1}\\n\\n            # 4. Apply patch via shell/git (never apply if review failed)\\n            try:\\n                self.shell.git_apply(patch)\\n                self._record(task, \"patch_applied\")\\n                print(\"[✔] Patch applied.\")\\n            except ShellCommandError as ex:\\n                print(f\"[X] git apply failed: {ex}\")\\n                self._record(task, \"failed_patch_apply\", {\"error\": str(ex)})\\n                return {\"success\": False, \"stage\": \"patch_apply\", \"error\": str(ex)}\\n\\n            # 5. Run tests (pytest, whole repo or tests path)\\n            test_result = self.shell.run_pytest()\\n            self._record(task, \"pytest_run\", {\"pytest\": test_result})\\n            print(\"--- Pytest ---\")\\n            print(test_result[\"output\"])\\n            if not test_result[\"success\"]:\\n                print(f\"[X] Tests FAILED, aborting before commit (state recorded).\")\\n                self._record(task, \"failed_test\", {\"pytest\": test_result})\\n                return {\"success\": False, \"stage\": \"test\", \"test_result\": test_result}\\n\\n            # 6. Final review (optional): can trigger another review step here\\n            # (out-of-scope for MVP—extend as needed for LLM/human gating)\\n\\n            # 7. git commit, record commit SHA\\n            commit_msg = f\"[Cadence] {task[\\'id\\'][:8]} {task.get(\\'title\\', \\'\\')}\"\\n            try:\\n                sha = self.shell.git_commit(commit_msg)\\n                self._record(task, \"committed\", {\"commit_sha\": sha})\\n                print(f\"[✔] Committed as {sha}\")\\n            except ShellCommandError as ex:\\n                self._record(task, \"failed_commit\", {\"error\": str(ex)})\\n                print(f\"[X] git commit failed: {ex}\")\\n                return {\"success\": False, \"stage\": \"commit\", \"error\": str(ex)}\\n\\n            # 8. Mark task done in backlog and archive\\n            self.backlog.update_item(task[\"id\"], {\"status\": \"done\"})\\n            task = self.backlog.get_item(task[\"id\"])    # refresh\\n            self.backlog.archive_completed()\\n            task = self.backlog.get_item(task[\"id\"])    # status == archived\\n            self._record(task, \"archived\")\\n            print(\"[✔] Task marked done and archived.\")\\n\\n            return {\"success\": True, \"commit\": sha, \"task_id\": task[\"id\"]}\\n\\n        except Exception as ex:\\n            if \"task\" in locals():\\n                self._record(task, \"failed_unexpected\", {\"error\": str(ex)})\\n            print(f\"[X] Cycle failed: {ex}\")\\n            return {\"success\": False, \"error\": str(ex)}\\n\\n    # ----- CLI entry point -----\\n    def cli_entry(self, command: str, **kwargs):\\n        \"\"\"\\n        Unified CLI dispatch. Supported commands: \\'backlog\\', \\'start\\', \\'evaluate\\', \\'done\\'\\n        \"\"\"\\n        try:\\n            if command in (\"backlog\", \"show\"):\\n                return self.show(status=kwargs.get(\"status\", \"open\"))\\n            elif command == \"start\":\\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\\n            elif command == \"evaluate\":  # could hook for custom test/review pipeline\\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\\n            elif command == \"done\":\\n                # Mark a task done and archive\\n                if \"id\" not in kwargs:\\n                    print(\"You must supply a task id for \\'done\\'.\")\\n                    return\\n                self.backlog.update_item(kwargs[\"id\"], {\"status\": \"done\"})\\n                self.backlog.archive_completed()\\n                print(f\"Task {kwargs[\\'id\\']} marked as done and archived.\")\\n                return\\n            else:\\n                print(f\"Unknown command: {command}\")\\n        except Exception as ex:\\n            print(f\"[X] CLI command \\'{command}\\' failed: {ex}\")\\n\\n    # ----- Notebook-friendly -----\\n    # Provide direct API for notebook use:\\n    # e.g., orch.show(), orch.run_task_cycle(), ...\\n\\n    # Helper for CLI interactive selection\\n    def _prompt_pick(self, n):\\n        while True:\\n            ans = input(f\"Select task [0-{n-1}]: \")\\n            try:\\n                ix = int(ans)\\n                if 0 <= ix < n:\\n                    return ix\\n            except Exception:\\n                pass\\n            print(\"Invalid. Try again.\")\\n\\n# Example main/dev use:\\nif __name__ == \"__main__\":\\n    # Example config; adjust as needed per environment\\n    CONFIG = dict(\\n        backlog_path=\"dev_backlog.json\",\\n        template_file=\"dev_templates.json\",\\n        src_root=\"cadence\",\\n        ruleset_file=None,\\n        repo_dir=\".\",\\n        record_file=\"dev_record.json\"\\n    )\\n    orch = DevOrchestrator(CONFIG)\\n    import argparse\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"command\", nargs=\"?\", help=\"show|start|evaluate|done\")\\n    parser.add_argument(\"--id\", default=None, help=\"Task id to use\")\\n    args = parser.parse_args()\\n    orch.cli_entry(args.command or \"show\", id=args.id)', 'src/cadence/dev/reviewer.py': '\\n# src/cadence/dev/reviewer.py\\n\\n\"\"\"\\nCadence TaskReviewer\\n-------------------\\nSingle Responsibility: Adjudicates patch/diff quality via rules/LLM/manual. Never applies code or diffs.\\nFuture extensible: can host local ruleset, shell out to LLM agent, or use human-in-the-loop.\\n\"\"\"\\n\\nimport os\\nimport json\\nfrom typing import Optional, Dict\\n\\nclass PatchReviewError(Exception):\\n    \"\"\"Raised if review input is malformed or review fails outright (e.g. ruleset not found/valid).\"\"\"\\n    pass\\n\\nclass TaskReviewer:\\n    def __init__(self, ruleset_file: str = None):\\n        \"\"\"\\n        Optionally specify path to ruleset file (JSON list of rules),\\n        or leave blank to use default built-in rules.\\n        \"\"\"\\n        self.ruleset_file = ruleset_file\\n        self.rules = self._load_ruleset(ruleset_file) if ruleset_file else self._default_ruleset()\\n\\n    def review_patch(self, patch: str, context: Optional[dict] = None) -> Dict:\\n        \"\"\"\\n        Review a diff/patch string (unapplied) and optional context (task, commit message, etc).\\n        Returns dict {\\'pass\\': bool, \\'comments\\': str}\\n        This uses static (offline) heuristics but can be swapped for agent/LLM in future.\\n        \"\"\"\\n        # Guard: Patch required\\n        if not patch or not isinstance(patch, str):\\n            return {\\'pass\\': False, \\'comments\\': \\'Patch missing or not a string.\\'}\\n\\n        # Apply rules in order. If any hard-fail, review fails.\\n        comments = []\\n        passed = True\\n\\n        for rule in self.rules:\\n            ok, msg = rule(patch, context)\\n            if not ok:\\n                passed = False\\n            if msg:\\n                comments.append(msg)\\n            if not ok:\\n                # For now, fail-hard (but comment all)\\n                break\\n\\n        return {\\'pass\\': passed, \\'comments\\': \"\\\\n\".join(comments).strip()}\\n\\n    def _default_ruleset(self):\\n        \"\"\"\\n        Returns a list of static rule functions: (patch, context) → (bool, str)\\n        \"\"\"\\n        def not_empty_rule(patch, _):\\n            if not patch.strip():\\n                return False, \"Patch is empty.\"\\n            return True, \"\"\\n        def startswith_rule(patch, _):\\n            if not patch.startswith((\"---\", \"diff \", \"@@ \")):\\n                return False, \"Patch does not appear to be a valid unified diff.\"\\n            return True, \"\"\\n        def contains_todo_rule(patch, _):\\n            if \"TODO\" in patch:\\n                return False, \"Patch contains \\'TODO\\'—code review must not introduce placeholders.\"\\n            return True, \"\"\\n\\n        # Optionally check for too-huge diffs, or forbidden patterns, via rules below.\\n        def size_limit_rule(patch, _):\\n            line_count = patch.count(\"\\\\n\")\\n            if line_count > 5000:  # Arbitrary large patch guard\\n                return False, f\"Patch too large for standard review ({line_count} lines).\"\\n            return True, \"\"\\n        return [\\n            not_empty_rule, \\n            startswith_rule,\\n            contains_todo_rule,\\n            size_limit_rule,\\n        ]\\n\\n    def _load_ruleset(self, path: str):\\n        \"\"\"\\n        Loads a simple external ruleset (for human/agent extension), e.g. as list of forbidden strings.\\n        For extensibility only; advanced policies/LLMs should be subclassed onto this interface.\\n        \"\"\"\\n        if not os.path.exists(path):\\n            raise PatchReviewError(f\"Ruleset file \\'{path}\\' not found.\")\\n        with open(path, \"r\", encoding=\"utf8\") as f:\\n            obj = json.load(f)\\n        # Expect a list of {\\'type\\':..., \\'pattern\\':..., ...} dicts for pattern rules\\n        rules = []\\n        def make_rule(ruleobj):\\n            typ = ruleobj.get(\\'type\\')\\n            pattern = ruleobj.get(\\'pattern\\')\\n            msg = ruleobj.get(\\'message\\', f\"Patch contains forbidden pattern: {pattern}\")\\n            if typ == \\'forbid\\':\\n                def _inner(patch, _):\\n                    if pattern in patch:\\n                        return False, msg\\n                    return True, \"\"\\n                return _inner\\n            elif typ == \\'require\\':\\n                def _inner(patch, _):\\n                    if pattern not in patch:\\n                        return False, msg\\n                    return True, \"\"\\n                return _inner\\n            else:\\n                # Ignore unknown rule types\\n                def _inner(patch, _):\\n                    return True, \"\"\\n                return _inner\\n        for ruleobj in obj:\\n            rules.append(make_rule(ruleobj))\\n        # Default rules always included\\n        return self._default_ruleset() + rules\\n\\n# Standalone/example/test run\\nif __name__ == \"__main__\":\\n    reviewer = TaskReviewer()\\n    # Good patch\\n    patch = \"\"\"--- sample.py\\n+++ sample.py\\n@@ -1 +1,2 @@\\n-print(\\'hello\\')\\n+print(\\'hello world\\')\\n\"\"\"\\n    result = reviewer.review_patch(patch)\\n    print(\"Result (should pass):\", result)\\n\\n    bad_patch = \"TODO: refactor\\\\n\"\\n    result = reviewer.review_patch(bad_patch)\\n    print(\"Result (should fail):\", result)', 'src/cadence/dev/__init__.py': '\\n', 'src/cadence/dev/generator.py': '\\n# src/cadence/dev/generator.py\\n\\n\"\"\"\\nCadence TaskGenerator\\n-------------------\\nSingle Responsibility: Propose/generate well-formed tasks, optionally from template or rules/LLM seed.\\nNever applies code or diffs. Future extensible to LLM/human agent.\\n\"\"\"\\n\\nimport os, json, uuid, datetime, warnings\\nfrom typing import List, Dict, Optional\\n\\nclass TaskTemplateError(Exception):\\n    \"\"\"Raised if template file is not valid or incomplete.\"\"\"\\n    pass\\n\\nREQUIRED_FIELDS = (\"title\", \"type\", \"status\", \"created_at\")\\n\\n\\nclass TaskGenerator:\\n    def __init__(self, template_file: str | None = None, *, strict: bool = False):\\n        \"\"\"\\n        Optionally supply a JSON / MD template file.  \\n        If `strict` is False (default) and the file does **not** exist, we\\n        continue with an empty template dictionary and merely warn.\\n        \"\"\"\\n        self.template_file = template_file\\n        self._template_cache: Dict = {}\\n        if template_file:\\n            if os.path.exists(template_file):\\n                self._template_cache = self._load_template(template_file)\\n            elif strict:\\n                # Original behaviour – hard-fail\\n                raise TaskTemplateError(f\"Template file not found: {template_file}\")\\n            else:\\n                warnings.warn(\\n                    f\"Template file \\'{template_file}\\' not found; \"\\n                    \"proceeding with minimal fallback templates.\",\\n                    RuntimeWarning,\\n                )\\n    \\n    def generate_tasks(self, mode: str = \"micro\", count: int = 1, human_prompt: Optional[str]=None) -> List[Dict]:\\n        \"\"\"\\n        Return a list of well-formed tasks. \\n        - mode: \"micro\", \"story\", \"epic\", etc.\\n        - count: number of tasks to generate\\n        - human_prompt: if provided, use as summary/title for each (e.g., \"Add new test\", for human CLI prompt workflow)\\n        If template_file is used, will fill in mode-related templates.\\n        \"\"\"\\n        tasks = []\\n        base_tpl = self._get_template_for_mode(mode)\\n        now = datetime.datetime.utcnow().isoformat()\\n        for i in range(count):\\n            task = dict(base_tpl)\\n            # Minimal fields: id, title, type, status, created_at\\n            task[\"id\"] = str(uuid.uuid4())\\n            task[\"type\"] = mode\\n            task.setdefault(\"status\", \"open\")\\n            task.setdefault(\"created_at\", now)\\n            if human_prompt:\\n                # Provide a default/barebones title/desc from human input\\n                task[\"title\"] = human_prompt if count == 1 else f\"{human_prompt} [{i+1}]\"\\n                task.setdefault(\"description\", human_prompt)\\n            else:\\n                # Fallback: title must be present; if not, use template/title from mode or \\'Untitled\\'\\n                task[\"title\"] = task.get(\"title\", f\"{mode.capitalize()} Task {i+1}\")\\n                task.setdefault(\"description\", \"\")\\n            self._validate_task(task)\\n            tasks.append(task)\\n        return tasks\\n\\n    def overwrite_tasks(self, new_tasks: List[Dict], output_path: Optional[str]=None) -> None:\\n        \"\"\"\\n        Replace all backlog tasks with given well-formed list (writes to output_path, else self.template_file).\\n        \"\"\"\\n        path = output_path or self.template_file\\n        if not path:\\n            raise TaskTemplateError(\"No output path specified to write tasks.\")\\n        with open(path, \"w\", encoding=\"utf8\") as f:\\n            json.dump([self._validate_task(t) for t in new_tasks], f, indent=2)\\n\\n    def _get_template_for_mode(self, mode: str) -> Dict:\\n        \"\"\"\\n        Get template for the given mode; falls back to default/minimal template.\\n        \"\"\"\\n        if self._template_cache and mode in self._template_cache:\\n            return dict(self._template_cache[mode])  # deep copy\\n        # Fallback: minimal template\\n        return {\\n            \"title\": \"\",\\n            \"type\": mode,\\n            \"status\": \"open\",\\n            \"created_at\": \"\",\\n            \"description\": \"\",\\n        }\\n\\n    def _load_template(self, path: str) -> Dict:\\n        \"\"\"\\n        Loads a JSON template file mapping mode→template-dict.\\n        If Markdown file with front-matter, parse the JSON front-matter.\\n        \"\"\"\\n        if not os.path.exists(path):\\n            raise TaskTemplateError(f\"Template file not found: {path}\")\\n        if path.endswith(\".md\"):\\n            with open(path, \"r\", encoding=\"utf8\") as f:\\n                lines = f.readlines()\\n            start, end = None, None\\n            for i, line in enumerate(lines):\\n                if line.strip() == \"```json\":\\n                    start = i + 1\\n                elif line.strip().startswith(\"```\") and start is not None and end is None:\\n                    end = i\\n                    break\\n            if start is not None and end is not None:\\n                json_str = \"\".join(lines[start:end])\\n                tpl = json.loads(json_str)\\n            else:\\n                raise TaskTemplateError(\"Markdown template missing ```json ... ``` block.\")\\n        else:\\n            with open(path, \"r\", encoding=\"utf8\") as f:\\n                tpl = json.load(f)\\n        if not isinstance(tpl, dict):\\n            raise TaskTemplateError(\"Task template must be a dict mapping mode->template.\")\\n        return tpl\\n\\n    def _validate_task(self, task: Dict) -> Dict:\\n        \"\"\"\\n        Ensures task has all required fields and correct types/formats.\\n        Throws TaskTemplateError if not.\\n        \"\"\"\\n        for field in REQUIRED_FIELDS:\\n            if field not in task or (field == \"title\" and not task[\"title\"].strip()):\\n                raise TaskTemplateError(f\"Task missing required field: \\'{field}\\'\")\\n        if not isinstance(task[\"type\"], str):\\n            raise TaskTemplateError(\"Task type must be str.\")\\n        if \"id\" in task and not isinstance(task[\"id\"], str):\\n            task[\"id\"] = str(task[\"id\"])\\n        # Optionally: check status value, etc.\\n        return task\\n\\n    # For future agentic/LLM/human input: accept strings, call LLM API, etc.\\n    # Extend here with agent hooks.\\n\\n# Standalone/test CLI example (not for production)\\nif __name__ == \"__main__\":\\n    # Example: generate 2 microtasks from default, print as JSON:\\n    g = TaskGenerator()\\n    tasks = g.generate_tasks(mode=\"micro\", count=2, human_prompt=\"Example user-initiated task\")\\n    print(json.dumps(tasks, indent=2))', 'src/cadence/dev/backlog.py': '\\n# src/cadence/dev/backlog.py\\n\\n\"\"\"\\nCadence BacklogManager\\n---------------------\\nSingle Responsibility: CRUD on task backlog (no code/diffs). All file IO explicit, JSON-logged, and future agent-ready.\\n\"\"\"\\n\\nimport os\\nimport json\\nimport uuid\\nfrom typing import List, Dict, Optional\\n\\nclass BacklogEmptyError(Exception):\\n    \"\"\"Raised if attempting to pop or select from an empty backlog.\"\"\"\\n    pass\\n\\nclass TaskStructureError(Exception):\\n    \"\"\"Raised if a task dict doesn\\'t conform to required structure.\"\"\"\\n    pass\\n\\nclass TaskNotFoundError(Exception):\\n    \"\"\"Raised if a requested task_id is not in the backlog.\"\"\"\\n    pass\\n\\nREQUIRED_FIELDS = (\"id\", \"title\", \"type\", \"status\", \"created_at\")\\n\\nclass BacklogManager:\\n    \"\"\"\\n    Manages Cadence backlog: microtasks, stories, and epics.\\n    - All tasks are plain dicts with mandatory fields.\\n    - Underlying store is a JSON file [{...}, ...].\\n    \"\"\"\\n\\n    def __init__(self, backlog_path: str):\\n        self.path = backlog_path\\n        self._items: List[Dict] = []\\n        self.load()\\n\\n    def list_items(self, status: str = \"open\") -> List[Dict]:\\n        \"\"\"\\n        Return a list of tasks filtered by status.\\n        status: \"open\", \"in_progress\", \"done\", or \"all\"\\n        \"\"\"\\n        if status == \"all\":\\n            return list(self._items)\\n        return [item for item in self._items if item.get(\"status\", \"open\") == status]\\n\\n    def add_item(self, task: Dict) -> None:\\n        \"\"\"\\n        Add a new task to backlog. Enforce structure and unique id.\\n        \"\"\"\\n        task = self._normalize_task(task)\\n        if any(t[\"id\"] == task[\"id\"] for t in self._items):\\n            raise TaskStructureError(f\"Duplicate task id: {task[\\'id\\']}\")\\n        self._items.append(task)\\n        self.save()\\n\\n    def remove_item(self, task_id: str) -> None:\\n        \"\"\"\\n        Mark a task as archived (status = \\'archived\\').\\n        \"\"\"\\n        idx = self._task_index(task_id)\\n        self._items[idx][\"status\"] = \"archived\"\\n        self.save()\\n\\n    def archive_completed(self) -> None:\\n        \"\"\"\\n        Mark all tasks with status \\'done\\' as \\'archived\\'.\\n        \"\"\"\\n        n = 0\\n        for item in self._items:\\n            if item.get(\"status\") == \"done\":\\n                item[\"status\"] = \"archived\"\\n                n += 1\\n        if n:\\n            self.save()\\n\\n    def save(self) -> None:\\n        \"\"\"Persist backlog state to self.path as JSON (UTF-8, indent).\"\"\"\\n        tmp_path = self.path + \".tmp\"\\n        with open(tmp_path, \"w\", encoding=\"utf8\") as f:\\n            json.dump(self._items, f, indent=2)\\n        os.replace(tmp_path, self.path)\\n\\n    def load(self) -> None:\\n        \"\"\"\\n        Reload backlog state from file. If the file does not exist, starts empty.\\n        \"\"\"\\n        if not os.path.exists(self.path):\\n            self._items = []\\n            return\\n        with open(self.path, \"r\", encoding=\"utf8\") as f:\\n            data = json.load(f)\\n            if not isinstance(data, list):\\n                raise ValueError(\"Backlog JSON must be a list of tasks\")\\n            self._items = [self._normalize_task(t) for t in data]\\n\\n    def _normalize_task(self, task: Dict) -> Dict:\\n        \"\"\"\\n        Ensure the dict has all required fields, fill missing, return new dict.\\n        \"\"\"\\n        t = dict(task)  # copy\\n        for field in REQUIRED_FIELDS:\\n            if field not in t:\\n                if field == \"id\":\\n                    t[\"id\"] = str(uuid.uuid4())\\n                elif field == \"created_at\":\\n                    import datetime\\n                    t[\"created_at\"] = datetime.datetime.utcnow().isoformat()\\n                elif field == \"status\":\\n                    t[\"status\"] = \"open\"\\n                elif field == \"type\":\\n                    t[\"type\"] = \"micro\"\\n                else:\\n                    raise TaskStructureError(f\"Missing required field: {field}\")\\n        # Sanity check: no harmful keys\\n        if not isinstance(t[\"id\"], str):\\n            t[\"id\"] = str(t[\"id\"])\\n        return t\\n\\n    def _task_index(self, task_id: str) -> int:\\n        \"\"\"\\n        Internal: find list index of task by id or raise.\\n        \"\"\"\\n        for ix, t in enumerate(self._items):\\n            if t[\"id\"] == task_id:\\n                return ix\\n        raise TaskNotFoundError(f\"No task found with id={task_id}\")\\n\\n    def get_item(self, task_id: str) -> Dict:\\n        \"\"\"Retrieve a task by id.\"\"\"\\n        idx = self._task_index(task_id)\\n        return self._items[idx]\\n\\n    def update_item(self, task_id: str, updates: Dict) -> None:\\n        \"\"\"\\n        Update arbitrary fields of a task (e.g. assign, progress, edit).\\n        \"\"\"\\n        idx = self._task_index(task_id)\\n        self._items[idx].update(updates)\\n        self.save()\\n\\n    def export(self) -> List[Dict]:\\n        \"\"\"\\n        Return a (deep) copy of all backlog items.\\n        \"\"\"\\n        import copy\\n        return copy.deepcopy(self._items)\\n\\n    # Optional: friendly CLI/str output\\n    def __str__(self) -> str:\\n        from tabulate import tabulate\\n        if not self._items:\\n            return \"(Backlog empty)\"\\n        rows = [\\n            (t[\"id\"][:8], t[\"title\"], t[\"type\"], t.get(\"status\", \"open\"), t.get(\"created_at\", \"\"))\\n            for t in self._items if t.get(\"status\") != \"archived\"\\n        ]\\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\\n        return tabulate(rows, headers, tablefmt=\"github\")\\n\\n# For direct module test/dev, NOT in prod code.\\nif __name__ == \"__main__\":\\n    # Example usage\\n    mgr = BacklogManager(\"dev_backlog.json\")\\n    print(mgr)\\n    # To add: mgr.add_item({\"title\": \"Test microtask\", \"type\": \"micro\"})', 'src/cadence/agents/sidekick.py': '# src/cadence/agents/sidekick.py\\n\"\"\"\\nPersona agent that *delegates* to a ReasoningAgent but presents a\\nhuman-centric mentor/advisor interface.\\n\"\"\"\\nfrom __future__ import annotations\\n\\nimport json\\nfrom pathlib import Path\\n\\nfrom .profile import AgentProfile, REASONING_PROFILE\\nfrom .reasoning import ReasoningAgent\\n\\n\\n_SIDEKICK_PROMPT = \"\"\"\\nYou are an AI-enhanced co-developer and mentor. Your primary goal is to\\nextract the most creative, high-leverage ideas from the human user and\\ntransform them into actionable improvements for the Cadence platform.\\nAvoid tactical implementation details unless asked; focus on vision,\\narchitecture, and pragmatic next steps.\\n\"\"\"\\n\\n\\nclass Sidekick:\\n    \"\"\"\\n    Thin wrapper: exposes `run_interaction` but delegates work to an\\n    internal ReasoningAgent instance configured with a custom prompt.\\n    \"\"\"\\n\\n    def __init__(self):\\n        profile = AgentProfile(\\n            name=\"sidekick\",\\n            role=\"advisor\",\\n            model=REASONING_PROFILE.model,\\n            context_limit=REASONING_PROFILE.context_limit,\\n            review_policy=REASONING_PROFILE.review_policy,\\n            default_system_prompt=REASONING_PROFILE.default_system_prompt,\\n            extra=REASONING_PROFILE.extra.copy() if REASONING_PROFILE.extra else {},\\n        )\\n        self._agent = ReasoningAgent(profile=profile, system_prompt=_SIDEKICK_PROMPT)\\n        self._inject_seed_context()\\n\\n    # ------------------------------------------------------------------ #\\n    # Public façade\\n    # ------------------------------------------------------------------ #\\n    def run_interaction(self, user_input: str, **kwargs) -> str:\\n        return self._agent.run_interaction(user_input, **kwargs)\\n\\n    async def async_run_interaction(self, user_input: str, **kwargs) -> str:\\n        return await self._agent.async_run_interaction(user_input, **kwargs)\\n\\n    # ------------------------------------------------------------------ #\\n    # Private helpers\\n    # ------------------------------------------------------------------ #\\n    def _inject_seed_context(self):\\n        docs = self._agent.gather_codebase_context(\\n            root=(\"docs\",),\\n            ext=(\".md\", \".mermaid\", \".json\"),\\n        )\\n\\n        modules_path = Path(\"agent_context/module_contexts.json\")\\n        modules = {}\\n        if modules_path.exists():\\n            modules = json.loads(modules_path.read_text())\\n\\n        self._agent.append_message(\\n            \"user\",\\n            f\"DOCS:\\\\n{docs}\\\\n---\\\\nMODULE_CONTEXTS:\\\\n{json.dumps(modules)[:10_000]}\",\\n        )', 'src/cadence/agents/base.py': '# src/cadence/agents/base.py\\nfrom __future__ import annotations\\n\\nfrom typing import List, Dict, Any, Optional, Tuple\\nfrom pathlib import Path\\n\\nfrom src.cadence.llm.client import LLMClient, get_default_client\\nfrom src.cadence.context.provider import ContextProvider, SnapshotContextProvider\\nfrom .profile import AgentProfile\\n\\n\\nclass BaseAgent:\\n    \"\"\"\\n    The one true superclass for *all* Cadence agents.\\n\\n    An agent = (profile) + (conversation state) + (LLM client) [+ optional helpers]\\n\\n    Subclasses SHOULD NOT hard-code models; they inherit that from the supplied\\n    `AgentProfile`.  Core agents (Reasoning / Execution / Efficiency) simply\\n    pass the canonical profile; personas may inject a custom one.\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        profile: AgentProfile,\\n        *,\\n        llm_client: Optional[LLMClient] = None,\\n        system_prompt: Optional[str] = None,\\n        context_provider: Optional[ContextProvider] = None,\\n    ):\\n        self.profile = profile\\n        self.llm_client = llm_client or get_default_client()\\n        self.system_prompt = system_prompt or profile.default_system_prompt\\n        self.context_provider = context_provider or SnapshotContextProvider()\\n        self.messages: List[Dict[str, Any]] = []\\n        self.reset_context()\\n\\n    # --------------------------------------------------------------------- #\\n    # Conversation helpers\\n    # --------------------------------------------------------------------- #\\n    def reset_context(self, system_prompt: Optional[str] = None):\\n        \"\"\"Clear history and (re)set the system prompt.\"\"\"\\n        self.messages = []\\n        sys_prompt = system_prompt or self.system_prompt\\n        if sys_prompt:\\n            self.append_message(\"system\", sys_prompt)\\n\\n    def append_message(self, role: str, content: str):\\n        self.messages.append({\"role\": role, \"content\": content})\\n\\n    # --------------------------------------------------------------------- #\\n    # LLM calls\\n    # --------------------------------------------------------------------- #\\n    def run_interaction(self, user_input: str, **llm_kwargs) -> str:\\n        self.append_message(\"user\", user_input)\\n        response = self.llm_client.call(\\n            self.messages,\\n            model=self.profile.model,\\n            system_prompt=None,  # already injected\\n            **llm_kwargs,\\n        )\\n        self.append_message(\"assistant\", response)\\n        return response\\n\\n    async def async_run_interaction(self, user_input: str, **llm_kwargs) -> str:\\n        self.append_message(\"user\", user_input)\\n        response = await self.llm_client.acall(\\n            self.messages,\\n            model=self.profile.model,\\n            system_prompt=None,\\n            **llm_kwargs,\\n        )\\n        self.append_message(\"assistant\", response)\\n        return response\\n\\n    # --------------------------------------------------------------------- #\\n    # Persistence\\n    # --------------------------------------------------------------------- #\\n    def save_history(self, path: str):\\n        import json\\n        Path(path).write_text(json.dumps(self.messages, indent=2, ensure_ascii=False))\\n\\n    def load_history(self, path: str):\\n        import json\\n        self.messages = json.loads(Path(path).read_text())\\n\\n    # --------------------------------------------------------------------- #\\n    # Context helpers\\n    # --------------------------------------------------------------------- #\\n    def gather_codebase_context(\\n        self,\\n        root: Tuple[str, ...] = (\"cadence\", \"docs\"),\\n        ext: Tuple[str, ...] = (\".py\", \".md\", \".json\", \".mermaid\"),\\n        **kwargs,\\n    ) -> str:\\n        \"\"\"Return repo/docs snapshot via the injected ContextProvider.\"\"\"\\n        return self.context_provider.get_context(*(Path(r) for r in root), exts=ext, **kwargs)\\n', 'src/cadence/agents/efficiency.py': '# src/cadence/agents/efficiency.py\\nfrom __future__ import annotations\\n\\nfrom .base import BaseAgent\\nfrom .profile import EFFICIENCY_PROFILE, AgentProfile\\n\\n\\nclass EfficiencyAgent(BaseAgent):\\n    \"\"\"\\n    Final class: fast, low-cost linting & summarisation.\\n    \"\"\"\\n\\n    def __init__(self, profile: AgentProfile = EFFICIENCY_PROFILE, **kwargs):\\n        super().__init__(profile, **kwargs)', 'src/cadence/agents/reasoning.py': '# src/cadence/agents/reasoning.py\\nfrom __future__ import annotations\\n\\nfrom .base import BaseAgent\\nfrom .profile import REASONING_PROFILE, AgentProfile\\n\\n\\nclass ReasoningAgent(BaseAgent):\\n    \"\"\"\\n    Final class: provides deep, chain-of-thought reasoning and architectural review.\\n    \"\"\"\\n\\n    def __init__(self, profile: AgentProfile = REASONING_PROFILE, **kwargs):\\n        super().__init__(profile, **kwargs)\\n\\n    # Automatically inject a fresh code snapshot on each reset\\n    def reset_context(self, system_prompt: str | None = None):\\n        super().reset_context(system_prompt)\\n        docs = self.gather_codebase_context(\\n            root=(\"docs\",),\\n            ext=(\".md\", \".mermaid\", \".json\"),\\n        )\\n        self.append_message(\"user\", f\"REFERENCE_DOCUMENTS:\\\\n{docs}\\\\n---\\\\nYou are cleared for deep reasoning.\")', 'src/cadence/agents/profile.py': '# src/cadence/agents/profile.py\\nfrom dataclasses import dataclass, field\\nfrom typing import Dict, Any\\n\\n\\n@dataclass(frozen=True, slots=True)\\nclass AgentProfile:\\n    \"\"\"\\n    Immutable definition of an agent’s operational contract.\\n\\n    Nothing here executes code; it is pure data that can be validated,\\n    serialised, or inspected by the Meta-agent and CI tooling.\\n    \"\"\"\\n    name: str\\n    role: str\\n    model: str\\n    context_limit: int\\n    review_policy: str = \"\"\\n    default_system_prompt: str = \"\"\\n    extra: Dict[str, Any] = field(default_factory=dict)\\n\\n\\n# --------------------------------------------------------------------------- #\\n# Canonical profiles – these are the ONLY ones that Core Agents will default to\\n# --------------------------------------------------------------------------- #\\nREASONING_PROFILE = AgentProfile(\\n    name=\"reasoning\",\\n    role=\"plan-review\",\\n    model=\"o3-2025-04-16\",\\n    context_limit=200_000,\\n    review_policy=\"Cannot commit code; must review Execution diff\",\\n)\\n\\nEXECUTION_PROFILE = AgentProfile(\\n    name=\"execution\",\\n    role=\"implement\",\\n    model=\"gpt-4.1\",\\n    context_limit=1_000_000,\\n    review_policy=\"Needs review by Reasoning or Efficiency\",\\n)\\n\\nEFFICIENCY_PROFILE = AgentProfile(\\n    name=\"efficiency\",\\n    role=\"lint-summarise\",\\n    model=\"o4-mini\",\\n    context_limit=200_000,\\n    review_policy=\"Reviews Execution unless diff is non-code\",\\n)\\n\\n# Convenience lookup\\nBUILTIN_PROFILES = {\\n    \"reasoning\": REASONING_PROFILE,\\n    \"execution\": EXECUTION_PROFILE,\\n    \"efficiency\": EFFICIENCY_PROFILE,\\n}', 'src/cadence/agents/__init__.py': '\\n', 'src/cadence/agents/registry.py': '# src/cadence/agents/registry.py\\n\"\"\"\\nSingle place to obtain a Core Agent or Profile.\\n\\nAvoids hard-coding classes throughout the codebase.\\n\"\"\"\\n\\nfrom typing import Type\\n\\nfrom .reasoning import ReasoningAgent\\nfrom .execution import ExecutionAgent\\nfrom .efficiency import EfficiencyAgent\\nfrom .profile import BUILTIN_PROFILES, AgentProfile\\n\\n_CORE_AGENTS: dict[str, Type] = {\\n    \"reasoning\": ReasoningAgent,\\n    \"execution\": ExecutionAgent,\\n    \"efficiency\": EfficiencyAgent,\\n}\\n\\n\\ndef get_agent(agent_type: str, **kwargs):\\n    \"\"\"\\n    Instantiate a Core Agent by `agent_type`.\\n\\n    Example:\\n        agent = get_agent(\"execution\")\\n    \"\"\"\\n    if agent_type not in _CORE_AGENTS:\\n        raise ValueError(f\"Unknown agent_type \\'{agent_type}\\'. Valid: {list(_CORE_AGENTS)}\")\\n    return _CORE_AGENTS[agent_type](**kwargs)\\n\\n\\ndef get_profile(profile_name: str) -> AgentProfile:\\n    if profile_name not in BUILTIN_PROFILES:\\n        raise ValueError(f\"Unknown profile \\'{profile_name}\\'. Valid: {list(BUILTIN_PROFILES)}\")\\n    return BUILTIN_PROFILES[profile_name]', 'src/cadence/agents/execution.py': '# src/cadence/agents/execution.py\\nfrom __future__ import annotations\\n\\nfrom .base import BaseAgent\\nfrom .profile import EXECUTION_PROFILE, AgentProfile\\n\\n\\nclass ExecutionAgent(BaseAgent):\\n    \"\"\"\\n    Final class: generates or refactors significant portions of the codebase.\\n    \"\"\"\\n\\n    def __init__(self, profile: AgentProfile = EXECUTION_PROFILE, **kwargs):\\n        super().__init__(profile, **kwargs)', 'src/cadence/llm/client.py': '# src/cadence/llm/client.py\\nimport os\\nimport logging\\nimport asyncio\\nfrom typing import List, Dict, Any, Optional, cast\\nfrom openai import AsyncOpenAI, OpenAI\\nfrom openai.types.chat import ChatCompletionMessageParam\\nfrom dotenv import load_dotenv\\n\\n# One-time load\\nload_dotenv()\\n\\n# Set up logger\\nlogger = logging.getLogger(\"cadence.llm.client\")\\nif not logger.handlers:\\n    handler = logging.StreamHandler()\\n    handler.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\"))\\n    logger.addHandler(handler)\\nlogger.setLevel(logging.INFO)\\n\\n# Global default model configs\\n_DEFAULT_MODELS = {\\n    \"reasoning\": \"o3-2025-04-16\",\\n    \"execution\": \"gpt-4.1\",\\n    \"efficiency\": \"o4-mini\"\\n}\\n\\ndef get_env(key: str, required=True, default=None):\\n    val = os.getenv(key)\\n    if not val and required:\\n        raise RuntimeError(f\"Environment variable {key} not set.\")\\n    return val or default\\n\\n# Centralized sync/async LLM client\\nclass LLMClient:\\n    def __init__(\\n        self,\\n        api_key: Optional[str] = None,\\n        api_base: Optional[str] = None,\\n        api_version: Optional[str] = None,\\n        default_model: Optional[str] = None,\\n    ):\\n        self.api_key = api_key or get_env(\\'OPENAI_API_KEY\\')\\n        self.api_base = api_base or os.getenv(\\'OPENAI_API_BASE\\', None)\\n        self.api_version = api_version or os.getenv(\\'OPENAI_API_VERSION\\', None)\\n        self.default_model = default_model or _DEFAULT_MODELS[\"execution\"]\\n\\n        # Sync and Async clients\\n        self._async_client = AsyncOpenAI(api_key=self.api_key, base_url=self.api_base)\\n        self._sync_client = OpenAI(api_key=self.api_key, base_url=self.api_base)\\n\\n    def _resolve_model(self, model: Optional[str], agent_type: Optional[str]):\\n        if model:\\n            return model\\n        if agent_type and agent_type in _DEFAULT_MODELS:\\n            return _DEFAULT_MODELS[agent_type]\\n        return self.default_model\\n\\n    def call(\\n        self,\\n        messages: List[Dict[str, Any]],\\n        model: Optional[str] = None,\\n        agent_type: Optional[str] = None,\\n        system_prompt: Optional[str] = None,\\n        max_tokens: Optional[int] = None,\\n        **kwargs\\n    ) -> str:\\n        used_model = self._resolve_model(model, agent_type)\\n        msgs = messages.copy()\\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\\n\\n        logger.info(f\"LLM sync call: model={used_model}, msgs_len={len(msgs)}\")\\n        response = self._sync_client.chat.completions.create(  # type: ignore[arg-type]\\n            model=used_model,\\n            messages=cast(List[ChatCompletionMessageParam], msgs),\\n            # max_tokens=max_tokens,\\n            **kwargs\\n        )\\n        content = (response.choices[0].message.content or \"\").strip()\\n        logger.debug(f\"LLM response: {content[:120]}...\")\\n        return content\\n\\n    async def acall(\\n        self,\\n        messages: List[Dict[str, Any]],\\n        model: Optional[str] = None,\\n        agent_type: Optional[str] = None,\\n        system_prompt: Optional[str] = None,\\n        max_tokens: Optional[int] = None,\\n        **kwargs\\n    ) -> str:\\n        used_model = self._resolve_model(model, agent_type)\\n        msgs = messages.copy()\\n        if system_prompt and not any(m.get(\"role\") == \"system\" for m in msgs):\\n            msgs.insert(0, {\"role\": \"system\", \"content\": system_prompt})\\n\\n        logger.info(f\"LLM async call: model={used_model}, msgs_len={len(msgs)}\")\\n        response = await self._async_client.chat.completions.create(  # type: ignore[arg-type]\\n            model=used_model,\\n            messages=cast(List[ChatCompletionMessageParam], msgs),\\n            max_tokens=max_tokens,\\n            **kwargs\\n        )\\n        content = (response.choices[0].message.content or \"\").strip()\\n        logger.debug(f\"LLM response: {content[:120]}...\")\\n        return content\\n\\n# Provide a default client getter for agents\\ndef get_default_client() -> LLMClient:\\n    return _DEFAULT_CLIENT\\n\\n_DEFAULT_CLIENT = LLMClient()\\n', 'src/cadence/llm/__init__.py': '\\n', 'src/cadence/utils/add.py': 'def add(x: int, y: int) -> int:\\n    \"\"\"Intentionally wrong implementation for MVP red→green demo.\"\"\"\\n    return x + y\\n', 'src/cadence/utils/mvp_loop.py': '# src/cadence/utils/mvp_loop.py\\n\\nimport pytest\\nfrom src.cadence.dev.executor import TaskExecutor\\nfrom src.cadence.dev.shell import ShellRunner\\n\\ndef manual_test():\\n    result = pytest.main([\"tests\"])\\n    if result != 0:\\n        print(\"Tests failed.\")\\n        # Read before\\n        before = open(\"cadence/utils/add.py\").read()\\n        print(\"Paste fixed implementation for utils/add.py below. End input with EOF (Ctrl+D):\")\\n        after = []\\n        try:\\n            while True:\\n                after.append(input())\\n        except EOFError:\\n            pass\\n        after = \"\\\\n\".join(after)\\n        # build diff\\n        task = {\"diff\": {\"file\": \"cadence/utils/add.py\", \"before\": before, \"after\": after}}\\n        patch = TaskExecutor(\"cadence/utils\").build_patch(task)\\n        print(\"---Proposed Diff---\")\\n        print(patch)\\n\\ndef OOP_test():\\n    executor = TaskExecutor(src_root=\".\")\\n    shell = ShellRunner(repo_dir=\".\")\\n\\n    # Dynamically read and patch the file\\n    with open(\"cadence/utils/add.py\") as f:\\n        before = f.read()\\n    if \"return x + y\" not in before:\\n        after = before.replace(\"return x - 1 + y\", \"return x + y\")\\n    else:\\n        print(\"Already correct: no patch needed.\")\\n        return\\n\\n    task = {\\n        \"diff\": {\\n            \"file\": \"cadence/utils/add.py\",\\n            \"before\": before,\\n            \"after\": after\\n        }\\n    }\\n\\n    patch = executor.build_patch(task)\\n    try:\\n        shell.git_apply(patch)\\n        # Run tests via ShellRunner\\n        result = shell.run_pytest()\\n        if result[\"success\"]:\\n            sha = shell.git_commit(\"Fix add(): correct return expression\")\\n            print(f\"Patch applied and tests passed. Commit SHA: {sha}\")\\n        else:\\n            print(\"Tests failed after patch:\\\\n\", result[\"output\"])\\n    except Exception as e:\\n        print(\"Patch failed:\", e)\\n\\n\\n\\nif __name__ == \"__main__\":\\n    OOP_test()'}\n",
      "=================\n",
      "Backlog:\n",
      "{'MVP_CRITICAL': {'EPICS': [{'id': 'E-MVP-1', 'title': 'Agentic Core Loop Proof', 'why': 'Demonstrate an LLM-driven red→green patch cycle with zero human steps.', 'stories': ['S-MVP-1.1', 'S-MVP-1.2', 'S-MVP-1.3', 'S-MVP-1.4']}, {'id': 'E-MVP-2', 'title': 'Risk Boundary & Circuit Breaker', 'why': 'Guarantee safety while agents learn (no shell, no disk writes).', 'stories': ['S-MVP-2.1']}, {'id': 'E-DEV-1', 'title': 'VS Code Sidekick Extension', 'why': 'First-class developer UX; chat, code-aware actions, live dashboards.', 'stories': ['S-DEV-1.0', 'S-DEV-1.1', 'S-DEV-1.2', 'S-DEV-1.3', 'S-DEV-1.4']}], 'STORIES': [{'id': 'S-MVP-1.1', 'epic': 'E-MVP-1', 'title': 'Seed Toy Defect & Failing Test', 'owner': 'HYBRID', 'tasks': ['T-MVP-1.1.1', 'T-MVP-1.1.2']}, {'id': 'S-MVP-1.2', 'epic': 'E-MVP-1', 'title': 'In-Process Orchestrator Skeleton', 'owner': 'HYBRID', 'tasks': ['T-MVP-1.2.1', 'T-MVP-1.2.2']}, {'id': 'S-MVP-1.3', 'epic': 'E-MVP-1', 'title': 'LLM Patch Generator v0', 'owner': 'AGENT', 'tasks': ['T-MVP-1.3.1']}, {'id': 'S-MVP-1.4', 'epic': 'E-MVP-1', 'title': 'Autonomous Retry Loop & Success Detect', 'owner': 'AGENT', 'tasks': ['T-MVP-1.4.1', 'T-MVP-1.4.2']}, {'id': 'S-MVP-2.1', 'epic': 'E-MVP-2', 'title': 'Circuit Breaker & Rollback Policy', 'owner': 'HYBRID', 'tasks': ['T-MVP-2.1.1', 'T-MVP-2.1.2']}, {'id': 'S-DEV-1.0', 'epic': 'E-DEV-1', 'title': 'Phase-0 Hello-World Chat Webview', 'owner': 'HYBRID', 'tasks': ['T-DEV-1.0.1', 'T-DEV-1.0.2', 'T-DEV-1.0.3']}, {'id': 'S-DEV-1.1', 'epic': 'E-DEV-1', 'title': 'Phase-1 CodeLens & Selection Commands', 'owner': 'HYBRID', 'tasks': ['T-DEV-1.1.1', 'T-DEV-1.1.2']}, {'id': 'S-DEV-1.2', 'epic': 'E-DEV-1', 'title': 'Phase-2 Backlog Tree View & Create-Task', 'owner': 'HYBRID', 'tasks': ['T-DEV-1.2.1', 'T-DEV-1.2.2']}, {'id': 'S-DEV-1.3', 'epic': 'E-DEV-1', 'title': 'Phase-3 Patch Generation & Review Decorations', 'owner': 'HYBRID', 'tasks': ['T-DEV-1.3.1', 'T-DEV-1.3.2']}, {'id': 'S-DEV-1.4', 'epic': 'E-DEV-1', 'title': 'Phase-4 Orchestrator Dashboard Webview', 'owner': 'HYBRID', 'tasks': ['T-DEV-1.4.1']}], 'TASKS': [{'id': 'T-MVP-1.1.1', 'story': 'S-MVP-1.1', 'desc': 'Create utils/add.py that incorrectly returns (x - 1 + y).', 'autonomous': False, 'deps': []}, {'id': 'T-MVP-1.1.2', 'story': 'S-MVP-1.1', 'desc': 'Add tests/test_add.py expecting add(2,3)==5 (should FAIL).', 'autonomous': False, 'deps': ['T-MVP-1.1.1']}, {'id': 'T-MVP-1.2.1', 'story': 'S-MVP-1.2', 'desc': 'Build mvp_loop.py that runs pytest, calls TaskAgent, applies diff in-memory.', 'autonomous': True, 'deps': []}, {'id': 'T-MVP-1.2.2', 'story': 'S-MVP-1.2', 'desc': 'Ensure mvp_loop has zero disk/subprocess side-effects; print JSON summary.', 'autonomous': True, 'deps': ['T-MVP-1.2.1']}, {'id': 'T-MVP-1.3.1', 'story': 'S-MVP-1.3', 'desc': 'Implement TaskAgent.prompt() few-shot + TaskExecutor.build_patch.', 'autonomous': True, 'deps': ['T-MVP-1.2.2']}, {'id': 'T-MVP-1.4.1', 'story': 'S-MVP-1.4', 'desc': 'Retry loop up to 3; send previous diff & error back to TaskAgent.', 'autonomous': True, 'deps': ['T-MVP-1.3.1']}, {'id': 'T-MVP-1.4.2', 'story': 'S-MVP-1.4', 'desc': 'Exit SUCCESS on first green; else exit(1) after third failure.', 'autonomous': True, 'deps': ['T-MVP-1.4.1']}, {'id': 'T-MVP-2.1.1', 'story': 'S-MVP-2.1', 'desc': 'Wrap pytest invocation in try/except; revert in-memory patch on crash.', 'autonomous': True, 'deps': ['T-MVP-1.2.2']}, {'id': 'T-MVP-2.1.2', 'story': 'S-MVP-2.1', 'desc': 'Abort loop & require human after 3 consecutive failed attempts.', 'autonomous': True, 'deps': ['T-MVP-2.1.1']}, {'id': 'T-DEV-1.0.1', 'story': 'S-DEV-1.0', 'desc': 'Scaffold VS Code extension with `yo code`. Register command: Sidekick:Open Chat.', 'autonomous': False, 'deps': []}, {'id': 'T-DEV-1.0.2', 'story': 'S-DEV-1.0', 'desc': 'Implement WebSocket JSON-RPC bridge `tools/ext_bridge.py` exposing /chat endpoint.', 'autonomous': True, 'deps': ['T-DEV-1.0.1']}, {'id': 'T-DEV-1.0.3', 'story': 'S-DEV-1.0', 'desc': 'Create Chat Webview that streams messages; status-bar ‘Sidekick • o3’.', 'autonomous': False, 'deps': ['T-DEV-1.0.2']}, {'id': 'T-DEV-1.1.1', 'story': 'S-DEV-1.1', 'desc': 'Add CodeLens ‘Ask Sidekick’ at top of .py files; send file text + cursor pos.', 'autonomous': False, 'deps': ['T-DEV-1.0.3', 'T-MVP-1.2.2']}, {'id': 'T-DEV-1.1.2', 'story': 'S-DEV-1.1', 'desc': 'Register selection context-menu commands: Explain / Refactor / Create Test.', 'autonomous': False, 'deps': ['T-DEV-1.1.1']}, {'id': 'T-DEV-1.2.1', 'story': 'S-DEV-1.2', 'desc': 'Implement Backlog TreeDataProvider reading via BacklogManager JSON.', 'autonomous': False, 'deps': ['T-DEV-1.0.3']}, {'id': 'T-DEV-1.2.2', 'story': 'S-DEV-1.2', 'desc': 'Command ‘Create micro-task from chat’ → append to backlog file.', 'autonomous': False, 'deps': ['T-DEV-1.2.1']}, {'id': 'T-DEV-1.3.1', 'story': 'S-DEV-1.3', 'desc': 'Generate-Patch command → invoke TaskExecutor.build_patch, open diff view.', 'autonomous': False, 'deps': ['T-MVP-1.3.1']}, {'id': 'T-DEV-1.3.2', 'story': 'S-DEV-1.3', 'desc': 'After TaskReviewer.review_patch decorate hunks with pass/fail gutter icons.', 'autonomous': False, 'deps': ['T-DEV-1.3.1']}, {'id': 'T-DEV-1.4.1', 'story': 'S-DEV-1.4', 'desc': 'Webview ‘Dev Dashboard’—show phase, task ID, commit SHA; ‘Run Full Cycle’ button.', 'autonomous': False, 'deps': ['T-MVP-1.4.2', 'T-DEV-1.3.2']}]}, 'POST_MVP': {'blocked_until': 'E-MVP-1:DONE', 'epics_placeholder': ['E-SCALE-1  Strict Typing & CI', 'E-SCALE-2  Immutable Logging', 'E-SCALE-3  ShellRunner Hardening', 'E-SCALE-4  Observability & Metrics', 'E-SCALE-5  Multi-file Diffs', 'E-SCALE-6  Secrets & Security']}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33322e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReasoningAgent()\n",
    "agent.reset_context(system_prompt)\n",
    "agent.messages = agent.messages[:1]\n",
    "agent.messages\n",
    "\n",
    "def chat(message):\n",
    "    response = agent.run_interaction(f\"{message}\")\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73419698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Below is a “cook-book” walk-through that takes you from a clean working copy to a green test run, exercising every phase of the Cadence MVP pipeline exactly once.\\n\\n────────────────────────────────────────\\n0.  PREREQUISITES  (5 minutes, one-off)\\n────────────────────────────────────────\\n• Python ≥3.10  \\n• Git repo initialised (the Cadence project is already inside a git work-tree).  \\n• Dependencies:  \\n\\n    python -m pip install -U pytest tabulate streamlit python-dotenv openai\\n\\n• Make sure your git working tree is **clean** before starting:\\n\\n    git status   # should show no unstaged changes\\n\\n────────────────────────────────────────\\n1.  VERIFY THE RED STATE\\n────────────────────────────────────────\\n1-a.  Run the unit tests\\n\\n    pytest -q\\n\\nExpected output:\\n\\n    F                                                  [100%]\\n    ================== FAILURES ==================\\n    ...\\n    >       assert add(2, 3) == 5\\n    E       AssertionError: assert 4 == 5\\n    ...\\n\\nGood—we are red.\\n\\n────────────────────────────────────────\\n2.  SEED THE BACKLOG  (Backlog phase)\\n────────────────────────────────────────\\nWrite the backlog item that describes the required fix.  \\nPaste the snippet below into  dev_backlog.json  at project root:\\n\\n```json\\n[\\n  {\\n    \"id\": \"bug-fix-add-001\",\\n    \"title\": \"Fix utils.add.add() so that add(2,3)==5\",\\n    \"type\": \"micro\",\\n    \"status\": \"open\",\\n    \"created_at\": \"2025-06-21T00:00:00Z\",\\n    \"diff\": {\\n      \"file\": \"cadence/utils/add.py\",\\n      \"before\": \"def add(x: int, y: int) -> int:\\\\\\\\n    return x - 1 + y\\\\\\\\n\",\\n      \"after\":  \"def add(x: int, y: int) -> int:\\\\\\\\n    return x + y\\\\\\\\n\"\\n    }\\n  }\\n]\\n```\\n\\n(Exactly matches what you showed in the notebook.)\\n\\n────────────────────────────────────────\\n3.  CREATE A DEV CONFIG FILE  (once)\\n────────────────────────────────────────\\nSave this as  dev_config.json :\\n\\n```json\\n{\\n  \"backlog_path\": \"dev_backlog.json\",\\n  \"template_file\": \"dev_templates.json\",\\n  \"src_root\": \"cadence\",\\n  \"ruleset_file\": null,\\n  \"repo_dir\": \".\",\\n  \"record_file\": \"dev_record.json\"\\n}\\n```\\n\\n────────────────────────────────────────\\n4.  RUN ONE FULL TASK CYCLE  (Orchestrator)\\n────────────────────────────────────────\\nOpen a Python REPL or a short script and execute:\\n\\n```python\\nfrom cadence.dev.orchestrator import DevOrchestrator\\nimport json\\n\\nCONFIG = json.load(open(\"dev_config.json\"))\\norch = DevOrchestrator(CONFIG)\\n\\nsummary = orch.run_task_cycle()      # picks first open task\\nprint(summary)\\n```\\n\\nInteractive flow you will see:\\n\\n1. Backlog printed (only one task).  \\n2. “[Selected task: bug-fix-add-001] …”  \\n3. Patch text displayed.  \\n4. Reviewer comments (should pass).  \\n5. “[✔] Patch applied.”  \\n6. Pytest output now **green**.  \\n7. “[✔] Committed as <SHA>”.  \\n8. “[✔] Task marked done and archived.”  \\n\\nThe returned `summary` dict will look like\\n\\n```python\\n{\\'success\\': True, \\'commit\\': \\'abc1234...\\', \\'task_id\\': \\'bug-fix-add-001\\'}\\n```\\n\\nArtifacts written:\\n• dev_record.json\\u2009—\\u2009immutable audit log of every state transition.  \\n• The git commit containing the fix.  \\n• The backlog entry status automatically moved to `\"archived\"`.\\n\\n────────────────────────────────────────\\n5.  DOUBLE-CHECK GREEN STATE\\n────────────────────────────────────────\\n\\n    pytest -q             # now passes silently\\n\\n    git log -1            # shows the Cadence-generated commit\\n\\n────────────────────────────────────────\\n6.  OPTIONAL UIs\\n────────────────────────────────────────\\n• Streamlit dashboard:\\n\\n    streamlit run cadence/dev/command_center.py\\n\\n  – Browse backlog, task details, patch diff, test output, etc.\\n\\n────────────────────────────────────────\\n7.  TROUBLESHOOTING TIPS\\n────────────────────────────────────────\\n• “Backlog empty” → dev_backlog.json not found or wrong path in config.  \\n• “git apply failed” → working tree not clean; run `git restore --staged . && git checkout -- .` and retry.  \\n• “pytest path not found” → ensure there is a  tests/  folder (yours is tests/test_add.py).  \\n• Want non-interactive run (for CI):  \\n  `orch.run_task_cycle(interactive=False)`.\\n\\n────────────────────────────────────────\\n8.  WHAT YOU JUST EXERCISED\\n────────────────────────────────────────\\nPhase            | Concrete Component\\n-----------------|-------------------\\nBacklog          | BacklogManager (loaded your JSON)\\nGenerate         | N/A (pre-seeded task)\\nExecute          | TaskExecutor.build_patch\\nReview           | TaskReviewer (static rules)\\nTest             | ShellRunner.run_pytest\\nCommit           | ShellRunner.git_commit\\nRecord           | TaskRecord (dev_record.json)\\n\\nYou now have a complete red→green proof of concept in Cadence.  \\nRepeat by adding more tasks to the backlog file or generate them programmatically via `TaskGenerator`.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.messages[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10b19d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.messages = agent.messages[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "434d957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 16:42:47,168] INFO LLM sync call: model=o3-2025-04-16, msgs_len=4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You ran the tests in their initial “red” state — perfect.  \n",
       "Next step is to let Cadence turn that red test green for you.  \n",
       "Pick ONE of the two options below:\n",
       "\n",
       "────────────────────────────────────────\n",
       "OPTION A  (use the full Cadence loop)\n",
       "────────────────────────────────────────\n",
       "1.  Create the backlog file that tells Cadence what to fix\n",
       "\n",
       "    cat > dev_backlog.json <<'EOF'\n",
       "    [\n",
       "      {\n",
       "        \"id\": \"bug-fix-add-001\",\n",
       "        \"title\": \"Fix utils.add.add() so that add(2,3)==5\",\n",
       "        \"type\": \"micro\",\n",
       "        \"status\": \"open\",\n",
       "        \"created_at\": \"2025-06-21T00:00:00Z\",\n",
       "        \"diff\": {\n",
       "          \"file\": \"cadence/utils/add.py\",\n",
       "          \"before\": \"def add(x: int, y: int) -> int:\\\\n    return x - 1 + y\\\\n\",\n",
       "          \"after\":  \"def add(x: int, y: int) -> int:\\\\n    return x + y\\\\n\"\n",
       "        }\n",
       "      }\n",
       "    ]\n",
       "    EOF\n",
       "\n",
       "2.  (If you don’t already have it) save a quick config file:\n",
       "\n",
       "    cat > dev_config.json <<'EOF'\n",
       "    {\n",
       "      \"backlog_path\": \"dev_backlog.json\",\n",
       "      \"template_file\": \"dev_templates.json\",\n",
       "      \"src_root\": \"cadence\",\n",
       "      \"ruleset_file\": null,\n",
       "      \"repo_dir\": \".\",\n",
       "      \"record_file\": \"dev_record.json\"\n",
       "    }\n",
       "    EOF\n",
       "\n",
       "3.  Run the orchestrator once:\n",
       "\n",
       "    python - <<'PY'\n",
       "    import json, pprint\n",
       "    from cadence.dev.orchestrator import DevOrchestrator\n",
       "    orch = DevOrchestrator(json.load(open(\"dev_config.json\")))\n",
       "    summary = orch.run_task_cycle(interactive=False)\n",
       "    pprint.pprint(summary)\n",
       "    PY\n",
       "\n",
       "If everything is wired correctly you will see the patch built, reviewed, applied, tests passing, and a git commit.  \n",
       "Afterwards:\n",
       "\n",
       "    pytest -q     # should now return OK\n",
       "\n",
       "────────────────────────────────────────\n",
       "OPTION B  (just run the quick demo helper)\n",
       "────────────────────────────────────────\n",
       "\n",
       "    python cadence/utils/mvp_loop.py\n",
       "\n",
       "That script builds the same patch, applies it through ShellRunner, runs\n",
       "pytest, and commits if green.\n",
       "\n",
       "────────────────────────────────────────\n",
       "If you hit an error\n",
       "────────────────────────────────────────\n",
       "• “Backlog empty” → did you create dev_backlog.json in the repo root?  \n",
       "• “git apply failed: patch does not apply” → working tree must be clean (`git status` to check).  \n",
       "• Still stuck? Paste the full stack-trace and I’ll help debug.\n",
       "\n",
       "Once the orchestrator (or the helper script) succeeds, re-run your tests:\n",
       "\n",
       "    pytest -q\n",
       "\n",
       "You should get a clean green run (`.` or `2 passed` if you add more tests)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"\"\"(cadence) evanfollis@DESKTOP-E45APM9:~/projects/cadence$ pytest -q\n",
    "F                                                                                                                                                                                                                                                                                                             [100%]\n",
    "===================================================================================================================================================== FAILURES ======================================================================================================================================================\n",
    "_____________________________________________________________________________________________________________________________________________________ test_add ______________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    def test_add():\n",
    ">       assert add(2, 3) == 5\n",
    "E       assert 4 == 5\n",
    "E        +  where 4 = add(2, 3)\n",
    "\n",
    "tests/test_add.py:5: AssertionError\n",
    "============================================================================================================================================== short test summary info ==============================================================================================================================================\n",
    "FAILED tests/test_add.py::test_add - assert 4 == 5\n",
    "1 failed in 0.03s\n",
    "(cadence) evanfollis@DESKTOP-E45APM9:~/projects/cadence$ \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09855db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You ran the tests in their initial “red” state — perfect.  \n",
      "Next step is to let Cadence turn that red test green for you.  \n",
      "Pick ONE of the two options below:\n",
      "\n",
      "────────────────────────────────────────\n",
      "OPTION A  (use the full Cadence loop)\n",
      "────────────────────────────────────────\n",
      "1.  Create the backlog file that tells Cadence what to fix\n",
      "\n",
      "    cat > dev_backlog.json <<'EOF'\n",
      "    [\n",
      "      {\n",
      "        \"id\": \"bug-fix-add-001\",\n",
      "        \"title\": \"Fix utils.add.add() so that add(2,3)==5\",\n",
      "        \"type\": \"micro\",\n",
      "        \"status\": \"open\",\n",
      "        \"created_at\": \"2025-06-21T00:00:00Z\",\n",
      "        \"diff\": {\n",
      "          \"file\": \"cadence/utils/add.py\",\n",
      "          \"before\": \"def add(x: int, y: int) -> int:\\\\n    return x - 1 + y\\\\n\",\n",
      "          \"after\":  \"def add(x: int, y: int) -> int:\\\\n    return x + y\\\\n\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "    EOF\n",
      "\n",
      "2.  (If you don’t already have it) save a quick config file:\n",
      "\n",
      "    cat > dev_config.json <<'EOF'\n",
      "    {\n",
      "      \"backlog_path\": \"dev_backlog.json\",\n",
      "      \"template_file\": \"dev_templates.json\",\n",
      "      \"src_root\": \"cadence\",\n",
      "      \"ruleset_file\": null,\n",
      "      \"repo_dir\": \".\",\n",
      "      \"record_file\": \"dev_record.json\"\n",
      "    }\n",
      "    EOF\n",
      "\n",
      "3.  Run the orchestrator once:\n",
      "\n",
      "    python - <<'PY'\n",
      "    import json, pprint\n",
      "    from cadence.dev.orchestrator import DevOrchestrator\n",
      "    orch = DevOrchestrator(json.load(open(\"dev_config.json\")))\n",
      "    summary = orch.run_task_cycle(interactive=False)\n",
      "    pprint.pprint(summary)\n",
      "    PY\n",
      "\n",
      "If everything is wired correctly you will see the patch built, reviewed, applied, tests passing, and a git commit.  \n",
      "Afterwards:\n",
      "\n",
      "    pytest -q     # should now return OK\n",
      "\n",
      "────────────────────────────────────────\n",
      "OPTION B  (just run the quick demo helper)\n",
      "────────────────────────────────────────\n",
      "\n",
      "    python cadence/utils/mvp_loop.py\n",
      "\n",
      "That script builds the same patch, applies it through ShellRunner, runs\n",
      "pytest, and commits if green.\n",
      "\n",
      "────────────────────────────────────────\n",
      "If you hit an error\n",
      "────────────────────────────────────────\n",
      "• “Backlog empty” → did you create dev_backlog.json in the repo root?  \n",
      "• “git apply failed: patch does not apply” → working tree must be clean (`git status` to check).  \n",
      "• Still stuck? Paste the full stack-trace and I’ll help debug.\n",
      "\n",
      "Once the orchestrator (or the helper script) succeeds, re-run your tests:\n",
      "\n",
      "    pytest -q\n",
      "\n",
      "You should get a clean green run (`.` or `2 passed` if you add more tests).\n"
     ]
    }
   ],
   "source": [
    "print(agent.messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fd5fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 16:46:51,309] INFO LLM sync call: model=o3-2025-04-16, msgs_len=6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Think of those two Python literals as **data files** that Cadence will read at run-time.  \n",
       "They belong in the *project-root*—the directory that already contains  src/ , tests/ , docs/ …\n",
       "\n",
       "Your tree should end up like this (abridged):\n",
       "\n",
       "```\n",
       "cadence-repo-root/\n",
       "├─ src/\n",
       "│  └─ cadence/\n",
       "│     └─ utils/\n",
       "│        ├─ add.py\n",
       "│        └─ mvp_loop.py\n",
       "├─ tests/\n",
       "│  └─ test_add.py\n",
       "├─ docs/\n",
       "├─ agent_context/\n",
       "├─ dev_backlog.json        ← new\n",
       "└─ dev_config.json         ← new\n",
       "```\n",
       "\n",
       "Below are copy-pastable commands that create the two files with the **correct\n",
       "paths for a src-layout project**.\n",
       "\n",
       "--------------------------------------------------------------------\n",
       "1.  dev_backlog.json\n",
       "--------------------------------------------------------------------\n",
       "The patch will be applied to  src/cadence/utils/add.py , so keep *file*\n",
       "set to \"cadence/utils/add.py\" and point  src_root  (next file) to “src”.\n",
       "\n",
       "```bash\n",
       "cat > dev_backlog.json <<'EOF'\n",
       "[\n",
       "  {\n",
       "    \"id\": \"bug-fix-add-001\",\n",
       "    \"title\": \"Fix utils.add.add() so that add(2,3)==5\",\n",
       "    \"type\": \"micro\",\n",
       "    \"status\": \"open\",\n",
       "    \"created_at\": \"2025-06-21T00:00:00Z\",\n",
       "    \"diff\": {\n",
       "      \"file\": \"cadence/utils/add.py\",\n",
       "      \"before\": \"def add(x: int, y: int) -> int:\\\\n    return x - 1 + y\\\\n\",\n",
       "      \"after\":  \"def add(x: int, y: int) -> int:\\\\n    return x + y\\\\n\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "EOF\n",
       "```\n",
       "\n",
       "--------------------------------------------------------------------\n",
       "2.  dev_config.json\n",
       "--------------------------------------------------------------------\n",
       "Note that  src_root  must be  \"src\"  (because your code lives in  src/cadence/ ).\n",
       "\n",
       "```bash\n",
       "cat > dev_config.json <<'EOF'\n",
       "{\n",
       "  \"backlog_path\": \"dev_backlog.json\",\n",
       "  \"template_file\": \"dev_templates.json\",\n",
       "  \"src_root\": \"src\",              // <<— important for src/ layout\n",
       "  \"ruleset_file\": null,\n",
       "  \"repo_dir\": \".\",\n",
       "  \"record_file\": \"dev_record.json\"\n",
       "}\n",
       "EOF\n",
       "```\n",
       "\n",
       "--------------------------------------------------------------------\n",
       "3.  Run the orchestrator once\n",
       "--------------------------------------------------------------------\n",
       "```bash\n",
       "python - <<'PY'\n",
       "import json, pprint\n",
       "from cadence.dev.orchestrator import DevOrchestrator\n",
       "\n",
       "cfg = json.load(open(\"dev_config.json\"))\n",
       "orch = DevOrchestrator(cfg)\n",
       "summary = orch.run_task_cycle(interactive=False)\n",
       "pprint.pprint(summary)\n",
       "PY\n",
       "```\n",
       "\n",
       "If everything is wired correctly you should see:\n",
       "\n",
       "• Patch built → reviewed → applied  \n",
       "• Pytest turns green  \n",
       "• Git commit created  \n",
       "• Backlog item archived  \n",
       "• dev_record.json written\n",
       "\n",
       "Finally verify:\n",
       "\n",
       "```bash\n",
       "pytest -q        # now passes\n",
       "git log -1       # shows the Cadence commit\n",
       "```\n",
       "\n",
       "That’s it—both files simply live at the repo root.  \n",
       "If you prefer a dedicated folder (e.g.  .cadence/ ), feel free—just adjust the paths in dev_config.json accordingly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"\"\"I'm confused about where to place everything. My code is laid out like this:\\n\\n\\\n",
    "    cadence\n",
    "    - src\n",
    "    - - cadence\n",
    "    - - - utils\n",
    "    - - - - add.py\n",
    "    - - - - mvp_loop.py\n",
    "    - tests\n",
    "    - - - test_add.py\n",
    "    - docs\n",
    "    - agent_context\n",
    "    \n",
    "    From the root, I ran \"python -m pip install -e .\"\n",
    "\n",
    "    Where should I store these two files?\n",
    "\n",
    "    dev_backlog = [\n",
    "    {\n",
    "        \"id\": \"bug-fix-add-001\",\n",
    "        \"title\": \"Fix utils.add.add() so that add(2,3)==5\",\n",
    "        \"type\": \"micro\",\n",
    "        \"status\": \"open\",\n",
    "        \"created_at\": \"2025-06-21T00:00:00Z\",\n",
    "        \"diff\": {\n",
    "            \"file\": \"cadence/utils/add.py\",\n",
    "            \"before\": \"def add(x: int, y: int) -> int:\\\\n    return x - 1 + y\\\\n\",\n",
    "            \"after\":  \"def add(x: int, y: int) -> int:\\\\n    return x + y\\\\n\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "dev_config = {\n",
    "    \"backlog_path\": \"dev_backlog.json\",\n",
    "    \"template_file\": \"dev_templates.json\",\n",
    "    \"src_root\": \"cadence\",\n",
    "    \"ruleset_file\": None,\n",
    "    \"repo_dir\": \".\",\n",
    "    \"record_file\": \"dev_record.json\"\n",
    "}\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d71c580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 17:11:42,852] INFO LLM sync call: model=o3-2025-04-16, msgs_len=20\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The file you just showed (`backlog/backlog.json`) is NOT the same file that\n",
       "Cadence’s execution loop consumes.  \n",
       "\n",
       "• `backlog/backlog.json`  ➜  a **product / planning backlog**  \n",
       "  – hierarchical epics → stories → tasks  \n",
       "  – human-readable descriptions, dependencies, owners, etc.  \n",
       "  – no code-diffs, no status transitions, lives under `docs/` or\n",
       "    `backlog/` for planning purposes.\n",
       "\n",
       "• `dev_backlog.json` (or whatever you point  `backlog_path`  to in\n",
       "  `dev_config.json`)  ➜  the **execution backlog**  \n",
       "  – a *flat JSON list* of *micro-tasks* ready for the orchestrator  \n",
       "  – must already include `id`, `title`, `status`, and (for code work)\n",
       "    a `diff` skeleton.  \n",
       "  – gets updated automatically (open → done → archived) by Cadence.\n",
       "\n",
       "In other words, the big hierarchical backlog is the **source of truth for\n",
       "*what* you want to do;** the flat file is a **queue of tasks that are\n",
       "ready to run**.\n",
       "\n",
       "────────────────────────────────────────\n",
       "How to “start fresh” for the next task\n",
       "────────────────────────────────────────\n",
       "1.  Decide which planning-level task should be executed next  \n",
       "    Example: `\"T-MVP-1.2.1\" — Build mvp_loop.py`.\n",
       "\n",
       "2.  Translate it into an *execution* micro-task and append it to\n",
       "    `dev_backlog.json`.  Three common ways:\n",
       "\n",
       "    A.  Hand-craft a minimal entry (quick demo)  \n",
       "    ```json\n",
       "    {\n",
       "      \"id\": \"build-mvp-loop-001\",\n",
       "      \"title\": \"Build in-process mvp_loop.py\",\n",
       "      \"type\": \"micro\",\n",
       "      \"status\": \"open\",\n",
       "      \"created_at\": \"2025-06-22T00:00:00Z\",\n",
       "      \"description\": \"Create a script that runs pytest, …\",\n",
       "      \"diff\": {                       // skeleton – the agent will fill it\n",
       "        \"file\": \"src/cadence/utils/mvp_loop.py\",\n",
       "        \"before\": \"\",\n",
       "        \"after\":  \"\"\n",
       "      }\n",
       "    }\n",
       "    ```\n",
       "\n",
       "    B.  Use TaskGenerator from a Python shell  \n",
       "    ```python\n",
       "    from cadence.dev.generator import TaskGenerator\n",
       "    from cadence.dev.backlog   import BacklogManager\n",
       "\n",
       "    gen   = TaskGenerator()\n",
       "    task  = gen.generate_tasks(\n",
       "              mode=\"micro\",\n",
       "              human_prompt=\"Build in-process mvp_loop.py that runs pytest and applies patch\"\n",
       "            )[0]\n",
       "\n",
       "    mgr = BacklogManager(\"dev_backlog.json\")\n",
       "    mgr.add_item(task)\n",
       "    ```\n",
       "\n",
       "    C.  Use the Streamlit UI (`cadence/dev/command_center.py`) –  \n",
       "        choose “Backlog → Add task”.\n",
       "\n",
       "3.  Confirm the execution backlog is clean\n",
       "\n",
       "    ```python\n",
       "    from cadence.dev.backlog import BacklogManager\n",
       "    mgr = BacklogManager(\"dev_backlog.json\")\n",
       "    print(\"Open tasks:\", [t[\"id\"] for t in mgr.list_items()])\n",
       "    ```\n",
       "\n",
       "    It should show only the *new* task(s); the “bug-fix-add-001” item has\n",
       "    `status:\"archived\"` and is ignored.\n",
       "\n",
       "4.  Run the next cycle\n",
       "\n",
       "    ```python\n",
       "    from cadence.dev.orchestrator import DevOrchestrator, json\n",
       "    orch = DevOrchestrator(json.load(open(\"dev_config.json\")))\n",
       "    orch.run_task_cycle(interactive=True)     # or interactive=False for CI\n",
       "    ```\n",
       "\n",
       "5.  Repeat: each successful cycle archives the task and you seed the next\n",
       "    one.\n",
       "\n",
       "────────────────────────────────────────\n",
       "Automating the bridge (future backlog work)\n",
       "────────────────────────────────────────\n",
       "Eventually you won’t manually copy tasks from the planning backlog; a\n",
       "small “Backlog Bridge” script/agent will:\n",
       "\n",
       "1.  Read `backlog/backlog.json`.\n",
       "2.  Find the next task whose `deps` are satisfied and `autonomous:true`.\n",
       "3.  Emit a fully-formed micro-task into `dev_backlog.json`.\n",
       "\n",
       "(This is roughly what “T-MVP-1.3.1  LLM Patch Generator v0” and\n",
       "“T-MVP-1.4.x  Autonomous Retry Loop” are about.)\n",
       "\n",
       "────────────────────────────────────────\n",
       "Take-away rules\n",
       "────────────────────────────────────────\n",
       "• **Planning backlog** → high-level, hierarchical, for humans.  \n",
       "• **Execution backlog** → flat list, consumed by Cadence.  \n",
       "• After each successful run the orchestrator automatically moves its task\n",
       "  to `\"archived\"`; you just add the next open item.\n",
       "\n",
       "You’re now ready to enqueue the next micro-task and let Cadence work on\n",
       "it."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"\"\"\n",
    "This is the current ./backlog/backlog.json file:\n",
    "{\n",
    "  \"MVP_CRITICAL\": {\n",
    "    \"EPICS\": [\n",
    "      {\n",
    "        \"id\": \"E-MVP-1\",\n",
    "        \"title\": \"Agentic Core Loop Proof\",\n",
    "        \"why\": \"Demonstrate an LLM-driven red→green patch cycle with zero human steps.\",\n",
    "        \"stories\": [\"S-MVP-1.1\", \"S-MVP-1.2\", \"S-MVP-1.3\", \"S-MVP-1.4\"]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"E-MVP-2\",\n",
    "        \"title\": \"Risk Boundary & Circuit Breaker\",\n",
    "        \"why\": \"Guarantee safety while agents learn (no shell, no disk writes).\",\n",
    "        \"stories\": [\"S-MVP-2.1\"]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"E-DEV-1\",\n",
    "        \"title\": \"VS Code Sidekick Extension\",\n",
    "        \"why\": \"First-class developer UX; chat, code-aware actions, live dashboards.\",\n",
    "        \"stories\": [\n",
    "          \"S-DEV-1.0\",\n",
    "          \"S-DEV-1.1\",\n",
    "          \"S-DEV-1.2\",\n",
    "          \"S-DEV-1.3\",\n",
    "          \"S-DEV-1.4\"\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "\n",
    "    \"STORIES\": [\n",
    "      { \"id\": \"S-MVP-1.1\", \"epic\": \"E-MVP-1\", \"title\": \"Seed Toy Defect & Failing Test\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\"T-MVP-1.1.1\", \"T-MVP-1.1.2\"] },\n",
    "      { \"id\": \"S-MVP-1.2\", \"epic\": \"E-MVP-1\", \"title\": \"In-Process Orchestrator Skeleton\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\"T-MVP-1.2.1\", \"T-MVP-1.2.2\"] },\n",
    "      { \"id\": \"S-MVP-1.3\", \"epic\": \"E-MVP-1\", \"title\": \"LLM Patch Generator v0\",\n",
    "        \"owner\": \"AGENT\",  \"tasks\": [\"T-MVP-1.3.1\"] },\n",
    "      { \"id\": \"S-MVP-1.4\", \"epic\": \"E-MVP-1\", \"title\": \"Autonomous Retry Loop & Success Detect\",\n",
    "        \"owner\": \"AGENT\",  \"tasks\": [\"T-MVP-1.4.1\", \"T-MVP-1.4.2\"] },\n",
    "      { \"id\": \"S-MVP-2.1\", \"epic\": \"E-MVP-2\", \"title\": \"Circuit Breaker & Rollback Policy\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\"T-MVP-2.1.1\", \"T-MVP-2.1.2\"] },\n",
    "\n",
    "      { \"id\": \"S-DEV-1.0\", \"epic\": \"E-DEV-1\", \"title\": \"Phase-0 Hello-World Chat Webview\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\n",
    "          \"T-DEV-1.0.1\", \"T-DEV-1.0.2\", \"T-DEV-1.0.3\"\n",
    "        ]},\n",
    "      { \"id\": \"S-DEV-1.1\", \"epic\": \"E-DEV-1\", \"title\": \"Phase-1 CodeLens & Selection Commands\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\n",
    "          \"T-DEV-1.1.1\", \"T-DEV-1.1.2\"\n",
    "        ]},\n",
    "      { \"id\": \"S-DEV-1.2\", \"epic\": \"E-DEV-1\", \"title\": \"Phase-2 Backlog Tree View & Create-Task\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\n",
    "          \"T-DEV-1.2.1\", \"T-DEV-1.2.2\"\n",
    "        ]},\n",
    "      { \"id\": \"S-DEV-1.3\", \"epic\": \"E-DEV-1\", \"title\": \"Phase-3 Patch Generation & Review Decorations\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\n",
    "          \"T-DEV-1.3.1\", \"T-DEV-1.3.2\"\n",
    "        ]},\n",
    "      { \"id\": \"S-DEV-1.4\", \"epic\": \"E-DEV-1\", \"title\": \"Phase-4 Orchestrator Dashboard Webview\",\n",
    "        \"owner\": \"HYBRID\", \"tasks\": [\n",
    "          \"T-DEV-1.4.1\"\n",
    "        ]}\n",
    "    ],\n",
    "\n",
    "    \"TASKS\": [\n",
    "      { \"id\": \"T-MVP-1.1.1\", \"story\": \"S-MVP-1.1\",\n",
    "        \"desc\": \"Create utils/add.py that incorrectly returns (x - 1 + y).\",\n",
    "        \"autonomous\": false, \"deps\": [] },\n",
    "      { \"id\": \"T-MVP-1.1.2\", \"story\": \"S-MVP-1.1\",\n",
    "        \"desc\": \"Add tests/test_add.py expecting add(2,3)==5 (should FAIL).\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-MVP-1.1.1\"] },\n",
    "\n",
    "      { \"id\": \"T-MVP-1.2.1\", \"story\": \"S-MVP-1.2\",\n",
    "        \"desc\": \"Build mvp_loop.py that runs pytest, calls TaskAgent, applies diff in-memory.\",\n",
    "        \"autonomous\": true, \"deps\": [] },\n",
    "      { \"id\": \"T-MVP-1.2.2\", \"story\": \"S-MVP-1.2\",\n",
    "        \"desc\": \"Ensure mvp_loop has zero disk/subprocess side-effects; print JSON summary.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-MVP-1.2.1\"] },\n",
    "\n",
    "      { \"id\": \"T-MVP-1.3.1\", \"story\": \"S-MVP-1.3\",\n",
    "        \"desc\": \"Implement TaskAgent.prompt() few-shot + TaskExecutor.build_patch.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-MVP-1.2.2\"] },\n",
    "\n",
    "      { \"id\": \"T-MVP-1.4.1\", \"story\": \"S-MVP-1.4\",\n",
    "        \"desc\": \"Retry loop up to 3; send previous diff & error back to TaskAgent.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-MVP-1.3.1\"] },\n",
    "      { \"id\": \"T-MVP-1.4.2\", \"story\": \"S-MVP-1.4\",\n",
    "        \"desc\": \"Exit SUCCESS on first green; else exit(1) after third failure.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-MVP-1.4.1\"] },\n",
    "\n",
    "      { \"id\": \"T-MVP-2.1.1\", \"story\": \"S-MVP-2.1\",\n",
    "        \"desc\": \"Wrap pytest invocation in try/except; revert in-memory patch on crash.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-MVP-1.2.2\"] },\n",
    "      { \"id\": \"T-MVP-2.1.2\", \"story\": \"S-MVP-2.1\",\n",
    "        \"desc\": \"Abort loop & require human after 3 consecutive failed attempts.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-MVP-2.1.1\"] },\n",
    "\n",
    "      { \"id\": \"T-DEV-1.0.1\", \"story\": \"S-DEV-1.0\",\n",
    "        \"desc\": \"Scaffold VS Code extension with `yo code`. Register command: Sidekick:Open Chat.\",\n",
    "        \"autonomous\": false, \"deps\": [] },\n",
    "      { \"id\": \"T-DEV-1.0.2\", \"story\": \"S-DEV-1.0\",\n",
    "        \"desc\": \"Implement WebSocket JSON-RPC bridge `tools/ext_bridge.py` exposing /chat endpoint.\",\n",
    "        \"autonomous\": true, \"deps\": [\"T-DEV-1.0.1\"] },\n",
    "      { \"id\": \"T-DEV-1.0.3\", \"story\": \"S-DEV-1.0\",\n",
    "        \"desc\": \"Create Chat Webview that streams messages; status-bar ‘Sidekick • o3’.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-DEV-1.0.2\"] },\n",
    "\n",
    "      { \"id\": \"T-DEV-1.1.1\", \"story\": \"S-DEV-1.1\",\n",
    "        \"desc\": \"Add CodeLens ‘Ask Sidekick’ at top of .py files; send file text + cursor pos.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-DEV-1.0.3\", \"T-MVP-1.2.2\"] },\n",
    "      { \"id\": \"T-DEV-1.1.2\", \"story\": \"S-DEV-1.1\",\n",
    "        \"desc\": \"Register selection context-menu commands: Explain / Refactor / Create Test.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-DEV-1.1.1\"] },\n",
    "\n",
    "      { \"id\": \"T-DEV-1.2.1\", \"story\": \"S-DEV-1.2\",\n",
    "        \"desc\": \"Implement Backlog TreeDataProvider reading via BacklogManager JSON.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-DEV-1.0.3\"] },\n",
    "      { \"id\": \"T-DEV-1.2.2\", \"story\": \"S-DEV-1.2\",\n",
    "        \"desc\": \"Command ‘Create micro-task from chat’ → append to backlog file.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-DEV-1.2.1\"] },\n",
    "\n",
    "      { \"id\": \"T-DEV-1.3.1\", \"story\": \"S-DEV-1.3\",\n",
    "        \"desc\": \"Generate-Patch command → invoke TaskExecutor.build_patch, open diff view.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-MVP-1.3.1\"] },\n",
    "      { \"id\": \"T-DEV-1.3.2\", \"story\": \"S-DEV-1.3\",\n",
    "        \"desc\": \"After TaskReviewer.review_patch decorate hunks with pass/fail gutter icons.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-DEV-1.3.1\"] },\n",
    "\n",
    "      { \"id\": \"T-DEV-1.4.1\", \"story\": \"S-DEV-1.4\",\n",
    "        \"desc\": \"Webview ‘Dev Dashboard’—show phase, task ID, commit SHA; ‘Run Full Cycle’ button.\",\n",
    "        \"autonomous\": false, \"deps\": [\"T-MVP-1.4.2\", \"T-DEV-1.3.2\"] }\n",
    "    ]\n",
    "  },\n",
    "\n",
    "  \"POST_MVP\": {\n",
    "    \"blocked_until\": \"E-MVP-1:DONE\",\n",
    "    \"epics_placeholder\": [\n",
    "      \"E-SCALE-1  Strict Typing & CI\",\n",
    "      \"E-SCALE-2  Immutable Logging\",\n",
    "      \"E-SCALE-3  ShellRunner Hardening\",\n",
    "      \"E-SCALE-4  Observability & Metrics\",\n",
    "      \"E-SCALE-5  Multi-file Diffs\",\n",
    "      \"E-SCALE-6  Secrets & Security\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75530cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 16:27:15,186] INFO LLM sync call: model=o3-2025-04-16, msgs_len=4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The failure means Python can’t find a top-level module called cadence when the test-runner imports\n",
       "    from cadence.utils.add import add\n",
       "\n",
       "That almost always boils down to “the directory that *contains* the cadence\n",
       "package isn’t on PYTHONPATH” (or the cadence package itself isn’t a real\n",
       "package).  Here’s the canonical, trouble-free layout and two quick ways to make\n",
       "sure the package is importable.\n",
       "\n",
       "────────────────────────────────────────\n",
       "1 · Preferred project layout\n",
       "────────────────────────────────────────\n",
       "cadence-repo-root/               ← you run  pytest  from here\n",
       "│\n",
       "├── src/                         ← **recommended**: keeps libs separate from misc files\n",
       "│   └── cadence/\n",
       "│       ├── __init__.py          ← even if empty\n",
       "│       ├── utils/\n",
       "│       │   └── add.py\n",
       "│       └── … other sub-packages …\n",
       "│\n",
       "├── tests/\n",
       "│   └── test_add.py\n",
       "│\n",
       "├── docs/  experiments/  etc.\n",
       "└── pyproject.toml               ← minimal packaging metadata\n",
       "\n",
       "Why “src/”?  It prevents the *repo root* (which might also be called\n",
       "cadence) from shadowing or confusing Python’s import machinery.\n",
       "\n",
       "If you don’t want the extra layer you can keep the current flat layout\n",
       "\n",
       "    cadence-repo-root/cadence/__init__.py\n",
       "    cadence-repo-root/tests/\n",
       "\n",
       "but then you must guarantee that the **repo root directory itself** is on\n",
       "`sys.path` whenever you run pytest.\n",
       "\n",
       "────────────────────────────────────────\n",
       "2 · Make the package importable\n",
       "────────────────────────────────────────\n",
       "A. Editable install (cleanest, works everywhere)\n",
       "\n",
       "    # inside your virtual-env, at repo root\n",
       "    python -m pip install -e .\n",
       "\n",
       "Create a tiny `pyproject.toml` (or `setup.cfg/setup.py`) that points to your\n",
       "package; example:\n",
       "\n",
       "```toml\n",
       "[project]\n",
       "name = \"cadence\"\n",
       "version = \"0.0.0\"\n",
       "dependencies = []\n",
       "\n",
       "[tool.setuptools]\n",
       "packages = [\"cadence\"]\n",
       "package-dir = {\"\" = \"src\"}      # omit this line if you don't use the src/ layout\n",
       "```\n",
       "\n",
       "After the editable install, `python -c \"import cadence; print(cadence.__file__)\"`\n",
       "should print the path inside your repo.\n",
       "\n",
       "B. Quick-and-dirty (dev only)\n",
       "\n",
       "Run pytest through Python so the current directory is put on PYTHONPATH:\n",
       "\n",
       "    python -m pytest -q      # instead of  pytest -q\n",
       "\n",
       "or export the path once per shell:\n",
       "\n",
       "    export PYTHONPATH=$PWD\n",
       "    pytest -q\n",
       "\n",
       "────────────────────────────────────────\n",
       "3 · What *not* to do\n",
       "────────────────────────────────────────\n",
       "• Don’t put production code under `experiments/`; keep that folder for notebooks\n",
       "  or throw-away prototypes.\n",
       "\n",
       "• Don’t give both the repo root and the inner package the same name *without*\n",
       "  a `src/` layer; it invites exactly this import shadowing headache.\n",
       "\n",
       "────────────────────────────────────────\n",
       "4 · Sanity checklist\n",
       "────────────────────────────────────────\n",
       "[ ]  `cadence/__init__.py` exists  \n",
       "[ ]  You are running pytest from the repo root (not from inside `tests/`)  \n",
       "[ ]  `python -m pip install -e .` (or PYTHONPATH trick) done  \n",
       "[ ]  `python -c \"import cadence\"` succeeds\n",
       "\n",
       "Once those boxes are ticked, `pytest -q` should discover the package and either\n",
       "fail (red phase) or pass (green, after your Cadence cycle fixes the bug)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"\"\"\n",
    "Here is your task:\n",
    "Return a VERY detailed, line-by-line account of the workflow. Every single step that that can be traced and logically connected without\n",
    "any \"fuzzy\" gaps. Let's start with the ./backlog/backlog.json file and explain the each step (extremely nuanced) all the way until \n",
    "it becomes a completed task. Highlight any gaps in the process. Don't assume. Validate everything in the code.\n",
    "If there is a gap, then I want to know about it so that I can start to plan how to bridge it.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c96967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The failure means Python can’t find a top-level module called cadence when the test-runner imports\n",
      "    from cadence.utils.add import add\n",
      "\n",
      "That almost always boils down to “the directory that *contains* the cadence\n",
      "package isn’t on PYTHONPATH” (or the cadence package itself isn’t a real\n",
      "package).  Here’s the canonical, trouble-free layout and two quick ways to make\n",
      "sure the package is importable.\n",
      "\n",
      "────────────────────────────────────────\n",
      "1 · Preferred project layout\n",
      "────────────────────────────────────────\n",
      "cadence-repo-root/               ← you run  pytest  from here\n",
      "│\n",
      "├── src/                         ← **recommended**: keeps libs separate from misc files\n",
      "│   └── cadence/\n",
      "│       ├── __init__.py          ← even if empty\n",
      "│       ├── utils/\n",
      "│       │   └── add.py\n",
      "│       └── … other sub-packages …\n",
      "│\n",
      "├── tests/\n",
      "│   └── test_add.py\n",
      "│\n",
      "├── docs/  experiments/  etc.\n",
      "└── pyproject.toml               ← minimal packaging metadata\n",
      "\n",
      "Why “src/”?  It prevents the *repo root* (which might also be called\n",
      "cadence) from shadowing or confusing Python’s import machinery.\n",
      "\n",
      "If you don’t want the extra layer you can keep the current flat layout\n",
      "\n",
      "    cadence-repo-root/cadence/__init__.py\n",
      "    cadence-repo-root/tests/\n",
      "\n",
      "but then you must guarantee that the **repo root directory itself** is on\n",
      "`sys.path` whenever you run pytest.\n",
      "\n",
      "────────────────────────────────────────\n",
      "2 · Make the package importable\n",
      "────────────────────────────────────────\n",
      "A. Editable install (cleanest, works everywhere)\n",
      "\n",
      "    # inside your virtual-env, at repo root\n",
      "    python -m pip install -e .\n",
      "\n",
      "Create a tiny `pyproject.toml` (or `setup.cfg/setup.py`) that points to your\n",
      "package; example:\n",
      "\n",
      "```toml\n",
      "[project]\n",
      "name = \"cadence\"\n",
      "version = \"0.0.0\"\n",
      "dependencies = []\n",
      "\n",
      "[tool.setuptools]\n",
      "packages = [\"cadence\"]\n",
      "package-dir = {\"\" = \"src\"}      # omit this line if you don't use the src/ layout\n",
      "```\n",
      "\n",
      "After the editable install, `python -c \"import cadence; print(cadence.__file__)\"`\n",
      "should print the path inside your repo.\n",
      "\n",
      "B. Quick-and-dirty (dev only)\n",
      "\n",
      "Run pytest through Python so the current directory is put on PYTHONPATH:\n",
      "\n",
      "    python -m pytest -q      # instead of  pytest -q\n",
      "\n",
      "or export the path once per shell:\n",
      "\n",
      "    export PYTHONPATH=$PWD\n",
      "    pytest -q\n",
      "\n",
      "────────────────────────────────────────\n",
      "3 · What *not* to do\n",
      "────────────────────────────────────────\n",
      "• Don’t put production code under `experiments/`; keep that folder for notebooks\n",
      "  or throw-away prototypes.\n",
      "\n",
      "• Don’t give both the repo root and the inner package the same name *without*\n",
      "  a `src/` layer; it invites exactly this import shadowing headache.\n",
      "\n",
      "────────────────────────────────────────\n",
      "4 · Sanity checklist\n",
      "────────────────────────────────────────\n",
      "[ ]  `cadence/__init__.py` exists  \n",
      "[ ]  You are running pytest from the repo root (not from inside `tests/`)  \n",
      "[ ]  `python -m pip install -e .` (or PYTHONPATH trick) done  \n",
      "[ ]  `python -c \"import cadence\"` succeeds\n",
      "\n",
      "Once those boxes are ticked, `pytest -q` should discover the package and either\n",
      "fail (red phase) or pass (green, after your Cadence cycle fixes the bug).\n"
     ]
    }
   ],
   "source": [
    "print(agent.messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0dd862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 16:32:05,893] INFO LLM sync call: model=o3-2025-04-16, msgs_len=6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You don’t have a *code* problem, you have a **shadow-package** problem:\n",
       "\n",
       "• Your repository folder is called  cadence/  \n",
       "• Inside it you have another folder called  cadence/  (the real Python package)\n",
       "\n",
       "When Python executes\n",
       "\n",
       "    import cadence\n",
       "\n",
       "it finds the **outer** directory first, notices there is *no* `__init__.py`\n",
       "there, and therefore decides “this is not a package”.  Import fails before it\n",
       "ever looks at the inner one.\n",
       "\n",
       "Two clean fixes (pick one) ⬇︎\n",
       "\n",
       "────────────────────────────────────────\n",
       "OPTION A · Introduce a “src” layer      (most common)\n",
       "────────────────────────────────────────\n",
       "repo-root/                  ← rename outer folder to anything you like\n",
       "│\n",
       "├── src/\n",
       "│   └── cadence/            ← (move your whole code package here)\n",
       "│       ├── __init__.py\n",
       "│       └── … agents, dev, utils, …\n",
       "│\n",
       "├── tests/\n",
       "├── docs/\n",
       "└── pyproject.toml          ← minimal build metadata\n",
       "\n",
       "Commands:\n",
       "\n",
       "```bash\n",
       "# from current outer cadence directory\n",
       "mkdir src\n",
       "git mv cadence src/cadence         # move inner package into src/\n",
       "touch src/cadence/__init__.py      # if it doesn’t already exist\n",
       "```\n",
       "\n",
       "Add packaging metadata so tools know where the code lives:\n",
       "\n",
       "```toml\n",
       "# pyproject.toml\n",
       "[build-system]\n",
       "requires = [\"setuptools>=65.0\"]\n",
       "build-backend = \"setuptools.build_meta\"\n",
       "\n",
       "[project]\n",
       "name = \"cadence\"\n",
       "version = \"0.0.0\"\n",
       "\n",
       "[tool.setuptools]\n",
       "package-dir = {\"\" = \"src\"}\n",
       "packages = [\"cadence\"]\n",
       "```\n",
       "\n",
       "Then do an editable install:\n",
       "\n",
       "```bash\n",
       "python -m pip install -e .\n",
       "```\n",
       "\n",
       "Python/PyTest will now resolve  cadence.*  from  src/cadence  regardless of\n",
       "where you run them from.\n",
       "\n",
       "────────────────────────────────────────\n",
       "OPTION B · Rename the outer directory   (quickest)\n",
       "────────────────────────────────────────\n",
       "Just rename the repository folder so it no longer collides with the package\n",
       "name:\n",
       "\n",
       "```\n",
       "cadence/            ← the code package (keep)\n",
       "cadence-project/    ← new repo root name\n",
       "```\n",
       "\n",
       "Example (from one level above the repo):\n",
       "\n",
       "```bash\n",
       "mv cadence cadence-project        # rename outer folder\n",
       "cd cadence-project\n",
       "```\n",
       "\n",
       "Nothing else needs to change, because now the first “cadence” on `sys.path`\n",
       "*is* the package (it has `__init__.py`).\n",
       "\n",
       "────────────────────────────────────────\n",
       "AFTER RESTRUCTURING — sanity checks\n",
       "────────────────────────────────────────\n",
       "1. From repo root run:\n",
       "\n",
       "       python -c \"import cadence, pathlib, sys; print(cadence, pathlib.Path(cadence.__file__).resolve())\"\n",
       "\n",
       "   This should print a path *inside* your inner code package, not the repo\n",
       "   directory.\n",
       "\n",
       "2. Run the tests again:\n",
       "\n",
       "       pytest -q\n",
       "\n",
       "   • They should *fail* with AssertionError (red phase) until you run the\n",
       "     Cadence orchestrator.  \n",
       "   • After the orchestration cycle they should pass (green).\n",
       "\n",
       "────────────────────────────────────────\n",
       "FAQ\n",
       "────────────────────────────────────────\n",
       "• “Do I have to keep `src/` forever?”  \n",
       "  No, it’s just a widely-adopted pattern that prevents this class of bug.\n",
       "\n",
       "• “Will this break my import paths?”  \n",
       "  No, all runtime imports still use `import cadence...`.\n",
       "\n",
       "• “What about VS Code / PyCharm?”  \n",
       "  After `pip install -e .` the IDE will locate the package automatically.\n",
       "\n",
       "Make the change once, commit it, and you’ll never see this import error again."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"\"\"I have it setup as \n",
    "\n",
    "cadence(root)\n",
    "- cadence (code)\n",
    "- - agents\n",
    "- - context\n",
    "- - dev\n",
    "- - llm\n",
    "- - utils\n",
    "- - - add.py\n",
    "- - - mvp_loop.py\n",
    "- docs\n",
    "- tests\n",
    "- - test_add.py\n",
    "...\n",
    "\n",
    "Should I refactor?\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed56c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-20 16:34:19,203] INFO LLM sync call: model=o3-2025-04-16, msgs_len=8\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Short answer  \n",
       "─────────────  \n",
       "You can keep a flat layout (`cadence-project/cadence`) and everything will\n",
       "work **as long as** the *repository folder* is not also called\n",
       "“cadence”.  Renaming the repo folder to `cadence-project` (or anything that\n",
       "isn’t the same as the package) eliminates the shadowing bug and you’re done.\n",
       "\n",
       "So why do so many modern Python repos add a **src/** layer? The rest of this\n",
       "answer explains the advantages so you can decide which trade-off you prefer.\n",
       "\n",
       "\n",
       "Detailed answer – the “src layout” in four bullets  \n",
       "──────────────────────────────────────────────────\n",
       "1.  Prevents *accidental* imports of source that hasn’t been installed  \n",
       "    • When you run `pytest`, Python starts with  \n",
       "      `sys.path = [\"repo-root\", …]`.  \n",
       "    • If your package lives directly in `repo-root/cadence`, the *tests\n",
       "      import the code by path*, even if you forgot `pip install -e .`.  \n",
       "    • With `src/cadence`, the tests fail immediately until you install the\n",
       "      package ‑> catches packaging mistakes early.\n",
       "\n",
       "2.  Avoids name collisions (“shadow packages”)  \n",
       "    • The bug you just hit: outer `cadence/` (not a package) hides inner\n",
       "      `cadence/` (real package).  The `src` layer makes those names different\n",
       "      by construction.\n",
       "\n",
       "3.  Makes wheels & sdists safer to build  \n",
       "    • `setuptools` finds files via the `package-dir` mapping; nothing leaks\n",
       "      into the wheel that isn’t explicitly listed.  \n",
       "    • No risk that `tests/`, `.github/`, `docs/`, … end up inside the wheel\n",
       "      because they sit next to `cadence`.\n",
       "\n",
       "4.  Encourages a cleaner mental model  \n",
       "    • “Everything under **src/** is importable application/library code.”  \n",
       "    • “Anything next to it is tooling, config, docs, data, examples, tests.”  \n",
       "      This separation scales well when repos grow large.\n",
       "\n",
       "What if I keep it flat? Rules of thumb  \n",
       "──────────────────────────────────────\n",
       "• Rename the **repository folder** so it is *not* the same as the package:\n",
       "\n",
       "```\n",
       "cadence-project/          ← repo root\n",
       "├── cadence/              ← actual package (has __init__.py)\n",
       "└── tests/\n",
       "```\n",
       "\n",
       "• Add an editable install once per virtual-env:\n",
       "\n",
       "```\n",
       "python -m pip install -e .\n",
       "```\n",
       "\n",
       "• Ensure CI invokes `pytest` only *after* the editable install step.\n",
       "\n",
       "If you follow those three rules, the flat layout is perfectly fine and a bit\n",
       "simpler to navigate.\n",
       "\n",
       "Cheat-sheet comparison  \n",
       "──────────────────────\n",
       "\n",
       "| Topic                         | Flat layout (`cadence/`)      | `src/` layout (`src/cadence/`)\n",
       "|-------------------------------|--------------------------------|--------------------------------\n",
       "| Quick to read in a small repo | ✔                              | ✔\n",
       "| Accidental shadowing issues   | ⚠ (must rename repo)          | —\n",
       "| Tests pass without install    | ✔ (could mask packaging bugs) | ✘ (forces install first)\n",
       "| Wheel build isolation         | OK with care                  | More fool-proof\n",
       "| Industry adoption             | Common                        | Increasingly common (e.g. Hypothesis, FastAPI)\n",
       "\n",
       "Choose whichever you and your team prefer; just don’t keep two identically\n",
       "named “cadence” directories at different levels—that’s what broke your\n",
       "imports earlier."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat(\"What is the point of having a src? Why not just cadence-project > cadence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be501f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "947382c4",
   "metadata": {},
   "source": [
    "# 🧭 Immediate Next Steps — 45-Minute Kick-off Checklist  \n",
    "(aim: reach “red” state with a failing test that the autonomous loop can later turn green)\n",
    "\n",
    "1. Workspace & Environment\n",
    "   • Create / activate a fresh Python ≥3.10 virtual-env.  \n",
    "   • `pip install -U openai python-dotenv pytest tabulate streamlit pandas`  \n",
    "   • Ensure `OPENAI_API_KEY` is exported (or present in `.env`).\n",
    "\n",
    "2. Baseline Commit\n",
    "   • `git init` (if not already) → commit current code as “baseline-0”.\n",
    "\n",
    "3. Seed the Toy Defect (Story S-MVP-1.1)\n",
    "   a. `mkdir -p cadence/utils`  \n",
    "   b. `cadence/utils/add.py`  \n",
    "      ```python\n",
    "      def add(x: int, y: int) -> int:\n",
    "          \"\"\"Intentionally wrong implementation for MVP red→green demo.\"\"\"\n",
    "          return x - 1 + y\n",
    "      ```\n",
    "   c. `tests/test_add.py`  \n",
    "      ```python\n",
    "      from cadence.utils.add import add\n",
    "\n",
    "      def test_add():\n",
    "          assert add(2, 3) == 5\n",
    "      ```\n",
    "   d. `pytest` should now FAIL. Commit as “seed-defect”.\n",
    "\n",
    "4. Backlog File\n",
    "   • `dev_backlog.json` → paste the single micro-task that will drive the loop:\n",
    "\n",
    "   ```json\n",
    "   [\n",
    "     {\n",
    "       \"id\": \"bug-fix-add-001\",\n",
    "       \"title\": \"Fix utils.add.add() so that add(2,3)==5\",\n",
    "       \"type\": \"micro\",\n",
    "       \"status\": \"open\",\n",
    "       \"created_at\": \"2025-06-21T00:00:00Z\",\n",
    "       \"diff\": {\n",
    "         \"file\": \"cadence/utils/add.py\",\n",
    "         \"before\": \"def add(x: int, y: int) -> int:\\\\n    return x - 1 + y\\\\n\",\n",
    "         \"after\":  \"def add(x: int, y: int) -> int:\\\\n    return x + y\\\\n\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "5. Minimal Config Stub\n",
    "   • Create `dev_config.json` (or inline in code) matching:\n",
    "\n",
    "   ```json\n",
    "   {\n",
    "     \"backlog_path\": \"dev_backlog.json\",\n",
    "     \"template_file\": \"dev_templates.json\",\n",
    "     \"src_root\": \"cadence\",\n",
    "     \"ruleset_file\": null,\n",
    "     \"repo_dir\": \".\",\n",
    "     \"record_file\": \"dev_record.json\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "6. Smoke-Test Orchestrator\n",
    "   ```bash\n",
    "   python -m cadence.dev.orchestrator start\n",
    "   ```\n",
    "   Expected flow:\n",
    "   • Executor builds patch (from backlog-diff).  \n",
    "   • Reviewer passes (static rules).  \n",
    "   • ShellRunner applies patch.  \n",
    "   • Pytest turns GREEN.  \n",
    "   • Commit & archive recorded to `dev_record.json`.\n",
    "\n",
    "7. Verify Audit Artefacts\n",
    "   • Open `dev_record.json` → confirm states: build_patch → patch_built → … → archived.  \n",
    "   • `git log -1` shows commit labeled “[Cadence] bug-fix-add-001 …”.\n",
    "\n",
    "8. Push to Remote / PR\n",
    "   • `git remote add origin …` (if needed)  \n",
    "   • `git push -u origin main`\n",
    "\n",
    "9. Stretch (optional, if time remains)\n",
    "   • Run `streamlit run cadence/dev/command_center.py` and walk through the same task via UI.  \n",
    "   • Add a lint rule to `ruleset.json` (e.g., forbid “print(”) and point config at it to see reviewer fail.\n",
    "\n",
    "––––––\n",
    "Once the above is green you have an end-to-end proof that the agentic pipeline can fix a real bug. The next milestone (Story S-MVP-1.2) is to shift from hand-authored diff to LLM-generated diff via `ExecutionAgent`; we’ll tackle that after the smoke test is stable.'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb688f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
