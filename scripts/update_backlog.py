#!/usr/bin/env python3
# scripts/update_backlog.py
"""
Generate or refresh **dev_backlog.json**.

• **ONLINE**  – Ask the ReasoningAgent to analyse the repo & docs and
               return a JSON backlog array (or an object with a top‑level
               `tasks` list).

• **OFFLINE** – Produce deterministic placeholder tasks so CI /
               air‑gapped environments never fail.

This script **never crashes**: it always writes a valid JSON array to
`repo‑root/dev_backlog.json`.
"""

from __future__ import annotations

import json
import datetime as _dt
from pathlib import Path
from typing import List
import uuid

from cadence.context.provider import SnapshotContextProvider
from cadence.agents.registry import get_agent
from cadence.context.prompts import UPDATE_BACKLOG

BACKLOG_PATH = Path("dev_backlog.json")
CODE_ROOTS = (
    Path("src/cadence"),
    Path("tests"),
    Path("tools"),
    Path("docs"),
    Path("scripts"),
)
CODE_EXTS = (
    ".py",
    ".md",
    ".json",
    ".mermaid",
    ".txt",
    ".yaml",
    ".yml",
)
DOCS_EXTS = (".md",)


# --------------------------------------------------------------------------- #
# helpers
# --------------------------------------------------------------------------- #

def _now_iso() -> str:
    """UTC timestamp (second resolution, TZ‑aware)."""
    return (
        _dt.datetime.utcnow()
        .replace(tzinfo=_dt.timezone.utc)
        .isoformat(timespec="seconds")
    )


def _stub_backlog(count: int = 3) -> List[dict]:
    """Return *count* placeholder micro‑tasks (never raises)."""
    return [
        {
            "id": uuid.uuid4().hex,
            "title": f"Placeholder task {idx + 1}",
            "type": "micro",
            "status": "open",
            "created_at": _now_iso(),
            "description": (
                "Autogenerated placeholder because the LLM is offline or "
                "returned an error. Replace with a real micro‑task once "
                "connectivity is restored."
            ),
        }
        for idx in range(count)
    ]


# --------------------------------------------------------------------------- #
# main flow
# --------------------------------------------------------------------------- #

def main() -> None:
    provider = SnapshotContextProvider()

    # ---- gather snapshots -------------------------------------------------
    code_snapshot = provider.get_context(*CODE_ROOTS, exts=CODE_EXTS, out="-")
    docs_snapshot = provider.get_context(Path("docs"), exts=DOCS_EXTS, out="-")

    agent = get_agent("reasoning")
    backlog: List[dict]

    # ---- ONLINE branch ----------------------------------------------------
    if not getattr(agent.llm_client, "stub", False):
        prompt = UPDATE_BACKLOG.format(
            code_snapshot=code_snapshot, docs_snapshot=docs_snapshot
        )
        try:
            # NOTE: temperature omitted; model o3‑2025‑04‑16 requires default
            raw = agent.run_interaction(prompt, json_mode=True)
            parsed = json.loads(raw) if isinstance(raw, str) else raw

            # ----------------- Option A: accept {"tasks": [...] } ----------
            if isinstance(parsed, dict) and "tasks" in parsed:
                parsed = parsed["tasks"]

            if not isinstance(parsed, list):
                raise ValueError(
                    "Assistant must return a JSON array or an object with a "
                    "'tasks' array inside."
                )
            backlog = parsed

        except Exception as exc:  # noqa: BLE001 – any LLM / JSON issue
            print(
                f"[WARN] ReasoningAgent failed → stub backlog used.  ({exc})"
            )
            backlog = _stub_backlog()

    # ---- OFFLINE / stub branch -------------------------------------------
    else:
        print("[INFO] LLM stub‑mode detected — using stub backlog")
        backlog = _stub_backlog()

    # ---- final safety pass ------------------------------------------------
    for task in backlog:
        task.setdefault("id", uuid.uuid4().hex)
        task.setdefault("type", "micro")
        task.setdefault("status", "open")
        task.setdefault("created_at", _now_iso())

    BACKLOG_PATH.write_text(json.dumps(backlog, indent=2, ensure_ascii=False))
    print(f"Wrote {len(backlog)} task(s) → {BACKLOG_PATH}")


if __name__ == "__main__":  # pragma: no cover
    main()
