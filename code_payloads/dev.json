{
  "docs/DEV_WORKFLOW.md": "# Cadence Solo-Developer Workflow (v0.2)\n\nThis file is executable documentation: follow it line-by-line for each\nmicro-task.\n\n## Shell aliases (one-time)\n\n```bash\nalias ktests='pytest -q'\nalias kprompt='python tools/gen_prompt.py --code-root cadence --docs-dir docs'\n```\n\n---\n\n## Cadence OOP/Agentic Workflow (2025+)\n\n| Phase    | Module/Class    | Responsibility                  | Error/Fault Handling                    |\n|----------|----------------|----------------------------------|-----------------------------------------|\n| Backlog  | BacklogManager | Presents task list/selection     | Empty: process halts; log required      |\n| Generate | TaskGenerator  | Produces new tasks (LLM/options) | Failed output: abort                    |\n| Diff     | TaskExecutor   | Produces/appplies patch          | Invalid/unapplicable: reject/abort      |\n| Test     | ShellRunner    | Runs tests on patch code         | Test fail: loop refinement              |\n| Review   | TaskReviewer   | Review diff for pass/fail        | Fail = loop back to Diff                |\n| Commit   | ShellRunner    | Git commit/record/close loop     | Commit fail: halt/error/log             |\n| Record   | TaskRecord     | Archive full state/history       | Always must succeed (blocking)          |\n\n> OOP classes handle each role, but can be swapped for specialized “agents” in a future distributed or multi-member system. All phases are explicit and logged.\n\n### Guard Rails & Fail Criteria\n\n- No step is automatic; all input/output must be logged.\n- No phase may be skipped or short-circuited (bypass is a policy violation).\n- Any deviation from prescribed interface is a bug.\n- All logs and state must remain audit-ready and reconstructable.\n\n---\n\n# **Adopt this foundation for all future Cadence projects.**\n- Keep roles single-responsibility, explicit, and logged.\n- Upgrade to agentic/distributed team as needs grow – code and docs are ready for growth.\n- Use these docs as onboarding and review materials for any collaborators.\n\n---",
  "docs/DEV_ORCHESTRATOR.md": "# Cadence Dev-Orchestrator — Quick Start\n\n> *Five minutes to turn your terminal or notebook into a mini-IDE for Cadence.*\n\n## Install (optional eye-candy)\n    pip install rich prompt_toolkit\n\n## CLI Cheat-Sheet\n| Action                | Command                                                   | OOP/Agent Role      |\n|-----------------------|-----------------------------------------------------------|---------------------|\n| View open micro-tasks | `python -m cadence.dev.orchestrator backlog`               | BacklogManager      |\n| Generate micro-tasks  | `python -m cadence.dev.orchestrator generate --mode micro` | TaskGenerator       |\n| Patch loop            | `python -m cadence.dev.orchestrator start`                 | TaskExecutor        |\n| Run tests/review      | `python -m cadence.dev.orchestrator evaluate`              | ShellRunner/Reviewer|\n| Commit & record done  | `python -m cadence.dev.orchestrator done`                  | Archive/Orchestrator|\n| (All via orchestrator)|                                                           |                     |\n\n*See internal docs for full agent/class mapping and extension points!*\n\n---\n\n\n## Notebook Flow\n```python\nfrom cadence.dev.orchestrator import DevOrchestrator\norch = DevOrchestrator()\norch.show()          # backlog overview\norch.run_task_cycle()\n```\n\n---\n## Conceptual Agent/OOP Pipeline\n\n```mermaid\nflowchart LR\n    BacklogManager --> TaskExecutor\n    TaskExecutor --> ShellRunner\n    ShellRunner --> TaskReviewer\n    TaskReviewer --> Orchestrator\n    Orchestrator --> BacklogManager\n    Orchestrator --> TaskGenerator\n```\n\n**All commands route through the Orchestrator**, which calls the appropriate role (class). Each role matches to a single responsibility in the dev pipeline and can be upgraded/swapped for an LLM agent or external service. Every step logs its state.\n\n\n## Troubleshooting\n\n| Symptom             | Hint                                                      |\n| ------------------- | --------------------------------------------------------- |\n| Plain text headings | `rich` not installed — fine, just less colourful.         |\n| LLM auth error      | Export `AZURE_OPENAI_*` vars or create `config/llm.json`. |\n| Patch rejects       | File already exists — delete or commit first, retry.      |\n| Tests fail          | Fix code or regenerate diff via `task`, then rerun.       |\n\n---\n\n© 2025 Cadence Project — personal developer tooling.",
  "docs/NORTH_STAR.md": "# **CADENCE PLATFORM — NORTH STAR BLUEPRINT**\n\n*Single source of truth for all human and agent contributors.*\n*Commit verbatim as `docs/NORTH_STAR.md` and load as root system prompt.*\n\n*Last revised 2025-06-18 (UTC-05:00)*\n\n---\n\n## 1. Mission\n\n> **To industrialize high-quality software delivery through a seamlessly unified, auditable, agentic-human workflow—enabling continuous, self-improving, and transparent execution across diverse projects from ML to infrastructure, at maximum velocity and minimal risk.**\n\n---\n\n## 2. End-State Vision\n\n| Axis                                      | Description                                                                                                                                                     |\n| ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Unified Orchestrator**                  | Central controller with exclusive authority to coordinate all phases; roles fully interchangeable between human and autonomous agents.                          |\n| **Agentic Flexibility**                   | Clear interfaces allowing any role (codegen, testing, review, etc.) to seamlessly switch between humans, LLMs, or microservices at runtime without refactoring. |\n| **Immutable Auditability**                | Tamper-proof logs of every action, decision, override, and state transition, enabling full historical replay and compliance audits.                             |\n| **Continuous Meta-Learning**              | Real-time and periodic meta-agent analyses detect bottlenecks, regressions, technical debt, and policy drift, actively enforcing quality standards.             |\n| **Universal Applicability**               | Flexible architecture supporting diverse domains (e.g., REIT, analytics, ML, APIs, operations) without special accommodations or workflow changes.              |\n| **Transparent Human-Agent Collaboration** | Complete visibility of rationale, state, and handoffs between human and agent roles, maintaining intuitive interaction and trust.                               |\n\n---\n\n## 3. Strategic Objectives & Key Results (12-month Horizon)\n\n| Objective                           | Key Results                                                                                                                           |\n| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n| **O1: Launch Robust MVP**           | KR1: Complete one non-trivial end-to-end project delivery (e.g., REIT modeling) with full auditability and agent-human collaboration. |\n| **O2: Maximize Velocity**           | KR2: Achieve median task cycle time (selection→commit) ≤1 day; minimum throughput of 5 tasks/week per developer.                      |\n| **O3: Ensure Reliability**          | KR3: Zero regressions post-commit; automated gating ensures 100% test pass before merge.                                              |\n| **O4: Enable Autonomous Operation** | KR4: Successfully operate at least 3 key workflow phases autonomously per project, logged overrides ≤10%.                             |\n| **O5: Continuous Improvement**      | KR5: Real-time analytics on task bottlenecks, rollback frequencies, and failure rates auto-generated and reviewed monthly.            |\n\n---\n\n## 4. Guiding Principles\n\n1. **Explicit Contracts:** Clear boundaries, single responsibilities, documented and enforced interfaces.\n2. **Audit by Default:** Immutable records for transparency, compliance, and learning.\n3. **Human Empowerment:** Agents augment humans; overrides are explicitly logged, reviewed, and integrated into learning.\n4. **No Hidden State:** All state transitions explicit, serialized, and transparent.\n5. **Rapid Feedback & Iteration:** Fail fast, surface clearly, and integrate feedback immediately.\n6. **Continuous Meta-Optimization:** Meta-process itself is monitored, self-correcting, and prioritized for improvement.\n\n---\n\n## 5. Workflow Phases & Responsibilities\n\n| Phase                                             | Responsibility                                     | Non-negotiable Outputs                 |\n| ------------------------------------------------- | -------------------------------------------------- | -------------------------------------- |\n| **Backlog** (`BacklogManager`)                    | Manage task lifecycle, prioritization              | Prioritized task queue                 |\n| **Generation** (`TaskGenerator`)                  | Task definition, clarity, context alignment        | Validated task objects                 |\n| **Execution** (`TaskExecutor`)                    | Create implementable diffs, no direct file edits   | Unified, reviewable diff               |\n| **Testing** (`ShellRunner`)                       | Isolated execution of tests, zero side effects     | Test logs, pass/fail                   |\n| **Review** (`TaskReviewer`)                       | Audit diff quality, correctness, compliance        | Approval/rejection with rationale      |\n| **Commit & Record** (`ShellRunner`, `TaskRecord`) | Persist approved changes, archive state            | Immutable logs and state transitions   |\n| **Meta-Review** (`MetaAgent`)                     | Identify workflow improvements, enforce compliance | Actionable improvement reports, alerts |\n\n---\n\n## 6. Audit, Override & Rollback Policy\n\n* All actions explicitly logged with timestamps, rationales, actors.\n* Overrides permitted but logged as exceptions with clear rationales.\n* Rollbacks documented with prior state and explicit reason.\n* No manual edits outside orchestrator without reconciliation.\n\n---\n\n## 7. Agentic Architecture\n\n```mermaid\nflowchart TD\nOrchestrator --> BacklogManager\nOrchestrator --> TaskGenerator\nOrchestrator --> TaskExecutor\nOrchestrator --> ShellRunner\nOrchestrator --> TaskReviewer\nShellRunner --> TaskRecord\nOrchestrator --> MetaAgent\nMetaAgent --> Orchestrator\n```\n\n---\n\n## 8. Metrics Dashboard & Alerts\n\n| Metric Category       | Metrics                                  | Frequency |\n| --------------------- | ---------------------------------------- | --------- |\n| **Velocity**          | Median task completion cycle time        | Real-time |\n| **Quality**           | Test pass rate on first attempt          | Nightly   |\n| **Stability**         | Rollback and override rate               | Real-time |\n| **Efficiency**        | Bottleneck detection (>24 hours blocked) | Real-time |\n| **Agent Integration** | Human-agent role distribution (%)        | Weekly    |\n\nDashboards include proactive alerts for outliers or policy breaches.\n\n---\n\n## 9. Strategic Roadmap (12-Month)\n\n| Phase                       | Duration  | Milestones                                   |\n| --------------------------- | --------- | -------------------------------------------- |\n| **Bootstrap**               | 2 weeks   | MVP orchestrator; initial end-to-end project |\n| **Agent Integration**       | +2 weeks  | ≥2 autonomous agent roles operational        |\n| **Full Audit & Compliance** | +2 weeks  | Immutable logs; audit replayable state       |\n| **Meta Optimization**       | +4 weeks  | Continuous bottleneck analytics active       |\n| **Domain Scaling**          | +12 weeks | ≥3 diverse domain implementations            |\n| **Autonomous Scaling**      | +12 weeks | Agentic parallel task management             |\n\n---\n\n## 10. Risk Management & Guardrails\n\n| Risk                 | Mitigation                                     |\n| -------------------- | ---------------------------------------------- |\n| Agent drift          | Automated alerts and mandatory human review    |\n| Hidden state         | Strict enforcement of serializable outputs     |\n| Workflow skips       | Orchestrator enforces phase sequence           |\n| Human override abuse | Overrides logged as exceptions, periodic audit |\n| Audit divergence     | Continuous reconciliation and alerting         |\n\n---\n\n## 11. Knowledge Transfer & Leverage Plan\n\n* **Internal Transparency:** Regular auto-generated dev logs.\n* **Public Trust:** Whitepapers, case studies, transparency reports.\n* **Onboarding & Training:** Recorded workflows, demos, documented procedures.\n\n---\n\n## 12. Glossary & Definitions\n\n| Term           | Definition                                                  |\n| -------------- | ----------------------------------------------------------- |\n| **Task**       | Serializable JSON object with full state history            |\n| **Patch**      | Unified diff representing proposed code changes             |\n| **Agent Slot** | Explicitly defined interface, swappable between agent/human |\n| **Override**   | Manual intervention with explicit logged rationale          |\n| **MetaAgent**  | Analytics system enforcing continuous workflow optimization |\n\n---\n\n**End of Cadence North Star Blueprint**\n",
  "docs/DEV_PROCESS.md": "# CADENCE DEVELOPMENT PROCESS: ROLES, RESPONSIBILITIES, AND WORKFLOW\n\n## Overview\n\nThe Cadence codebase is architected with a clear separation of concerns, assigning every major phase of the developer loop to a dedicated **role**. Each role is implemented as a class with a precise interface. In the future, these roles may be filled by “agents”—be they LLMs, microservices, or human collaborators—without fundamental changes to the orchestrator or workflow.\n\nThis process is summarized in the following policy, guard rails, and fail criteria.\n\n---\n\n## 1. **Core Agent/Class Roles**\n\n| Role (Class/Agent) | Responsibility                               | Future agent?            | Critical Interface               |\n|--------------------|----------------------------------------------|--------------------------|----------------------------------|\n| `BacklogManager`   | Manage backlog: microtasks, stories, epics   | Yes                      | `list_items`, `add_item`, `remove_item`, `archive_completed` |\n| `TaskGenerator`    | Propose new tasks (via LLM/rules/templates)  | Yes                      | `generate_tasks`, `overwrite_tasks`    |\n| `TaskExecutor`     | Given a task description, produce code diff  | Yes                      | `build_patch`, `refine_patch`      |\n| `TaskReviewer`     | Given a diff, adjudicate quality/pass/fail   | Yes                      | `review_patch`                   |\n| `ShellRunner`      | Safely run local shell commands (git/tests)  | Maybe (for remote/CI)    | `git_apply`, `run_pytest`, `git_commit` |\n| `TaskRecord`       | Persist task history throughout workflow     | Not directly             | `save`, `load`, `append_iteration` |\n| `DevOrchestrator`  | Drives process, connects all roles           | Maybe (AI orchestrator)  | Main CLI/user interface          |\n\n> **Note:** Each role is a future \"swap point\"—today a Python class, tomorrow a remote caller, LLM, or collaborative tool.\n\n---\n\n## 2. **Process and Expected Sequence**\n\n### **A. Developer Workflow Loop**\n\n| Phase            | Responsibility (role)         | Fail Criteria                      |\n|------------------|------------------------------|------------------------------------|\n| **Backlog**      | Select task from `BacklogManager` | If no items: Workflow blocks      |\n| **Task Generation (optional)** | `TaskGenerator.generate_tasks`       | Tasks not well-formed/constrained  |\n| **Patch Proposal**| `TaskExecutor.build_patch`     | Patch not produced or invalid      |\n| **Patch Application** | `ShellRunner.git_apply`   | Patch does not apply cleanly       |\n| **Testing**      | `ShellRunner.run_pytest`      | Tests fail or incomplete           |\n| **Review**       | `TaskReviewer.review_patch`   | Diff fails hard rules or review    |\n| **Iteration/Refinement** | `TaskExecutor.refine_patch` | Feedback not incorporated or endless loops |\n| **Commit/Archive** | `ShellRunner.git_commit`, `TaskRecord.save` | Commit fails, state not saved   |\n\n**Guard Rails**\n- **No step may skip its predecessor.** E.g., cannot commit code that didn’t pass tests and review.\n- **Every phase must expose clear, testable input/output (CLI, method, or file).**\n- **All items, diffs, and outcomes are logged and auditable (via `TaskRecord`).**\n\n---\n\n## 3. **Key Expectations (What Is Success?)**\n\n- Each role/class/agent does **only one job**.\n- Inputs and outputs are strictly defined.\n- Human, LLM, or other agents can be swapped for in any given role, provided they honor the contract.\n- **Automation is always overridable** by a human, but never skipped without trace.\n- The orchestrator is the **sole entity permitted to coordinate all roles**; all coordination flows through its interface.\n- **Backlogs, history, and outcomes are always persistently logged.**\n\n---\n\n## 4. **Failure Criteria** *(When Is It a Bug or Policy Violation?)*\n\n- Any class/agent does more than its prescribed role, or has unclear boundaries.\n- Orchestration is hidden (implicit flows, tight coupling).\n- Steps in the workflow are skipped or merged, process becomes opaque.\n- Human/manual editing outside orchestrated flow (unless explicitly logged/recorded).\n- Persistent record not up-to-date with reality (backlog drift, missed commits).\n- Reviews (auto or manual) not enforced, or tasks proceeding after test/review fail.\n\n---\n\n## 5. **Reference Workflow Diagram**\n\n```mermaid\nflowchart LR\n    subgraph Human/CLI\n        Start\n    end\n    subgraph Orchestrator\n        Orchestrator\n    end\n    subgraph Agents/Roles\n        BacklogManager --> TaskExecutor\n        TaskExecutor --> ShellRunner\n        ShellRunner --> TaskReviewer\n        TaskReviewer --> Orchestrator\n        Orchestrator --> BacklogManager\n        Orchestrator --> TaskGenerator\n    end\n    Start --> Orchestrator\n    Orchestrator --> BacklogManager\n    Orchestrator --> TaskExecutor\n    Orchestrator --> ShellRunner\n    Orchestrator --> TaskReviewer\n    Orchestrator --> TaskGenerator\n```\n\n---\n\n## 6. **Role Swapping: OOP ↔ Agentic (Future-Proofing Table)**\n\n| Step            | Today (OOP/class)      | Tomorrow (Agent)              |\n|-----------------|-----------------------|-------------------------------|\n| Backlog         | Sync/backed by class/file | LLM, microservice, shared board |\n| Patch Proposal  | LLM called from class  | External LLM or crowd agent   |\n| Patch Apply     | Local GitShell class   | Remote shell, CI pipeline     |\n| Test            | Local shell/pytest     | Kubernetes job, CI/CD web     |\n| Review          | Class, LLM, or human   | AI copilot, human reviewers   |\n| Orchestrate     | Python orchestrator    | Multi-agent conductor         |\n\n---\n\n## 7. **Canonical Example: End-to-End Flow**\n\n```python\nfrom cadence.dev.orchestrator import DevOrchestrator\n\norch = DevOrchestrator()\norch.view_backlog(\"micro\")\norch.start_task(\"micro\")      # Uses TaskExecutor, logs TaskRecord\n# (inspect/modify code as needed, if dry_run: try without side effect)\norch.evaluate_task()          # shell/tests + review agent\norch.finalise_task(\"micro\")   # archives task, updates records\n```\n\n---",
  "docs/OPEN_TASKS.md": "# Open Tasks\n\n## Bootstrap MVP Cadence Platform\n\n* Implement basic `BacklogManager` class with CRUD operations and JSON persistence.\n* Develop initial `TaskExecutor` to produce simple unified diffs.\n* Build initial `ShellRunner` capable of safely applying diffs, running pytest, and committing changes.\n* Integrate `TaskRecord` for initial task state logging.\n* Validate basic orchestrator end-to-end via notebook and CLI.\n\n## Autonomous Agent Roles Implementation\n\n* Integrate initial LLM (e.g., OpenAI API) into `TaskGenerator` for generating task descriptions.\n* Automate initial code diff generation via LLM-driven `TaskExecutor`.\n* Create basic automated review ruleset in `TaskReviewer`.\n* Implement autonomous testing in `ShellRunner` leveraging pytest.\n* Validate autonomous agent workflow on a controlled test scenario.\n\n## Full Audit and Compliance Infrastructure\n\n* Develop a secure, append-only logging mechanism in `TaskRecord`.\n* Implement state serialization ensuring full replayability.\n* Test and validate rollback logging explicitly.\n* Validate audit replay capability through intentional manual overrides.\n\n## Meta-Agent Continuous Improvement\n\n* Implement initial data collection on task metrics (velocity, failures, rollbacks).\n* Develop real-time alerting logic for significant workflow deviations.\n* Deliver a prototype monthly improvement report with actionable insights.\n\n## Scalable Multi-Domain Deployment\n\n* Setup initial ML model deployment pipeline orchestrated by Cadence.\n* Execute a simple analytics pipeline using Cadence.\n* Validate REST API development orchestration including testing and deployment.\n\n## React Frontend Transition\n\n* Setup initial React development environment.\n* Develop basic React components for task backlog management.\n* Integrate React frontend with Cadence backend APIs.\n* Replace Streamlit components with React equivalents.\n* Validate frontend-backend integration end-to-end.\n\n---\n\n## Deferred Tasks (Not permitted in this phase)\n\n* Advanced agent negotiation and collaboration mechanisms.\n* Enhanced UI for broader user groups.\n* Complex external integrations with third-party systems.\n",
  "docs/OPEN_STORIES.md": "# Open Stories\n\n## Bootstrap MVP Cadence Platform\n\n* Establish basic orchestration loop with BacklogManager, TaskExecutor, ShellRunner, and TaskRecord.\n* Validate full lifecycle from task creation to commit and archive.\n* Demonstrate basic human override and rollback mechanisms.\n\n## Autonomous Agent Roles Implementation\n\n* Integrate initial LLM agent for code patch generation.\n* Implement automated testing agent to run and validate pytest results.\n* Establish initial automated code review agent.\n\n## Full Audit and Compliance Infrastructure\n\n* Develop immutable logging mechanism ensuring tamper-proof audit trails.\n* Implement audit replay and historical state reconstruction.\n* Validate audit trail completeness with intentional overrides and rollbacks.\n\n## Meta-Agent Continuous Improvement\n\n* Deploy meta-agent analytics to monitor task velocity, failure rates, and rollback frequency.\n* Automate bottleneck identification and proactive alerts.\n* Deliver monthly actionable improvement reports.\n\n## Scalable Multi-Domain Deployment\n\n* Deploy Cadence to manage and audit an ML model deployment pipeline.\n* Successfully orchestrate an analytics pipeline with Cadence.\n* Validate Cadence on a REST API development and deployment cycle.\n\n---\n\n## Deferred (Do Not Start)\n\n* Advanced UI/UX enhancements.\n* Comprehensive multi-user permissions and authentication.\n* External system integration beyond basic functionality.\n",
  "docs/OPEN_EPICS.md": "# Open Epics\n\n* **Epic: Bootstrap MVP Cadence Platform**\n  Establish the foundational orchestration loop, agent interfaces, and core functionality.\n  **Outcome:** End-to-end functioning orchestrator delivering one full lifecycle task, fully auditable.\n\n* **Epic: Implement Autonomous Agent Roles**\n  Develop and deploy autonomous agents capable of independently managing workflow phases (code generation, testing, review).\n  **Outcome:** At least two autonomous agents active with minimal human override.\n\n* **Epic: Full Audit and Compliance Infrastructure**\n  Integrate immutable logging and state management ensuring complete traceability and audit readiness.\n  **Outcome:** Fully reconstructable task history and audit logs operational.\n\n* **Epic: Meta-Agent for Continuous Improvement**\n  Implement analytics-driven meta-agent for proactive detection of bottlenecks, technical debt, and workflow inefficiencies.\n  **Outcome:** Real-time analytics and monthly actionable improvement reports active.\n\n* **Epic: Scalable Multi-Domain Deployment**\n  Prove universal applicability by orchestrating diverse project types (ML, analytics, APIs, infra).\n  **Outcome:** Successfully deliver and audit at least three distinct project implementations.\n\n---\n\n**Explicitly Out of Scope**\n\n* Integration with external project management platforms.\n* Advanced UI/UX features beyond basic functional Streamlit interfaces.\n* Multi-user authentication and security layers beyond basic operational controls.\n",
  "cadence/agents/task_agent.py": "# cadence/agents/task_agent.py\n\"\"\"\nTaskAgent — Conversational design/code agent with episodic, task-centric context.\n\n- Maintains a chat-style history (OpenAI format), with message[0] always the\n  latest full snapshot of the codebase and docs (from tools/collect_code.py).\n- Exposes API to reset context, append dialog, run LLM, and save/load history.\n- Integrates with orchestrator to refresh code context and archive old dialog\n  after each finalized task.\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Any\nfrom .base import Agent\n\nDEFAULT_COLLECT_PATH = \"tools/collect_code.py\"\nDEFAULT_COLLECT_ARGS = [\n    \"--root\", \"kairos\",\n    \"--root\", \"docs\",\n    \"--ext\", \".py\", \".md\", \".cfg\", \".toml\", \".ini\",\n    \"--max-bytes\", \"50000\",\n    \"--out\", \"-\"\n]\n\nclass TaskAgent(Agent):\n    def __init__(self):\n        super().__init__()\n        # These can be overridden for per-agent context\n        self.collect_roots = [\"docs\", \"kairos\", \"tools\"]\n        self.collect_ext = (\".py\", \".md\")\n        self.max_bytes = 50000\n\n    def set_collect_code_args(self, roots, ext, max_bytes=None):\n        self.collect_roots = list(roots)\n        self.collect_ext = tuple(ext)\n        if max_bytes is not None:\n            self.max_bytes = max_bytes\n\n    def _collect_code_snapshot(self) -> str:\n        \"\"\"Return a JSON string of the current codebase/docs snapshot.\"\"\"\n        args = [\"--root\"] + list(self.collect_roots)\n        args += [\"--ext\"] + list(self.collect_ext)\n        args += [\"--max-bytes\", str(self.max_bytes), \"--out\", \"-\"]\n        try:\n            result = subprocess.run(\n                [sys.executable, DEFAULT_COLLECT_PATH, *args],\n                capture_output=True, text=True, check=True\n            )\n        except subprocess.CalledProcessError as e:\n            print(\"ERROR: collect_code.py failed\")\n            print(\"STDOUT:\", e.stdout)\n            print(\"STDERR:\", e.stderr)\n            raise\n        return result.stdout\n\n    def reset_context(self, system_prompt: Optional[str] = None):\n        \"\"\"\n        Wipe conversation history and set messages[0] as latest snapshot.\n        Optionally prepend a custom system prompt (as message[0]).\n        \"\"\"\n        code_snapshot = self._collect_code_snapshot()\n        self.messages = []\n        if system_prompt:\n            self.append_message(\"system\", system_prompt)\n        self.append_message(\n            \"user\",\n            f\"Full codebase and docs snapshot as JSON:\\n```json\\n{code_snapshot}\\n```\"\n        )\n",
  "cadence/agents/base.py": "# cadence/agents/base.py\nfrom typing import List, Dict, Any\nfrom cadence.llm.client import _CLIENT as default_llm_client\nfrom cadence.llm.client import _MODEL as default_model\n\nclass Agent:\n    def __init__(self, llm_client: Any = default_llm_client, model: str = default_model):\n        self.llm_client = llm_client\n        self.model = model\n        self.messages: List[Dict[str, Any]] = []\n\n    def reset_context(self, *args, **kwargs):\n        \"\"\"Reset the agent's conversational context (override in subclass).\"\"\"\n        self.messages = []\n\n    def append_message(self, role: str, content: str):\n        self.messages.append({\"role\": role, \"content\": content})\n\n    def run_interaction(self, user_input: str, **llm_kwargs) -> str:\n        \"\"\"\n        Add user_input, run the LLM, append the response, and return it.\n        Assumes self.llm_client follows OpenAI client conventions.\n        \"\"\"\n        self.append_message(\"user\", user_input)\n        response = self.llm_client.chat.completions.create(\n            model=self.model,\n            messages=self.messages,\n            **llm_kwargs\n        )\n        assistant_msg = response.choices[0].message.content.strip()\n        self.append_message(\"assistant\", assistant_msg)\n        return assistant_msg\n\n    def save_history(self, path):\n        import json\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.messages, f, indent=2, ensure_ascii=False)\n\n    def load_history(self, path):\n        import json\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            self.messages = json.load(f)\n",
  "cadence/dev/command_center.py": "# Cadence/dev/command_center.py\n\nimport streamlit as st\n\n# You may need to adjust the import path according to your setup\nfrom cadence.dev.orchestrator import DevOrchestrator\n\n# ---- Basic Config (map to your dev environment) ----\nCONFIG = dict(\n    backlog_path=\"dev_backlog.json\",\n    template_file=\"dev_templates.json\",\n    src_root=\"cadence\",\n    ruleset_file=None,\n    repo_dir=\".\",\n    record_file=\"dev_record.json\"\n)\norch = DevOrchestrator(CONFIG)\n\n# ---- Session State Initialization ----\nif \"selected_task_id\" not in st.session_state:\n    st.session_state[\"selected_task_id\"] = None\nif \"phase\" not in st.session_state:\n    st.session_state[\"phase\"] = \"Backlog\"\n\n# ---- Sidebar: Phase Navigation ----\nst.sidebar.title(\"Cadence Dev Center\")\nphase = st.sidebar.radio(\n    \"Workflow phase\",\n    [\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"],\n    index=[\"Backlog\", \"Task Detail\", \"Patch Review\", \"Run Test\", \"Archive\"].index(st.session_state[\"phase\"])\n)\nst.session_state[\"phase\"] = phase\n\n# ---- Main: Backlog View ----\nif phase == \"Backlog\":\n    st.title(\"Task Backlog\")\n    open_tasks = orch.backlog.list_items(status=\"open\")\n    if not open_tasks:\n        st.info(\"No open tasks! Add tasks via CLI/Notebook.\")\n    else:\n        import pandas as pd\n        df = pd.DataFrame(open_tasks)\n        st.dataframe(df[[\"id\", \"title\", \"type\", \"status\", \"created_at\"]])\n        selected = st.selectbox(\n            \"Select a task to work on\",\n            options=[t[\"id\"] for t in open_tasks],\n            format_func=lambda tid: f'{tid[:8]}: {next(t[\"title\"] for t in open_tasks if t[\"id\"] == tid)}'\n        )\n        if st.button(\"Continue to task detail\"):\n            st.session_state[\"selected_task_id\"] = selected\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Task Detail View ----\nelif phase == \"Task Detail\":\n    st.title(\"Task Details\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    st.markdown(f\"**Title:** {task['title']}\\n\\n**Type:** {task['type']}\\n\\n**Status:** {task['status']}\\n\\n**Created:** {task['created_at']}\")\n    st.code(task.get(\"description\", \"\"), language=\"markdown\")\n    st.json(task)\n    if st.button(\"Proceed to Patch Review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()\n\n# ---- Patch Review ----\nelif phase == \"Patch Review\":\n    st.title(\"Patch Review & Approval\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    task = orch.backlog.get_item(task_id)\n    try:\n        patch = orch.executor.build_patch(task)\n        st.code(patch, language=\"diff\")\n        review = orch.reviewer.review_patch(patch, context=task)\n        st.markdown(\"### Review Comments\")\n        st.markdown(review[\"comments\"] or \"_No issues detected._\")\n        if review[\"pass\"]:\n            if st.button(\"Approve and Apply Patch\"):\n                # Apply patch, save, and proceed\n                orch.shell.git_apply(patch)\n                orch.record.save(task, state=\"patch_applied\", extra={})\n                st.success(\"Patch applied.\")\n                st.session_state[\"phase\"] = \"Run Test\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Patch failed review; please revise before continuing.\")\n            if st.button(\"Back to task detail\"):\n                st.session_state[\"phase\"] = \"Task Detail\"\n                st.experimental_rerun()\n    except Exception as ex:\n        st.error(f\"Patch build/review failed: {ex}\")\n        if st.button(\"Back to task detail\"):\n            st.session_state[\"phase\"] = \"Task Detail\"\n            st.experimental_rerun()\n\n# ---- Run Test ----\nelif phase == \"Run Test\":\n    st.title(\"Run Pytest\")\n    task_id = st.session_state.get(\"selected_task_id\")\n    if not task_id:\n        st.warning(\"No task selected.\")\n        st.stop()\n    st.markdown(\"Apply code patch complete. Run tests to confirm correctness.\")\n    if st.button(\"Run tests now\"):\n        test_result = orch.shell.run_pytest()\n        st.text_area(\"Test Output\", test_result[\"output\"], height=200)\n        if test_result[\"success\"]:\n            st.success(\"Tests passed!\")\n            if st.button(\"Proceed to Archive/Done\"):\n                # Commit and archive task\n                task = orch.backlog.get_item(task_id)\n                sha = orch.shell.git_commit(f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\")\n                orch.backlog.update_item(task_id, {\"status\": \"done\"})\n                orch.backlog.archive_completed()\n                orch.record.save(task, state=\"committed\", extra={\"commit_sha\": sha})\n                orch.record.save(task, state=\"archived\", extra={})\n                st.session_state[\"phase\"] = \"Archive\"\n                st.experimental_rerun()\n        else:\n            st.error(\"Tests failed, fix required before progressing.\")\n    if st.button(\"Back to patch review\"):\n        st.session_state[\"phase\"] = \"Patch Review\"\n        st.experimental_rerun()\n\n# ---- Archive / Task Complete ----\nelif phase == \"Archive\":\n    st.title(\"Task Archived\")\n    st.success(\"Task flow completed. You may return to the backlog.\")\n    if st.button(\"Back to backlog\"):\n        st.session_state[\"selected_task_id\"] = None\n        st.session_state[\"phase\"] = \"Backlog\"\n        st.experimental_rerun()",
  "cadence/dev/shell.py": "# cadence/dev/shell.py\n\n\"\"\"\nCadence ShellRunner\n-------------------\nSingle Responsibility: Isolated safe shell/git/pytest operations, *never* creates code/diffs.\nNever does role boundaries' work. All subprocesses run in isolated, safe manner.\n\"\"\"\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Optional, Dict\n\nclass ShellCommandError(Exception):\n    \"\"\"Raised when a shell/git/pytest command fails.\"\"\"\n    pass\n\nclass ShellRunner:\n    def __init__(self, repo_dir: str = \".\"):\n        self.repo_dir = os.path.abspath(repo_dir)\n        if not os.path.isdir(self.repo_dir):\n            raise ValueError(f\"repo_dir '{self.repo_dir}' does not exist or is not a directory.\")\n\n    def git_apply(self, patch: str) -> bool:\n        \"\"\"\n        Applies patch to working tree using 'git apply'.\n        Returns True if successful.\n        Raises ShellCommandError if fail.\n        \"\"\"\n        if not patch or not isinstance(patch, str):\n            raise ShellCommandError(\"No patch supplied to apply.\")\n        # Write patch to a temp file to apply\n        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".patch\", delete=False) as tf:\n            tf.write(patch)\n            tf.flush()\n            tf_path = tf.name\n        try:\n            result = subprocess.run(\n                [\"git\", \"apply\", tf_path],\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False\n            )\n            if result.returncode != 0:\n                raise ShellCommandError(f\"git apply failed: {result.stderr.strip()}\")\n            return True\n        finally:\n            os.remove(tf_path)\n\n    def run_pytest(self, test_path: Optional[str] = None) -> Dict:\n        \"\"\"\n        Runs pytest on the given path (default: repo_dir or ./tests).\n        Returns summary dict: {'success': bool, 'output': str}\n        Raises ShellCommandError if pytest is not found.\n        \"\"\"\n        path = test_path or os.path.join(self.repo_dir, \"tests\")\n        if not os.path.exists(path):\n            raise ShellCommandError(f\"Tests path '{path}' does not exist.\")\n        try:\n            result = subprocess.run(\n                [\"pytest\", \"-q\", path],\n                cwd=self.repo_dir,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n                check=False\n            )\n            passed = (result.returncode == 0)\n            output = (result.stdout or \"\") + \"\\n\" + (result.stderr or \"\")\n            return {\"success\": passed, \"output\": output.strip()}\n        except FileNotFoundError as e:\n            raise ShellCommandError(\"pytest is not installed or not in PATH.\") from e\n\n    def git_commit(self, message: str) -> str:\n        \"\"\"\n        Commits all staged/changed files with given commit message in repo_dir.\n        Returns commit SHA string.\n        Raises ShellCommandError on fail.\n        \"\"\"\n        # Stage all (for MVP); fine-grained logic can be added if needed.\n        result = subprocess.run(\n            [\"git\", \"add\", \"-A\"],\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=False\n        )\n        if result.returncode != 0:\n            raise ShellCommandError(f\"git add failed: {result.stderr.strip()}\")\n        # Commit\n        result = subprocess.run(\n            [\"git\", \"commit\", \"-m\", message],\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=False\n        )\n        if result.returncode != 0:\n            if \"nothing to commit\" in (result.stderr + result.stdout).lower():\n                raise ShellCommandError(\"git commit: nothing to commit.\")\n            else:\n                raise ShellCommandError(f\"git commit failed: {result.stderr.strip()}\")\n        # Get last commit SHA\n        result = subprocess.run(\n            [\"git\", \"rev-parse\", \"HEAD\"],\n            cwd=self.repo_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            encoding=\"utf-8\",\n            check=True\n        )\n        sha = result.stdout.strip()\n        return sha\n\n# Example CLI/dev use\nif __name__ == \"__main__\":\n    runner = ShellRunner(\".\")\n    # runner.git_apply('--- a/foo.py\\n+++ b/foo.py\\n...')  # Patch string\n    # print(runner.run_pytest())\n    # print(runner.git_commit(\"Demo commit\"))",
  "cadence/dev/__init__.py": "",
  "cadence/dev/backlog.py": "# cadence/dev/backlog.py\n\n\"\"\"\nCadence BacklogManager\n---------------------\nSingle Responsibility: CRUD on task backlog (no code/diffs). All file IO explicit, JSON-logged, and future agent-ready.\n\"\"\"\n\nimport os\nimport json\nimport uuid\nfrom typing import List, Dict, Optional\n\nclass BacklogEmptyError(Exception):\n    \"\"\"Raised if attempting to pop or select from an empty backlog.\"\"\"\n    pass\n\nclass TaskStructureError(Exception):\n    \"\"\"Raised if a task dict doesn't conform to required structure.\"\"\"\n    pass\n\nclass TaskNotFoundError(Exception):\n    \"\"\"Raised if a requested task_id is not in the backlog.\"\"\"\n    pass\n\nREQUIRED_FIELDS = (\"id\", \"title\", \"type\", \"status\", \"created_at\")\n\nclass BacklogManager:\n    \"\"\"\n    Manages Cadence backlog: microtasks, stories, and epics.\n    - All tasks are plain dicts with mandatory fields.\n    - Underlying store is a JSON file [{...}, ...].\n    \"\"\"\n\n    def __init__(self, backlog_path: str):\n        self.path = backlog_path\n        self._items: List[Dict] = []\n        self.load()\n\n    def list_items(self, status: str = \"open\") -> List[Dict]:\n        \"\"\"\n        Return a list of tasks filtered by status.\n        status: \"open\", \"in_progress\", \"done\", or \"all\"\n        \"\"\"\n        if status == \"all\":\n            return list(self._items)\n        return [item for item in self._items if item.get(\"status\", \"open\") == status]\n\n    def add_item(self, task: Dict) -> None:\n        \"\"\"\n        Add a new task to backlog. Enforce structure and unique id.\n        \"\"\"\n        task = self._normalize_task(task)\n        if any(t[\"id\"] == task[\"id\"] for t in self._items):\n            raise TaskStructureError(f\"Duplicate task id: {task['id']}\")\n        self._items.append(task)\n        self.save()\n\n    def remove_item(self, task_id: str) -> None:\n        \"\"\"\n        Mark a task as archived (status = 'archived').\n        \"\"\"\n        idx = self._task_index(task_id)\n        self._items[idx][\"status\"] = \"archived\"\n        self.save()\n\n    def archive_completed(self) -> None:\n        \"\"\"\n        Mark all tasks with status 'done' as 'archived'.\n        \"\"\"\n        n = 0\n        for item in self._items:\n            if item.get(\"status\") == \"done\":\n                item[\"status\"] = \"archived\"\n                n += 1\n        if n:\n            self.save()\n\n    def save(self) -> None:\n        \"\"\"Persist backlog state to self.path as JSON (UTF-8, indent).\"\"\"\n        tmp_path = self.path + \".tmp\"\n        with open(tmp_path, \"w\", encoding=\"utf8\") as f:\n            json.dump(self._items, f, indent=2)\n        os.replace(tmp_path, self.path)\n\n    def load(self) -> None:\n        \"\"\"\n        Reload backlog state from file. If the file does not exist, starts empty.\n        \"\"\"\n        if not os.path.exists(self.path):\n            self._items = []\n            return\n        with open(self.path, \"r\", encoding=\"utf8\") as f:\n            data = json.load(f)\n            if not isinstance(data, list):\n                raise ValueError(\"Backlog JSON must be a list of tasks\")\n            self._items = [self._normalize_task(t) for t in data]\n\n    def _normalize_task(self, task: Dict) -> Dict:\n        \"\"\"\n        Ensure the dict has all required fields, fill missing, return new dict.\n        \"\"\"\n        t = dict(task)  # copy\n        for field in REQUIRED_FIELDS:\n            if field not in t:\n                if field == \"id\":\n                    t[\"id\"] = str(uuid.uuid4())\n                elif field == \"created_at\":\n                    import datetime\n                    t[\"created_at\"] = datetime.datetime.utcnow().isoformat()\n                elif field == \"status\":\n                    t[\"status\"] = \"open\"\n                elif field == \"type\":\n                    t[\"type\"] = \"micro\"\n                else:\n                    raise TaskStructureError(f\"Missing required field: {field}\")\n        # Sanity check: no harmful keys\n        if not isinstance(t[\"id\"], str):\n            t[\"id\"] = str(t[\"id\"])\n        return t\n\n    def _task_index(self, task_id: str) -> int:\n        \"\"\"\n        Internal: find list index of task by id or raise.\n        \"\"\"\n        for ix, t in enumerate(self._items):\n            if t[\"id\"] == task_id:\n                return ix\n        raise TaskNotFoundError(f\"No task found with id={task_id}\")\n\n    def get_item(self, task_id: str) -> Dict:\n        \"\"\"Retrieve a task by id.\"\"\"\n        idx = self._task_index(task_id)\n        return self._items[idx]\n\n    def update_item(self, task_id: str, updates: Dict) -> None:\n        \"\"\"\n        Update arbitrary fields of a task (e.g. assign, progress, edit).\n        \"\"\"\n        idx = self._task_index(task_id)\n        self._items[idx].update(updates)\n        self.save()\n\n    def export(self) -> List[Dict]:\n        \"\"\"\n        Return a (deep) copy of all backlog items.\n        \"\"\"\n        import copy\n        return copy.deepcopy(self._items)\n\n    # Optional: friendly CLI/str output\n    def __str__(self) -> str:\n        from tabulate import tabulate\n        if not self._items:\n            return \"(Backlog empty)\"\n        rows = [\n            (t[\"id\"][:8], t[\"title\"], t[\"type\"], t.get(\"status\", \"open\"), t.get(\"created_at\", \"\"))\n            for t in self._items if t.get(\"status\") != \"archived\"\n        ]\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n        return tabulate(rows, headers, tablefmt=\"github\")\n\n# For direct module test/dev, NOT in prod code.\nif __name__ == \"__main__\":\n    # Example usage\n    mgr = BacklogManager(\"dev_backlog.json\")\n    print(mgr)\n    # To add: mgr.add_item({\"title\": \"Test microtask\", \"type\": \"micro\"})",
  "cadence/dev/generator.py": "# cadence/dev/generator.py\n\n\"\"\"\nCadence TaskGenerator\n-------------------\nSingle Responsibility: Propose/generate well-formed tasks, optionally from template or rules/LLM seed.\nNever applies code or diffs. Future extensible to LLM/human agent.\n\"\"\"\n\nimport os\nimport json\nimport uuid\nfrom typing import List, Dict, Optional\nimport datetime\n\nclass TaskTemplateError(Exception):\n    \"\"\"Raised if template file is not valid or incomplete.\"\"\"\n    pass\n\nREQUIRED_FIELDS = (\"title\", \"type\", \"status\", \"created_at\")\n\nclass TaskGenerator:\n    def __init__(self, template_file: str = None):\n        \"\"\"\n        Optionally specify a JSON (or Markdown with JSON front-matter) template file.\n        \"\"\"\n        self.template_file = template_file\n        self._template_cache = None\n        if template_file:\n            self._template_cache = self._load_template(template_file)\n    \n    def generate_tasks(self, mode: str = \"micro\", count: int = 1, human_prompt: Optional[str]=None) -> List[Dict]:\n        \"\"\"\n        Return a list of well-formed tasks. \n        - mode: \"micro\", \"story\", \"epic\", etc.\n        - count: number of tasks to generate\n        - human_prompt: if provided, use as summary/title for each (e.g., \"Add new test\", for human CLI prompt workflow)\n        If template_file is used, will fill in mode-related templates.\n        \"\"\"\n        tasks = []\n        base_tpl = self._get_template_for_mode(mode)\n        now = datetime.datetime.utcnow().isoformat()\n        for i in range(count):\n            task = dict(base_tpl)\n            # Minimal fields: id, title, type, status, created_at\n            task[\"id\"] = str(uuid.uuid4())\n            task[\"type\"] = mode\n            task.setdefault(\"status\", \"open\")\n            task.setdefault(\"created_at\", now)\n            if human_prompt:\n                # Provide a default/barebones title/desc from human input\n                task[\"title\"] = human_prompt if count == 1 else f\"{human_prompt} [{i+1}]\"\n                task.setdefault(\"description\", human_prompt)\n            else:\n                # Fallback: title must be present; if not, use template/title from mode or 'Untitled'\n                task[\"title\"] = task.get(\"title\", f\"{mode.capitalize()} Task {i+1}\")\n                task.setdefault(\"description\", \"\")\n            self._validate_task(task)\n            tasks.append(task)\n        return tasks\n\n    def overwrite_tasks(self, new_tasks: List[Dict], output_path: Optional[str]=None) -> None:\n        \"\"\"\n        Replace all backlog tasks with given well-formed list (writes to output_path, else self.template_file).\n        \"\"\"\n        path = output_path or self.template_file\n        if not path:\n            raise TaskTemplateError(\"No output path specified to write tasks.\")\n        with open(path, \"w\", encoding=\"utf8\") as f:\n            json.dump([self._validate_task(t) for t in new_tasks], f, indent=2)\n\n    def _get_template_for_mode(self, mode: str) -> Dict:\n        \"\"\"\n        Get template for the given mode; falls back to default/minimal template.\n        \"\"\"\n        if self._template_cache and mode in self._template_cache:\n            return dict(self._template_cache[mode])  # deep copy\n        # Fallback: minimal template\n        return {\n            \"title\": \"\",\n            \"type\": mode,\n            \"status\": \"open\",\n            \"created_at\": \"\",\n            \"description\": \"\",\n        }\n\n    def _load_template(self, path: str) -> Dict:\n        \"\"\"\n        Loads a JSON template file mapping mode→template-dict.\n        If Markdown file with front-matter, parse the JSON front-matter.\n        \"\"\"\n        if not os.path.exists(path):\n            raise TaskTemplateError(f\"Template file not found: {path}\")\n        if path.endswith(\".md\"):\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                lines = f.readlines()\n            start, end = None, None\n            for i, line in enumerate(lines):\n                if line.strip() == \"```json\":\n                    start = i + 1\n                elif line.strip().startswith(\"```\") and start is not None and end is None:\n                    end = i\n                    break\n            if start is not None and end is not None:\n                json_str = \"\".join(lines[start:end])\n                tpl = json.loads(json_str)\n            else:\n                raise TaskTemplateError(\"Markdown template missing ```json ... ``` block.\")\n        else:\n            with open(path, \"r\", encoding=\"utf8\") as f:\n                tpl = json.load(f)\n        if not isinstance(tpl, dict):\n            raise TaskTemplateError(\"Task template must be a dict mapping mode->template.\")\n        return tpl\n\n    def _validate_task(self, task: Dict) -> Dict:\n        \"\"\"\n        Ensures task has all required fields and correct types/formats.\n        Throws TaskTemplateError if not.\n        \"\"\"\n        for field in REQUIRED_FIELDS:\n            if field not in task or (field == \"title\" and not task[\"title\"].strip()):\n                raise TaskTemplateError(f\"Task missing required field: '{field}'\")\n        if not isinstance(task[\"type\"], str):\n            raise TaskTemplateError(\"Task type must be str.\")\n        if \"id\" in task and not isinstance(task[\"id\"], str):\n            task[\"id\"] = str(task[\"id\"])\n        # Optionally: check status value, etc.\n        return task\n\n    # For future agentic/LLM/human input: accept strings, call LLM API, etc.\n    # Extend here with agent hooks.\n\n# Standalone/test CLI example (not for production)\nif __name__ == \"__main__\":\n    # Example: generate 2 microtasks from default, print as JSON:\n    g = TaskGenerator()\n    tasks = g.generate_tasks(mode=\"micro\", count=2, human_prompt=\"Example user-initiated task\")\n    print(json.dumps(tasks, indent=2))",
  "cadence/dev/reviewer.py": "# cadence/dev/reviewer.py\n\n\"\"\"\nCadence TaskReviewer\n-------------------\nSingle Responsibility: Adjudicates patch/diff quality via rules/LLM/manual. Never applies code or diffs.\nFuture extensible: can host local ruleset, shell out to LLM agent, or use human-in-the-loop.\n\"\"\"\n\nimport os\nimport json\nfrom typing import Optional, Dict\n\nclass PatchReviewError(Exception):\n    \"\"\"Raised if review input is malformed or review fails outright (e.g. ruleset not found/valid).\"\"\"\n    pass\n\nclass TaskReviewer:\n    def __init__(self, ruleset_file: str = None):\n        \"\"\"\n        Optionally specify path to ruleset file (JSON list of rules),\n        or leave blank to use default built-in rules.\n        \"\"\"\n        self.ruleset_file = ruleset_file\n        self.rules = self._load_ruleset(ruleset_file) if ruleset_file else self._default_ruleset()\n\n    def review_patch(self, patch: str, context: Optional[dict] = None) -> Dict:\n        \"\"\"\n        Review a diff/patch string (unapplied) and optional context (task, commit message, etc).\n        Returns dict {'pass': bool, 'comments': str}\n        This uses static (offline) heuristics but can be swapped for agent/LLM in future.\n        \"\"\"\n        # Guard: Patch required\n        if not patch or not isinstance(patch, str):\n            return {'pass': False, 'comments': 'Patch missing or not a string.'}\n\n        # Apply rules in order. If any hard-fail, review fails.\n        comments = []\n        passed = True\n\n        for rule in self.rules:\n            ok, msg = rule(patch, context)\n            if not ok:\n                passed = False\n            if msg:\n                comments.append(msg)\n            if not ok:\n                # For now, fail-hard (but comment all)\n                break\n\n        return {'pass': passed, 'comments': \"\\n\".join(comments).strip()}\n\n    def _default_ruleset(self):\n        \"\"\"\n        Returns a list of static rule functions: (patch, context) → (bool, str)\n        \"\"\"\n        def not_empty_rule(patch, _):\n            if not patch.strip():\n                return False, \"Patch is empty.\"\n            return True, \"\"\n        def startswith_rule(patch, _):\n            if not patch.startswith((\"---\", \"diff \", \"@@ \")):\n                return False, \"Patch does not appear to be a valid unified diff.\"\n            return True, \"\"\n        def contains_todo_rule(patch, _):\n            if \"TODO\" in patch:\n                return False, \"Patch contains 'TODO'—code review must not introduce placeholders.\"\n            return True, \"\"\n\n        # Optionally check for too-huge diffs, or forbidden patterns, via rules below.\n        def size_limit_rule(patch, _):\n            line_count = patch.count(\"\\n\")\n            if line_count > 5000:  # Arbitrary large patch guard\n                return False, f\"Patch too large for standard review ({line_count} lines).\"\n            return True, \"\"\n        return [\n            not_empty_rule, \n            startswith_rule,\n            contains_todo_rule,\n            size_limit_rule,\n        ]\n\n    def _load_ruleset(self, path: str):\n        \"\"\"\n        Loads a simple external ruleset (for human/agent extension), e.g. as list of forbidden strings.\n        For extensibility only; advanced policies/LLMs should be subclassed onto this interface.\n        \"\"\"\n        if not os.path.exists(path):\n            raise PatchReviewError(f\"Ruleset file '{path}' not found.\")\n        with open(path, \"r\", encoding=\"utf8\") as f:\n            obj = json.load(f)\n        # Expect a list of {'type':..., 'pattern':..., ...} dicts for pattern rules\n        rules = []\n        def make_rule(ruleobj):\n            typ = ruleobj.get('type')\n            pattern = ruleobj.get('pattern')\n            msg = ruleobj.get('message', f\"Patch contains forbidden pattern: {pattern}\")\n            if typ == 'forbid':\n                def _inner(patch, _):\n                    if pattern in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            elif typ == 'require':\n                def _inner(patch, _):\n                    if pattern not in patch:\n                        return False, msg\n                    return True, \"\"\n                return _inner\n            else:\n                # Ignore unknown rule types\n                def _inner(patch, _):\n                    return True, \"\"\n                return _inner\n        for ruleobj in obj:\n            rules.append(make_rule(ruleobj))\n        # Default rules always included\n        return self._default_ruleset() + rules\n\n# Standalone/example/test run\nif __name__ == \"__main__\":\n    reviewer = TaskReviewer()\n    # Good patch\n    patch = \"\"\"--- sample.py\n+++ sample.py\n@@ -1 +1,2 @@\n-print('hello')\n+print('hello world')\n\"\"\"\n    result = reviewer.review_patch(patch)\n    print(\"Result (should pass):\", result)\n\n    bad_patch = \"TODO: refactor\\n\"\n    result = reviewer.review_patch(bad_patch)\n    print(\"Result (should fail):\", result)",
  "cadence/dev/orchestrator.py": "# cadence/dev/orchestrator.py\n\n\"\"\"\nCadence DevOrchestrator\n----------------------\nCoordinated, single-point-of-control workflow runner\nacross backlog, generation, patching, review, shell, and record roles.\nNo cross-cutting, no skipped steps. \nAgent/extensible. Ready for CLI or notebook invocation.\n\"\"\"\n\nfrom .backlog import BacklogManager\nfrom .generator import TaskGenerator\nfrom .executor import TaskExecutor, PatchBuildError\nfrom .reviewer import TaskReviewer\nfrom .shell import ShellRunner, ShellCommandError\nfrom .record import TaskRecord, TaskRecordError\n\nimport sys\n\nclass DevOrchestrator:\n    def __init__(self, config: dict):\n        self.backlog = BacklogManager(config[\"backlog_path\"])\n        self.generator = TaskGenerator(config.get(\"template_file\"))\n        self.executor = TaskExecutor(config[\"src_root\"])\n        self.reviewer = TaskReviewer(config.get(\"ruleset_file\"))\n        self.shell = ShellRunner(config[\"repo_dir\"])\n        self.record = TaskRecord(config[\"record_file\"])\n\n    # ----- Backlog overview -----\n    def show(self, status: str = \"open\", printout: bool = True):\n        \"\"\"Print or return backlog overview.\"\"\"\n        items = self.backlog.list_items(status)\n        if printout:\n            print(self._format_backlog(items))\n        return items\n\n    def _format_backlog(self, items):\n        if not items:\n            return \"(Backlog empty)\"\n        from tabulate import tabulate\n        rows = [\n            (\n                t[\"id\"][:8], t.get(\"title\", \"\")[:48], t.get(\"type\", \"\"),\n                t.get(\"status\", \"\"), t.get(\"created_at\", \"\")[:19]\n            )\n            for t in items if t.get(\"status\") != \"archived\"\n        ]\n        headers = [\"id\", \"title\", \"type\", \"status\", \"created\"]\n        return tabulate(rows, headers, tablefmt=\"github\")\n    \n    # ----- Main workflow -----\n    def run_task_cycle(self, select_id: str = None, interactive: bool = True):\n        \"\"\"\n        Full end-to-end workflow for one microtask:\n        1. Select task\n        2. Build patch\n        3. Reviewer check\n        4. Apply patch (git)\n        5. Run pytest\n        6. Reviewer final check (optional)\n        7. git commit if passes; record everything\n        8. Mark task done/archived if complete\n\n        Args:\n            select_id: If provided, pick directly; else let user pick interactively\n            interactive: If True, allow prompts for selection/confirmation.\n        Returns summary dict for the cycle.\n        \"\"\"\n        try:\n            # 1. Select Task\n            open_tasks = self.backlog.list_items(status=\"open\")\n            if not open_tasks:\n                raise RuntimeError(\"No open tasks in backlog.\")\n            if select_id:\n                task = next((t for t in open_tasks if t[\"id\"] == select_id), None)\n                if not task:\n                    raise RuntimeError(f\"Task id '{select_id}' not found in open backlog.\")\n            elif interactive:\n                print(self._format_backlog(open_tasks))\n                print(\"---\")\n                idx = self._prompt_pick(len(open_tasks))\n                task = open_tasks[idx]\n            else:\n                task = open_tasks[0]  # default: pick first open\n\n            print(f\"\\n[Selected task: {task['id'][:8]}] {task.get('title')}\\n\")\n\n            # 2. Build a patch from executor\n            self.record.save(task, state=\"build_patch\")\n            patch = self.executor.build_patch(task)\n            self.record.save(task, state=\"patch_built\", extra={\"patch\": patch})\n            print(\"--- Patch built ---\\n\", patch)\n\n            # 3. Reviewer initial check\n            review1 = self.reviewer.review_patch(patch, context=task)\n            self.record.save(task, state=\"patch_reviewed\", extra={\"review\": review1})\n            print(\"--- Review 1 ---\")\n            print(review1[\"comments\"])\n            if not review1[\"pass\"]:\n                print(f\"[X] Patch failed review, aborting (state recorded).\")\n                return {\"success\": False, \"stage\": \"patch_review\", \"review\": review1}\n\n            # 4. Apply patch via shell/git (never apply if review failed)\n            applied = self.shell.git_apply(patch)\n            self.record.save(task, state=\"patch_applied\", extra={})\n            print(\"[✔] Patch applied.\")\n\n            # 5. Run tests (pytest, whole repo or tests path)\n            test_result = self.shell.run_pytest()\n            self.record.save(task, state=\"pytest_run\", extra={\"pytest\": test_result})\n            print(\"--- Pytest ---\")\n            print(test_result[\"output\"])\n            if not test_result[\"success\"]:\n                print(f\"[X] Tests FAILED, aborting before commit (state recorded).\")\n                return {\"success\": False, \"stage\": \"test\", \"test_result\": test_result}\n\n            # 6. Final review (optional): can trigger another review step here\n            # (out-of-scope for MVP—extend as needed for LLM/human gating)\n\n            # 7. git commit, record commit SHA\n            commit_msg = f\"[Cadence] {task['id'][:8]} {task.get('title', '')}\"\n            sha = self.shell.git_commit(commit_msg)\n            self.record.save(task, state=\"committed\", extra={\"commit_sha\": sha})\n            print(f\"[✔] Committed as {sha}\")\n\n            # 8. Mark task done in backlog and archive\n            self.backlog.update_item(task[\"id\"], {\"status\": \"done\"})\n            self.backlog.archive_completed()\n            self.record.save(task, state=\"archived\", extra={})\n            print(\"[✔] Task marked done and archived.\")\n\n            return {\"success\": True, \"commit\": sha, \"task_id\": task[\"id\"]}\n\n        except Exception as ex:\n            print(f\"[X] Cycle failed: {ex}\")\n            return {\"success\": False, \"error\": str(ex)}\n\n    # ----- CLI entry point -----\n    def cli_entry(self, command: str, **kwargs):\n        \"\"\"\n        Unified CLI dispatch. Supported commands: 'backlog', 'start', 'evaluate', 'done'\n        \"\"\"\n        try:\n            if command in (\"backlog\", \"show\"):\n                return self.show(status=kwargs.get(\"status\", \"open\"))\n            elif command == \"start\":\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            elif command == \"evaluate\":  # could hook for custom test/review pipeline\n                return self.run_task_cycle(select_id=kwargs.get(\"id\"))\n            elif command == \"done\":\n                # Mark a task done and archive\n                if \"id\" not in kwargs:\n                    print(\"You must supply a task id for 'done'.\")\n                    return\n                self.backlog.update_item(kwargs[\"id\"], {\"status\": \"done\"})\n                self.backlog.archive_completed()\n                print(f\"Task {kwargs['id']} marked as done and archived.\")\n                return\n            else:\n                print(f\"Unknown command: {command}\")\n        except Exception as ex:\n            print(f\"[X] CLI command '{command}' failed: {ex}\")\n\n    # ----- Notebook-friendly -----\n    # Provide direct API for notebook use:\n    # e.g., orch.show(), orch.run_task_cycle(), ...\n\n    # Helper for CLI interactive selection\n    def _prompt_pick(self, n):\n        while True:\n            ans = input(f\"Select task [0-{n-1}]: \")\n            try:\n                ix = int(ans)\n                if 0 <= ix < n:\n                    return ix\n            except Exception:\n                pass\n            print(\"Invalid. Try again.\")\n\n# Example main/dev use:\nif __name__ == \"__main__\":\n    # Example config; adjust as needed per environment\n    CONFIG = dict(\n        backlog_path=\"dev_backlog.json\",\n        template_file=\"dev_templates.json\",\n        src_root=\"cadence\",\n        ruleset_file=None,\n        repo_dir=\".\",\n        record_file=\"dev_record.json\"\n    )\n    orch = DevOrchestrator(CONFIG)\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command\", nargs=\"?\", help=\"show|start|evaluate|done\")\n    parser.add_argument(\"--id\", default=None, help=\"Task id to use\")\n    args = parser.parse_args()\n    orch.cli_entry(args.command or \"show\", id=args.id)",
  "cadence/dev/record.py": "# cadence/dev/record.py\n\n\"\"\"\nCadence TaskRecord\n-----------------\nSingle Responsibility: Append/persist task processtates for full audit/repro. \nWrite/read only here. \nFile format: JSON list of dicts, one per unique task_id; each has history of states/iterations.\n\"\"\"\n\nimport os\nimport json\nimport threading\nimport copy\nfrom typing import List, Dict, Optional\n\nclass TaskRecordError(Exception):\n    \"\"\"Custom error for task record issues.\"\"\"\n    pass\n\nclass TaskRecord:\n    def __init__(self, record_file: str):\n        self.record_file = record_file\n        self._lock = threading.Lock()\n        # Always keep in-memory up to date with file\n        self._records: List[Dict] = []\n        self._idmap: Dict[str, Dict] = {}  # task_id -> record dict\n        self._load()\n\n    def save(self, task: dict, state: str, extra: dict = None) -> None:\n        \"\"\"\n        Records a snapshot for given task_id and state (e.g. \"patch_proposed\", \"patch_reviewed\", etc).\n        Extra provides arbitrary info (patch, review, commit_sha, test_results, etc).\n        If task does not exist (task_id is new), creates new record.\n        \"\"\"\n        with self._lock:\n            record = self._find_or_create_record(task)\n            snapshot = {\n                \"state\": state,\n                \"timestamp\": self._now(),\n                \"task\": copy.deepcopy(task),\n                \"extra\": copy.deepcopy(extra) if extra else {},\n            }\n            record[\"history\"].append(snapshot)\n            self._sync_idmap()\n            self._persist()\n\n    def load(self) -> List[Dict]:\n        \"\"\"\n        Returns a (deep) copy of all records (full history).\n        \"\"\"\n        with self._lock:\n            return copy.deepcopy(self._records)\n\n    def append_iteration(self, task_id: str, iteration: dict) -> None:\n        \"\"\"\n        Appends a new step/edit/review (dict) to a task's record—usually finer-grained than save().\n        \"\"\"\n        with self._lock:\n            record = self._find_record(task_id)\n            if record is None:\n                raise TaskRecordError(f\"No record for task id={task_id}\")\n            iter_snapshot = {\n                \"timestamp\": self._now(),\n                **copy.deepcopy(iteration)\n            }\n            record.setdefault(\"iterations\", []).append(iter_snapshot)\n            self._persist()\n\n    # ========== Internal Below ==========\n\n    def _find_or_create_record(self, task: dict) -> Dict:\n        \"\"\"\n        Finds or creates a new record for given task.\n        \"\"\"\n        tid = self._get_task_id(task)\n        rec = self._idmap.get(tid)\n        if rec is None:\n            rec = {\n                \"task_id\": tid,\n                \"created_at\": self._now(),\n                \"history\": [],\n                \"iterations\": []\n            }\n            self._records.append(rec)\n            self._idmap[tid] = rec\n        return rec\n\n    def _find_record(self, task_id: str) -> Optional[Dict]:\n        return self._idmap.get(task_id)\n\n    def _get_task_id(self, task: dict) -> str:\n        tid = task.get(\"id\")\n        if not tid:\n            raise TaskRecordError(\"Task dict missing 'id'. Cannot save record.\")\n        return tid\n\n    def _persist(self) -> None:\n        \"\"\"\n        Writes in-memory records to disk, atomic/overwrite (JSON).\n        \"\"\"\n        tmp = self.record_file + \".tmp\"\n        with open(tmp, \"w\", encoding=\"utf8\") as f:\n            json.dump(self._records, f, indent=2)\n        os.replace(tmp, self.record_file)\n\n    def _load(self) -> None:\n        \"\"\"Loads file into memory iff exists. Otherwise, empty record.\"\"\"\n        if not os.path.exists(self.record_file):\n            self._records = []\n            self._idmap = {}\n            return\n        with open(self.record_file, \"r\", encoding=\"utf8\") as f:\n            self._records = json.load(f)\n        self._sync_idmap()\n\n    def _sync_idmap(self):\n        \"\"\"Ensures self._idmap is up to date with self._records.\"\"\"\n        self._idmap = {rec[\"task_id\"]: rec for rec in self._records}\n\n    def _now(self):\n        from datetime import datetime\n        return datetime.utcnow().isoformat()\n\n# Example CLI/sanity use (not for prod)\nif __name__ == \"__main__\":\n    rec = TaskRecord(\"dev_record.json\")\n    tid = \"a1b2c3\"\n    # Save new record\n    task = {\"id\": tid, \"title\": \"Do something\", \"status\": \"open\"}\n    rec.save(task, state=\"patch_proposed\", extra={\"patch\": \"--- foo\"})\n    # Append an iteration (e.g., reviewer comment)\n    rec.append_iteration(tid, {\"reviewer\": \"alice\", \"opinion\": \"looks good\"})\n    # Print record for tid\n    print(json.dumps(rec.load(), indent=2))",
  "cadence/dev/executor.py": "# cadence/dev/executor.py\n\n\"\"\"\nCadence TaskExecutor\n-------------------\nSingle Responsibility: Given a task, produce a code/text patch (unapplied). Never applies, commits, or tests.\nExtensible: can be subclassed or composed with LLM/crowd agents for codegen/refinement.\n\"\"\"\n\nimport os\nimport difflib\nimport tempfile\nfrom typing import Dict, Optional, List\n\nclass PatchBuildError(Exception):\n    \"\"\"Raised if patch/diff cannot be produced.\"\"\"\n    pass\n\n\nclass TaskExecutor:\n    def __init__(self, src_root: str):\n        if not os.path.isdir(src_root):\n            raise ValueError(f\"src_root '{src_root}' is not a directory.\")\n        self.src_root = os.path.abspath(src_root)\n\n    def build_patch(self, task: Dict) -> str:\n        \"\"\"\n        Given selected task (dict), produce diff/patch string.\n        - For simplicity, expects 'file', 'before', 'after' in task['diff'].\n        - Never applies patch.\n        - Returns unified diff as UTF-8 str.\n        \"\"\"\n        try:\n            diff_info = task.get('diff')\n            if not diff_info:\n                raise PatchBuildError(\"Task missing 'diff' key. Task must include code diff directives.\")\n\n            file_rel = diff_info.get('file')\n            before = diff_info.get('before')\n            after = diff_info.get('after')\n            if not file_rel or before is None or after is None:\n                raise PatchBuildError(\"Diff dict must have 'file', 'before', and 'after' (as strings).\")\n\n            file_abs = os.path.join(self.src_root, file_rel)\n            # Optionally validate file paths\n            before_lines = before.splitlines(keepends=True)\n            after_lines = after.splitlines(keepends=True)\n\n            diff_lines = list(difflib.unified_diff(\n                before_lines,\n                after_lines,\n                fromfile=file_rel,\n                tofile=file_rel,\n                lineterm=''\n            ))\n            patch = \"\".join(line + '\\n' for line in diff_lines)\n            if not patch.strip():\n                raise PatchBuildError(\"Generated patch is empty.\")\n\n            # Logically, do NOT write/apply - that's ShellRunner's responsibility.\n            return patch\n        except Exception as e:\n            raise PatchBuildError(f\"Failed to build patch: {e}\")\n\n    def refine_patch(self, task: Dict, feedback: str) -> str:\n        \"\"\"\n        Propose a revised patch, given task and feedback (from reviewer/human).\n        Here, we're stubbed for simplicity - can be extended to call LLM/code agent.\n        - Returns new diff/patch string.\n        \"\"\"\n        # In a future agentic system, call out to LLM or microservice here with context.\n        # Example hook: (pseudo) agent.generate_patch(task, feedback)\n        # For now, just raise if not implemented.\n        raise NotImplementedError(\"Patch refinement requires agent integration or human intervention.\")\n\n    # Optionally: you can add utility for validating a patch (not apply!).\n    def validate_patch(self, patch: str) -> bool:\n        \"\"\"\n        Returns True if patch is nontrivial and properly formatted.\n        (Simple heuristic only; actual application/testing is ShellRunner's job.)\n        \"\"\"\n        return bool(patch and patch.startswith('---'))\n\n# Example CLI/dev usage\nif __name__ == \"__main__\":\n    # Example simulated task:\n    executor = TaskExecutor(src_root=\"cadence\")\n    sample_task = {\n        \"id\": \"testid\",\n        \"diff\": {\n            \"file\": \"sample_module.py\",\n            \"before\": \"# Old code\\nprint('Hello')\\n\",\n            \"after\":  \"# Old code\\nprint('Hello, world!')\\n\"\n        }\n    }\n    patch = executor.build_patch(sample_task)\n    print(\"--- PATCH OUTPUT ---\")\n    print(patch)",
  "cadence/agents/chat/agent_registry.py": "# cadence/agents/chat/agent_registry.py\nfrom cadence.agents.task_agent import TaskAgent\n\nclass AgentRegistry:\n    def __init__(self):\n        self._agents = {}\n\n    def get_or_create(self, name, roots, ext=(\".py\", \".md\"), system_prompt=None):\n        if name not in self._agents:\n            agent = TaskAgent()\n            agent.set_collect_code_args(roots=roots, ext=ext)\n            agent.reset_context(system_prompt=system_prompt)\n            self._agents[name] = agent\n        return self._agents[name]\n\n    def list_agents(self):\n        return list(self._agents.keys())\n\n    def reset_agent(self, name, roots, ext=(\".py\", \".md\"), system_prompt=None):\n        agent = TaskAgent()\n        agent.set_collect_code_args(roots=roots, ext=ext)\n        agent.reset_context(system_prompt=system_prompt)\n        self._agents[name] = agent\n        return agent\n",
  "cadence/agents/chat/message_renderer.py": "# message_renderer.py\n\nimport streamlit as st\n\ndef render_message(msg, idx, is_editing, save_callback, cancel_callback):\n    role = msg[\"role\"]\n    content = msg[\"content\"]\n    timestamp = msg.get(\"timestamp\", \"\")\n    preview = content.split(\"\\n\")[0]\n    with st.expander(f\"{role.capitalize()} ({timestamp}) — {preview}\", expanded=False):\n        if is_editing:\n            # Show editable text area with save/cancel\n            new_content = st.text_area(\"Edit message:\", value=content, key=f\"edit_{idx}\")\n            col1, col2 = st.columns(2)\n            with col1:\n                if st.button(\"Save\", key=f\"save_{idx}\"):\n                    save_callback(new_content)\n            with col2:\n                if st.button(\"Cancel\", key=f\"cancel_{idx}\"):\n                    cancel_callback()\n        else:\n            # Show message and an Edit button\n            st.markdown(content)\n            if st.button(\"Edit\", key=f\"edit_btn_{idx}\"):\n                return True  # signals to enter edit mode\n    return False  # not in edit mode\n",
  "cadence/agents/chat/chat_utils.py": "# cadence/agents/chat/chat_utils.py\n\ndef preview_message(content: str, n_lines=2) -> str:\n    lines = content.splitlines()\n    return \"\\n\".join(lines[:n_lines]) + (\"…\" if len(lines) > n_lines else \"\")\n",
  "cadence/agents/chat/main.py": "import os\nimport streamlit as st\nimport datetime\n\nfrom cadence.agents.chat.agent_registry import AgentRegistry\nfrom chat_utils import preview_message\nfrom message_renderer import render_message\n\n\ndef get_timestamp():\n    return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\ndef parse_paths(text):\n    # Split on comma or space, strip whitespace, filter empty\n    return [x.strip() for x in text.replace(\",\", \" \").split() if x.strip()]\n\n# ---- Initialize the registry (persistent per session) ----\nif \"agent_registry\" not in st.session_state:\n    st.session_state[\"agent_registry\"] = AgentRegistry()\nagent_registry = st.session_state[\"agent_registry\"]\n\n# 1. List agents, handle default\nagents = agent_registry.list_agents()\ndefault_agent = agents[0] if agents else None\n\n# 2. Handle \"just created\" logic *before* rendering selectbox\nif \"just_created_agent\" in st.session_state:\n    st.session_state[\"selected_agent\"] = st.session_state[\"just_created_agent\"]\n    del st.session_state[\"just_created_agent\"]\n\nif \"selected_agent\" not in st.session_state:\n    st.session_state[\"selected_agent\"] = default_agent\n\n# 3. Prepare agent selection UI\nselect_options = ([\"+ Create new agent\"] + agents) if agents else [\"+ Create new agent\"]\ndefault_index = 1 if agents else 0\n\nselected_agent_name = st.sidebar.selectbox(\n    \"Select agent\", select_options, index=default_index, key=\"selected_agent\"\n)\n\n# 4. If creating new agent, show creation form\nif not agents or selected_agent_name == \"+ Create new agent\":\n    st.sidebar.markdown(\"### Create a new agent\")\n    new_agent_name = st.sidebar.text_input(\"Agent name\", key=\"new_agent_name\")\n    new_roots = st.sidebar.text_input(\"Code roots (comma/space separated)\", value=\"docs cadence\", key=\"new_roots\")\n    roots = parse_paths(new_roots)\n    bad_roots = [r for r in roots if not os.path.isdir(r)]\n    if bad_roots:\n        st.sidebar.error(f\"Invalid directory(ies): {', '.join(bad_roots)}\")\n        st.stop()\n    new_ext = st.sidebar.text_input(\"File extensions (space/comma)\", value=\".py .md\", key=\"new_ext\")\n    ext = tuple(parse_paths(new_ext))\n    system_prompt = st.sidebar.text_area(\"System prompt (optional)\", key=\"new_system_prompt\")\n    if st.sidebar.button(\"Create agent\", key=\"btn_create_agent\"):\n        if new_agent_name.strip():\n            agent_registry.get_or_create(new_agent_name, roots=roots, ext=ext, system_prompt=system_prompt)\n            st.session_state[\"just_created_agent\"] = new_agent_name\n            st.rerun()\n    st.info(\"Create an agent to begin chatting.\")\n    st.stop()\n\n# -- Otherwise, agent exists, display config --\nagent = agent_registry._agents[selected_agent_name]\nroots_val = \" \".join(agent.collect_roots)\next_val = \" \".join(agent.collect_ext)\nsystem_prompt_val = getattr(agent, \"system_prompt\", \"\") if hasattr(agent, \"system_prompt\") else \"\"\n\nroots_input = st.sidebar.text_input(\n    \"Code roots (comma/space)\", value=roots_val, key=\"edit_roots\"\n)\nroots = parse_paths(roots_input)\nbad_roots = [r for r in roots if not os.path.isdir(r)]\nif bad_roots:\n    st.sidebar.error(f\"Invalid directory(ies): {', '.join(bad_roots)}\")\n    st.stop()\n\next_input = st.sidebar.text_input(\n    \"Extensions (space/comma)\", value=ext_val, key=\"edit_ext\"\n)\next = tuple(parse_paths(ext_input))\n\nsystem_prompt = st.sidebar.text_area(\n    \"System prompt (optional)\",\n    value=system_prompt_val,\n    key=\"edit_system_prompt\"\n)\n\nif st.sidebar.button(\"Reset context for this agent\", key=\"reset_ctx_btn\"):\n    agent.set_collect_code_args(\n        roots=roots,\n        ext=ext\n    )\n    agent.reset_context(system_prompt=system_prompt if system_prompt.strip() else None)\n    st.rerun()\n\n# ---- Get the active agent ----\nif selected_agent_name == \"+ Create new agent\" or selected_agent_name is None:\n    st.info(\"Create or select an agent to begin chatting.\")\n    st.stop()\nagent = agent_registry._agents[selected_agent_name]\n\nst.title(f\"Cadence: Agentic Dev Chat — {selected_agent_name}\")\n\nmessages = agent.messages\n\n# ---- Render chat history with edit/minimize features ----\nif \"editing_idx\" not in st.session_state:\n    st.session_state[\"editing_idx\"] = None\nediting_idx = st.session_state[\"editing_idx\"]\n\nfor idx, msg in enumerate(messages):\n    is_editing = (editing_idx == idx)\n    def make_save(idx):\n        def save(new_content):\n            msg['content'] = new_content\n            st.session_state[\"editing_idx\"] = None\n        return save\n    def make_cancel():\n        def cancel():\n            st.session_state[\"editing_idx\"] = None\n        return cancel\n\n    entered_edit = render_message(\n        msg, idx, is_editing,\n        save_callback=make_save(idx),\n        cancel_callback=make_cancel()\n    )\n    if entered_edit and not is_editing:\n        st.session_state[\"editing_idx\"] = idx\n        st.rerun()\n\n# ---- Input box for new user message ----\nst.divider()\nuser_input = st.text_area(\"Enter your message:\", key=\"new_message\")\n\nif st.button(\"Send\", key=\"send\") and user_input.strip():\n    # 1. Append user message (with timestamp)\n    agent.append_message(\"user\", user_input.strip())\n    # 2. Run the agent and get the response\n    response = agent.run_interaction(user_input.strip())\n    st.rerun()\n",
  "tools/collect_code.py": "#!/usr/bin/env python3\n\"\"\"\ncollect_code.py  –  Export Cadence source files to a single JSON payload.\n\nUsage\n-----\npython tools/collect_code.py \\\n       --root cadence              # package folder(s) to scan (repeatable)\n       --out  code_payload.json   # written JSON (stdout if omitted)\n       --ext .py .md              # file extensions to keep\n       --max-bytes 50000          # skip giant files (>50 kB)\n\nResult\n------\nA JSON dict   { \"relative/path/to/file\": \"UTF-8 text …\", ... }\n\"\"\"\n\nfrom __future__ import annotations\nfrom pathlib import Path\nimport argparse\nimport json\nimport sys\n\nDEFAULT_EXT = (\".py\", \".md\", \".cfg\", \".toml\", \".ini\")\n\n\ndef collect(\n    roots: list[Path],\n    *,\n    extensions: tuple[str, ...] = DEFAULT_EXT,\n    max_bytes: int | None = None,\n) -> dict[str, str]:\n    \"\"\"\n    Walk *roots* and return {relative_path: code_text}.\n    Skips __pycache__, hidden folders, and files larger than *max_bytes*.\n    \"\"\"\n    out: dict[str, str] = {}\n    for root in roots:\n        for path in root.rglob(\"*\"):\n            if (\n                path.is_file()\n                and path.suffix in extensions\n                and \"__pycache__\" not in path.parts\n                and not any(p.startswith(\".\") for p in path.parts)\n            ):\n                if max_bytes and path.stat().st_size > max_bytes:\n                    continue\n                try:\n                    text = path.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n                out[str(path.relative_to(Path.cwd()))] = text\n    return out\n\n\ndef parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Collect source files into JSON.\")\n    p.add_argument(\n        \"--root\",\n        nargs=\"+\",\n        default=[\"cadence\"],\n        help=\"Directories to scan (repeatable).\",\n    )\n    p.add_argument(\n        \"--ext\",\n        nargs=\"+\",\n        default=DEFAULT_EXT,\n        help=\"File extensions to include (repeatable).\",\n    )\n    p.add_argument(\n        \"--max-bytes\",\n        type=int,\n        default=50000,\n        help=\"Skip files larger than this size (bytes).\",\n    )\n    p.add_argument(\n        \"--out\",\n        type=str,\n        default=\"-\",\n        help=\"Output JSON file path or '-' for stdout.\",\n    )\n    return p.parse_args(argv)\n\n\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = parse_args(argv)\n    payload = collect(\n        [Path(r).resolve() for r in args.root],\n        extensions=tuple(args.ext),\n        max_bytes=args.max_bytes,\n    )\n    if args.out == \"-\":\n        json.dump(payload, sys.stdout, indent=2, ensure_ascii=False)\n    else:\n        out_path = Path(args.out)\n        out_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False))\n        print(f\"Wrote {len(payload)} files → {out_path}\")\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    main()\n",
  "tools/gen_prompt.py": "#!/usr/bin/env python3\n\"\"\"\ngen_prompt.py  –  Assemble a mega-prompt that contains\n\n  • Ground-truth docs (blueprint, progress logs, etc.)\n  • Full source snapshot (or whatever roots you point at)\n  • A header that gives the LLM an explicit NEXT TASK and ENV context\n\nUsage\n-----\npython tools/gen_prompt.py \\\n       --code-root cadence \\\n       --docs-dir docs \\\n       --task \"Implement FactorRegistry API and unit tests\" \\\n       --env  \"Python 3.11, pandas 2.2, scikit-learn 1.4\" \\\n       --out  prompt.txt\n\"\"\"\n\nfrom __future__ import annotations\nfrom pathlib import Path\nimport argparse\nimport sys\nimport textwrap\n\n# --------------------------------------------------------------------------- #\n#  Config\n# --------------------------------------------------------------------------- #\nDEFAULT_CODE_EXT = (\".py\", \".md\", \".toml\", \".ini\", \".cfg\")\n\n\n# --------------------------------------------------------------------------- #\n#  Helpers\n# --------------------------------------------------------------------------- #\ndef _collect_files(\n    roots: list[Path],\n    *,\n    include_ext: tuple[str, ...],\n    max_bytes: int | None = None,\n) -> list[tuple[str, str]]:\n    \"\"\"Return [(relative_path, text), …] for all files matching *include_ext*.\"\"\"\n    records: list[tuple[str, str]] = []\n    cwd = Path.cwd()\n\n    for root in roots:\n        root = Path(root).resolve()\n        if not root.exists():\n            print(f\"WARNING: directory not found → {root}\", file=sys.stderr)\n            continue\n\n        for p in root.rglob(\"*\"):\n            if (\n                p.is_file()\n                and p.suffix in include_ext\n                and \"__pycache__\" not in p.parts\n                and not any(part.startswith(\".\") for part in p.parts)\n            ):\n                if max_bytes and p.stat().st_size > max_bytes:\n                    continue\n                try:\n                    txt = p.read_text(encoding=\"utf-8\")\n                except UnicodeDecodeError:\n                    txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n                records.append((str(p.relative_to(cwd)), txt))\n\n    records.sort()\n    return records\n\n\ndef _build_prompt(\n    docs: list[tuple[str, str]],\n    code: list[tuple[str, str]],\n    *,\n    header: str,\n) -> str:\n    parts: list[str] = [header]\n\n    # -- docs ---------------------------------------------------------------\n    parts.append(\"\\n## 1. Ground-Truth Documents\")\n    if not docs:\n        parts.append(\"\\n_No Markdown / text documents found in docs directory._\")\n    for path, txt in docs:\n        parts.append(f\"\\n### {path}\\n```markdown\\n{txt}\\n```\")\n\n    # -- code ---------------------------------------------------------------\n    parts.append(\"\\n## 2. Source Code Snapshot\")\n    if not code:\n        parts.append(\"\\n_No source files found in code roots._\")\n    for path, txt in code:\n        fence = \"```python\" if path.endswith(\".py\") else \"```text\"\n        parts.append(f\"\\n### {path}\\n{fence}\\n{txt}\\n```\")\n\n    return \"\\n\".join(parts)\n\n\n# --------------------------------------------------------------------------- #\n#  CLI\n# --------------------------------------------------------------------------- #\ndef _parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Generate mega-prompt for LLM.\")\n    p.add_argument(\"--code-root\", nargs=\"+\", default=[\"cadence\"],\n                   help=\"Package directories to scan (repeatable).\")\n    p.add_argument(\"--docs-dir\", default=\"docs\",\n                   help=\"Directory holding NORTH_STAR.md, progress logs, etc.\")\n    p.add_argument(\"--ext\", nargs=\"+\", default=DEFAULT_CODE_EXT,\n                   help=\"File extensions to include from code roots.\")\n    p.add_argument(\"--max-bytes\", type=int, default=100_000,\n                   help=\"Skip individual files larger than this size (bytes).\")\n    p.add_argument(\"--skip-code\", action=\"store_true\",\n               help=\"Omit source snapshot (tasks only).\")\n    p.add_argument(\"--task\", default=\"Tell me the next highest-leverage step and write the code.\",\n                   help=\"Explicit next task instruction injected into header.\")\n    p.add_argument(\"--env\", default=\"Python 3.11, pandas 2.2, scikit-learn 1.4\",\n                   help=\"Runtime environment string added to header.\")\n    p.add_argument(\"--out\", default=\"-\",\n                   help=\"Output file path or '-' for stdout.\")\n    return p.parse_args(argv)\n\n\n# --------------------------------------------------------------------------- #\n#  Main\n# --------------------------------------------------------------------------- #\ndef main(argv: list[str] | None = None) -> None:  # pragma: no cover\n    args = _parse_args(argv)\n\n    docs = _collect_files(\n        [Path(args.docs_dir).resolve()],\n        include_ext=(\".md\", \".txt\"),\n        max_bytes=args.max_bytes,\n    )\n\n    if args.skip_code:\n        code = []\n    else:\n        code = _collect_files(\n            [Path(r).resolve() for r in args.code_root],\n            include_ext=tuple(args.ext),\n            max_bytes=args.max_bytes,\n        )\n\n    header = textwrap.dedent(\n        f\"\"\"\\\n        ### CADENCE STATUS / ALIGNMENT REQUEST  (auto-generated)\n\n        **Task**: {args.task}\n        **Environment**: {args.env}\n\n        You are an expert reviewer. Read ALL content below — docs first, then full\n        code — and report:\n\n          1. Alignment gaps between implementation and blueprint  \n          2. Missing risk / compliance safeguards  \n          3. Highest-leverage next actions  \n\n        Be brutally honest. No cheerleading. Return your analysis **only**.\n\n        ---\n        \"\"\"\n    )\n\n    prompt = _build_prompt(docs, code, header=header)\n\n    if args.out == \"-\":\n        sys.stdout.write(prompt)\n    else:\n        Path(args.out).write_text(prompt, encoding=\"utf-8\")\n        print(f\"Wrote prompt to {args.out}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
}